{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DSITTC\\AppData\\Local\\Temp\\ipykernel_492\\1916310447.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(api_key=api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if api_key:\n",
    "    llm = OpenAI(api_key=api_key)\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=api_key, temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader =TextLoader(\"mytext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x20b8d0886a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_data= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mytext.txt'}, page_content='\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\n\\n because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pdf\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_pdf = PyPDFLoader(\"LatestResume_GenAI_SK.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_pdf_data = loader_pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-10T09:40:26+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-10T09:40:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LatestResume_GenAI_SK.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Sandeep Kumar\\n♂phone9305525008 — /linkedinsandeepkumar — /githubSandeepk14 — /envel⌢peskraj5873@gmail.com — /user-circlePortfolio\\nSummary — Aspiring Data Scientist with practical experience in Machine Learning and Data Analysis. Gained hands-on\\nexpertise during a 6-month internship at Jupiter AI Lab, working with Python and its robust data libraries (Pandas, NumPy ,\\nMatplotlib, and Seaborn) for model building and EDA (Exploratory Data Analysis). Proficient in SQL, BI tools like Power BI,\\nand Excel for effective data management and visualization.\\nSkills\\nLanguages: Python,Java, SQL\\nLibraries: Pandas, NumPy , Matplotlib,SckiteLearn Seaborn, Flask, TensorFlow\\nGenerative AI: Hugging Face Transformers, LangChain, Rag, Fine Tunning, FastAPI\\nTools: Git, Github, Power BI, Excel, Vs Code, Anaconda, Jupyter Notebook, Google Colab\\nTechniques: ML, DL, Data Analysis, Model Building, EDA, Data Manipulation, Analyzing Large Datasets\\nSoft Skills: Communication Skills, Presentation Skills, Hard-Working, Problem-Solving\\nExperience\\nMachine Learning Intern Aug 2024 – Currently Working\\nJupiter AI Lab Onsite- Noida\\n– Enhanced data visualization by designing dynamic dashboards and reports in Power BI, which led to improved data\\ninterpretation and actionable insights.\\n– Streamlined data workflows Automating routine data extraction and cleaning processes, resulting in a 40 percent\\nreduction in manual effort and increased efficiency .\\n– Applied statistical techniques and machine learning algorithms to analyze complex datasets, providing predictive\\ninsights and supporting strategic planning.\\n– Gained hands-on experience with model training, evaluation, and feature engineering, contributing to real-world\\nprojects and supporting data-driven decision-making.\\nEducation\\nDegree Institute Board / University CGPA/Percentage Year\\nB.Tech IT Rajkiya Engineering College, Azamgarh AKTU 7.0 CGPA 2024\\nSenior Secondary Bal Krishna I C Khatirpur, Ghazipur UP Board 75.20% 2019\\nMatriculation S R D A H S S Bhajayatilari, Ghazipur UP Board 84.12% 2017\\nProjects\\nOnline Written Examination System Using Machine Learning and Blockchain Mar 2024\\nMajor Project (B.tech)\\n– Implementing an Online Written Examination System using Blockchain and Machine Learning with the MERN stack ensures\\nsecure, tamper-proof exam records and intelligent evaluation.\\n– Blockchain guarantees data integrity, while ML algorithms enhance the evaluation process by providing smart analytics and\\npersonalized feedback.\\nHouse Price Prediction Aug 2024\\nReal Time Mini Project Github\\n– Data Preprocessing: Experience with data cleaning, normalization, feature engineering, and handling missing values.\\n– Model Selection and Evaluation: Various regression algorithms (linear , ridge, lasso, decision trees, random forests, etc.) and\\nevaluation metrics (MSE, RMSE, MAE, R-squared).\\nText Summrization Gen AI Project Feb 2025\\nMini Project Github\\n– Developed a text summarization model using transformers like BART/GPT to generate concise summaries from long-form\\ntext.\\n– Fine-tuned a pre-trained transformer model on a custom dataset to improve summarization quality.\\n– Model Evaluation: Performed evaluation using ROUGE and BLEU scores to measure summarization accuracy.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual_pdf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load wekipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wekipedia\n",
    "from langchain.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_wiki = WikipediaLoader(\"Albert Einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSITTC\\.conda\\envs\\langchain_env\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\DSITTC\\.conda\\envs\\langchain_env\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "text_wiki_data = loader_wiki.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Albert Einstein', 'summary': 'Albert Einstein (, EYEN-styne; German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is best known for developing the theory of relativity. Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.\\nBorn in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss Federal Polytechnic School in Zurich, graduating in 1900. He acquired Swiss citizenship a year later and afterwards secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin to join the Prussian Academy of Sciences and the Humboldt University of Berlin, becoming director of the Kaiser Wilhelm Institute for Physics. In 1933, while Einstein was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi persecution of his fellow Jews, he decided to remain in the US, and was granted American citizenship in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research.\\nIn 1905, sometimes described as his annus mirabilis (miracle year), he published four groundbreaking papers. In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole. In 1917, Einstein wrote a paper which introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which contained a trove of information that would be beneficial to developments in physics later on, such as quantum electrodynamics and quantum optics.\\nIn the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With physicist Satyendra Nath Bose, he laid the groundwork for Bose-Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that ultimately proved unsuccessful. First, he advocated against quantum theory\\'s introduction of fundamental randomness into science\\'s picture of the world, objecting that God does not play dice. Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism. As a result, he became increasingly isolated from mainstream modern physics. In 1999, Einstein was named Time\\'s Person of the Century. That same year, a poll of leading physicists named Einstein the greatest physicist of all time.', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein'}, page_content='Albert Einstein (, EYEN-styne; German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is best known for developing the theory of relativity. Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.\\nBorn in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss Federal Polytechnic School in Zurich, graduating in 1900. He acquired Swiss citizenship a year later and afterwards secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin to join the Prussian Academy of Sciences and the Humboldt University of Berlin, becoming director of the Kaiser Wilhelm Institute for Physics. In 1933, while Einstein was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi persecution of his fellow Jews, he decided to remain in the US, and was granted American citizenship in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research.\\nIn 1905, sometimes described as his annus mirabilis (miracle year), he published four groundbreaking papers. In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole. In 1917, Einstein wrote a paper which introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which contained a trove of information that would be beneficial to developments in physics later on, such as quantum electrodynamics and quantum optics.\\nIn the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With physicist Satyendra Nath Bose, he laid the groundwork for Bose-Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that ultimately proved unsuccessful. First, he advocated against quantum theory\\'s introduction of fundamental randomness into science\\'s picture of the world, objecting that God does not play dice. Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism. As a result, he became increasingly isolated from mainstream modern physics. In 1999, Einstein was named Time\\'s Person of the Century. That same year, a poll of leading physicists named Einstein the greatest physicist of all time.\\n\\n\\n== Life and career ==\\n\\n\\n=== Childhood, youth and education ===\\n\\nAlbert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879. His parents, secular Ashkenazi Jews, were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich\\'s borough of Ludwigsvorstadt-Isarvorstadt, where Einstein\\'s father and his uncle Ja'),\n",
       " Document(metadata={'title': 'Hans Albert Einstein', 'summary': 'Hans Albert Einstein (May 14, 1904 – July 26, 1973) was a Swiss-American engineer, the second child and first son of physicists Albert Einstein and Mileva Marić. He was a long-time professor of hydraulic engineering at the University of California, Berkeley.\\nEinstein was widely recognized for his research on sediment transport. To honor his outstanding achievement in hydraulic engineering, the American Society of Civil Engineers established the \"Hans Albert Einstein Award\" in 1988 and the annual award is given to those who have made significant contributions to the field.', 'source': 'https://en.wikipedia.org/wiki/Hans_Albert_Einstein'}, page_content='Hans Albert Einstein (May 14, 1904 – July 26, 1973) was a Swiss-American engineer, the second child and first son of physicists Albert Einstein and Mileva Marić. He was a long-time professor of hydraulic engineering at the University of California, Berkeley.\\nEinstein was widely recognized for his research on sediment transport. To honor his outstanding achievement in hydraulic engineering, the American Society of Civil Engineers established the \"Hans Albert Einstein Award\" in 1988 and the annual award is given to those who have made significant contributions to the field.\\n\\n\\n== Early life ==\\nHans Albert Einstein was born on May 14, 1904, in Bern, Switzerland, where his father, Albert Einstein, worked as a clerk in the Swiss Federal Institute of Intellectual Property. His father was of German-Jewish descent and his mother, Mileva Marić, Serbian. His younger brother, Eduard Einstein, was born in 1910 and died in 1965. In 1913, Hans and Eduard were baptized as \\nOrthodox Christians in the Orthodox Church of Saint Nicholas in Novi Sad. The fate of his older sister, Lieserl Einstein, Albert Einstein\\'s and Mileva Marić\\'s first child, is unknown, although it has been suggested she died of scarlet fever in 1903. Their parents divorced in 1919 after living apart for five years.\\n\\n\\n== Career ==\\nIn 1922, Hans followed in his parents\\' footsteps and entered ETH Zurich, where he studied civil engineering, graduating in 1926. From 1926 to 1930 he worked at the steel design company Klönne, in Dortmund, Germany. From 1931 to 1938, he worked as a research engineer at the newly founded Laboratory of Hydraulics and Soil Mechanics (VAWE) at ETH Zurich. There, in 1936 Hans Albert obtained a doctorate in technical science. His doctoral thesis \"Bed Load Transport as a Probability Problem\" (Der Geschiebetrieb als Wahrscheinlichkeitsproblem) is considered the definitive work on sediment transport.\\nHans\\' father, Albert, left Germany in 1933 to escape the persecution of Jews by the Nazi government. Heeding his father\\'s advice, Hans emigrated from Switzerland to Greenville, South Carolina, in 1938. He worked for the US Department of Agriculture, studying sediment transport from 1938 to 1943. He continued working for the USDA at the California Institute of Technology starting in 1943. In 1947 he took a position as associate professor of hydraulic engineering at the University of California, Berkeley. He advanced to full professor, and later professor emeritus.  Einstein traveled the world to participate in hydraulic engineering conferences.\\nEinstein was honored by a Guggenheim Fellowship (1953), research awards from the American Society of Civil Engineers (1959 and 1960), the Berkeley Citation from the University of California (1971), the Certificate of Merit from the U.S. Department of Agriculture (1971), and a certificate of recognition for more than 20 years of devoted and distinguished service to Applied Mechanics Reviews by the American Society of Mechanical Engineers (1972).\\nHans was also made a member of Pi Tau Sigma in December 1949 with honorary membership grade.\\nIn 1958 he was the principal guest of honor at the Technion\\'s dedication of a new building housing the Albert Einstein Institute of Physics.\\n\\n \\nHans Albert Einstein collapsed and died of heart failure on July 26, 1973 while attending a symposium at Woods Hole, Massachusetts. His papers are held at the Water Resources Collections and Archives in the University of California, Riverside Libraries and in the University of Iowa Libraries Special Collections and Archives.\\n\\n\\n== Personal life ==\\nIn 1927, Hans Albert Einstein married Frieda Knecht. They had four children:\\n\\nBernhard Caesar Einstein (10 July 1930 – 30 September 2008), who was a physicist and engineer.\\nKlaus Martin Einstein (1932–1939), died of diphtheria aged six.\\nDavid Einstein (October–November 1939), died aged one month.\\nEvelyn Einstein (28 March 1941 – 13 April 2011), adopted.\\nKnecht died in 1958, and Hans Albert married neurochemi'),\n",
       " Document(metadata={'title': 'Einstein family', 'summary': \"The Einstein family is the family of physicist Albert Einstein (1879–1955). Einstein's great-great-great-great-grandfather, Jakob Weil, was his oldest recorded relative, born in the late 17th century, and the family continues to this day. Albert Einstein's great-great-grandfather, Löb Moses Sontheimer (1745–1831), was also the grandfather of the tenor Heinrich Sontheim (1820–1912) of Stuttgart.\\nAlbert's three children were from his relationship with his first wife, Mileva Marić, his daughter Lieserl being born a year before they married. Albert Einstein's second wife was Elsa Einstein, whose mother Fanny Koch was the sister of Albert's mother, and whose father, Rudolf Einstein, was the son of Raphael Einstein, a brother of Albert's paternal grandfather. Albert and Elsa were thus first cousins through their mothers and second cousins through their fathers.\", 'source': 'https://en.wikipedia.org/wiki/Einstein_family'}, page_content='The Einstein family is the family of physicist Albert Einstein (1879–1955). Einstein\\'s great-great-great-great-grandfather, Jakob Weil, was his oldest recorded relative, born in the late 17th century, and the family continues to this day. Albert Einstein\\'s great-great-grandfather, Löb Moses Sontheimer (1745–1831), was also the grandfather of the tenor Heinrich Sontheim (1820–1912) of Stuttgart.\\nAlbert\\'s three children were from his relationship with his first wife, Mileva Marić, his daughter Lieserl being born a year before they married. Albert Einstein\\'s second wife was Elsa Einstein, whose mother Fanny Koch was the sister of Albert\\'s mother, and whose father, Rudolf Einstein, was the son of Raphael Einstein, a brother of Albert\\'s paternal grandfather. Albert and Elsa were thus first cousins through their mothers and second cousins through their fathers.\\n\\n\\n== Etymology ==\\nEinstein (English:  EYEN-styne, German: [ˈaɪnʃtaɪn] ) is either a German habitational surname from various places named with a Middle High German derivative of the verb einsteinen \\'to enclose, surround with stone\\'; or a Jewish (Ashkenazic) adaptation of the German name, or else an ornamental name using the ending -stein \\'stone\\'.\\n\\n\\n== Pauline Einstein (Albert\\'s mother) ==\\n\\nPauline Einstein (née Koch) (8 February 1858 – 20 February 1920) was the mother of the physicist Albert Einstein.  She was born in Cannstatt, Kingdom of Württemberg. She was Jewish and had an older sister, Fanny, and two older brothers, Jacob and Caesar. Her parents were Julius Doerzbacher, who had adopted the family name Koch in 1842, and Jette Bernheimer. They were married in 1847. Pauline\\'s father was from Jebenhausen, now part of the city of Göppingen, and grew up in modest economic circumstances. Later, he lived in Cannstatt and together with his brother Heinrich, made a considerable fortune in the corn trade. They even became \"Royal Württemberg Purveyor to the Court\". Their mother was from Cannstatt and was a quiet and caring person.\\n\\n\\n=== Early life ===\\nAt 18 years old, Pauline married the merchant Hermann Einstein who lived in Ulm. They married in Cannstatt on 8 August 1876. After the wedding, the young couple lived in Ulm, where Hermann became joint partner in a bed feathers company. Their son, Albert was born on 14 March 1879. On the initiative of Hermann\\'s brother Jakob the family moved to Munich\\'s borough of Ludwigsvorstadt-Isarvorstadt in the summer of 1880, where the two brothers together founded an electrical engineering company called Einstein & Cie. The second child of Hermann and Pauline, their daughter Maria (called Maja), was born in Munich on 18 November 1881. Pauline Einstein was a well-educated and quiet woman who had an inclination for the arts. She was a talented and dedicated piano player. She made Albert begin violin lessons at the age of five.\\n\\n\\n=== Business problems ===\\nThe factory of Hermann and Jakob was moved to Pavia, Italy, in 1894. Hermann, Maria and Pauline moved to Milan in the same year and one year later, moved to Pavia. Albert stayed with relatives in Munich to continue his education there. \\nUnfortunately, the business was unsuccessful and the brothers had to abandon their factory in 1896. Though Hermann had lost most of his money, he founded (without his brother) another electrical engineering company in Milan. This time business was better. However, Hermann\\'s health had deteriorated, and he died of heart failure in Milan on 10 October 1902.\\n\\n\\n=== After Hermann ===\\nIn 1903, Pauline went to live with her sister Fanny and her husband Rudolf Einstein, a first cousin of Hermann, in Hechingen, Württemberg. Fanny\\'s daughter, Elsa was to become the second wife of Albert in 1919.\\nIn 1910, Pauline moved with her sister, Fanny and her family to Berlin. She took on a job as housekeeper in Heilbronn, Kingdom of Württemberg in 1911. She lived with her brother Jacob Koch in Zurich and from 1915 in Heilbronn again.\\n\\n\\n=== Death ===\\nDuring World War I, Pauline fell i'),\n",
       " Document(metadata={'title': 'Albert Brooks', 'summary': 'Albert Brooks (born Albert Lawrence Einstein; July 22, 1947) is an American actor, director and screenwriter. He received an Academy Award nomination for Best Supporting Actor for his performance in the 1987 comedy-drama film Broadcast News and was widely praised for his performance in the 2011 action drama film Drive. Brooks has also acted in films such as Taxi Driver (1976), Private Benjamin (1980), Unfaithfully Yours (1984), Out of Sight (1998) and My First Mister (2001). He has written, directed, and starred in several comedy films, such as Modern Romance (1981), Lost in America (1985), and Defending Your Life (1991). He is also the author of 2030: The Real Story of What Happens to America (2011).\\nBrooks has also voiced several characters in animated films and television shows. His voice acting roles include Marlin in Finding Nemo (2003) and its sequel Finding Dory (2016), Tiberius in The Secret Life of Pets (2016), and several one-time characters in The Simpsons, including Hank Scorpio in \"You Only Move Twice\" (1996) and Russ Cargill in The Simpsons Movie (2007).', 'source': 'https://en.wikipedia.org/wiki/Albert_Brooks'}, page_content='Albert Brooks (born Albert Lawrence Einstein; July 22, 1947) is an American actor, director and screenwriter. He received an Academy Award nomination for Best Supporting Actor for his performance in the 1987 comedy-drama film Broadcast News and was widely praised for his performance in the 2011 action drama film Drive. Brooks has also acted in films such as Taxi Driver (1976), Private Benjamin (1980), Unfaithfully Yours (1984), Out of Sight (1998) and My First Mister (2001). He has written, directed, and starred in several comedy films, such as Modern Romance (1981), Lost in America (1985), and Defending Your Life (1991). He is also the author of 2030: The Real Story of What Happens to America (2011).\\nBrooks has also voiced several characters in animated films and television shows. His voice acting roles include Marlin in Finding Nemo (2003) and its sequel Finding Dory (2016), Tiberius in The Secret Life of Pets (2016), and several one-time characters in The Simpsons, including Hank Scorpio in \"You Only Move Twice\" (1996) and Russ Cargill in The Simpsons Movie (2007).\\n\\n\\n== Early life ==\\nBrooks was born Albert Lawrence Einstein on July 22, 1947, into a Jewish show business family in Beverly Hills, California, to Thelma Leeds (née Goodman), an actress, and Harry Einstein, a radio comedian who performed on Eddie Cantor\\'s radio program and was known as \"Parkyakarkus\". He is the youngest of three sons. His older brothers are the late comedic actor Bob Einstein (1942–2019), and Clifford Einstein (b. 1939), a partner and longtime chief creative officer at Los Angeles advertising agency Dailey & Associates. His older half-brother was Charles Einstein (1926–2007), a writer for such television programs as Playhouse 90 and Lou Grant. His grandparents emigrated from Austria and Russia. He grew up among show business families in Southern California, attending Beverly Hills High School with Richard Dreyfuss and Rob Reiner.\\n\\n\\n== Career ==\\nBrooks attended Carnegie Institute of Technology (now Carnegie Mellon University) in Pittsburgh (where his classmates included Michael McKean and David L. Lander), but dropped out after one year to focus on his comedy career. By the age of 19, he had changed his professional name to Albert Brooks, joking that \"the real Albert Einstein changed his name to sound more intelligent\". He quickly became a regular on variety and talk shows during the late 1960s and early 1970s, and was on the writing staff for the ill-fated ABC show Turn-On, which was cancelled after one episode. In 1970–71, he also worked with college friends McKean and Lander (alongside Harry Shearer) as a writer/guest performer on some early material by radio and LP record comedy group The Credibility Gap. Brooks led a new generation of self-reflective baby-boomer comics appearing on NBC\\'s The Tonight Show Starring Johnny Carson. His on-stage persona, that of an egotistical, narcissistic, nervous comic, an ironic showbiz insider who punctured himself before an audience by disassembling his mastery of comedic stagecraft, influenced other post-modern comedians of the 1970s, including Steve Martin, Martin Mull, and Andy Kaufman.\\nAfter two successful comedy albums, Comedy Minus One (1973) and the Grammy Award-nominated A Star Is Bought (1975), Brooks left the stand-up circuit to try his hand as a filmmaker. He had already made his first short film, The Famous Comedians School, a satiric short and an early example of the mockumentary subgenre that was aired in 1972 on the PBS show The Great American Dream Machine.\\nIn 1975, Brooks directed six short films for the first season of NBC\\'s Saturday Night Live. In 1976, he appeared in his first mainstream film role, in Martin Scorsese\\'s landmark Taxi Driver; Scorsese allowed Brooks to improvise much of his dialogue.\\nBrooks directed his first feature film, Real Life, in 1979, which he co-wrote with Harry Shearer and Monica Johnson. The film, in which Brooks (playing a version of himself) films a typical subu'),\n",
       " Document(metadata={'title': 'Religious and philosophical views of Albert Einstein', 'summary': 'Albert Einstein\\'s religious views have been widely studied and often misunderstood. Albert Einstein stated \"I believe in Spinoza\\'s God\". He did not believe in a personal God who concerns himself with fates and actions of human beings, a view which he described as naïve. He clarified, however, that, \"I am not an atheist\", preferring to call himself an agnostic, or a \"religious nonbeliever.\" In other interviews, he stated that he thought that there is a \"lawgiver\" who sets the laws of the universe. Einstein also stated he did not believe in life after death, adding \"one life is enough for me.\" He was closely involved in his lifetime with several humanist groups. Einstein rejected a conflict between science and religion, and held that cosmic religion was necessary for science.', 'source': 'https://en.wikipedia.org/wiki/Religious_and_philosophical_views_of_Albert_Einstein'}, page_content='Albert Einstein\\'s religious views have been widely studied and often misunderstood. Albert Einstein stated \"I believe in Spinoza\\'s God\". He did not believe in a personal God who concerns himself with fates and actions of human beings, a view which he described as naïve. He clarified, however, that, \"I am not an atheist\", preferring to call himself an agnostic, or a \"religious nonbeliever.\" In other interviews, he stated that he thought that there is a \"lawgiver\" who sets the laws of the universe. Einstein also stated he did not believe in life after death, adding \"one life is enough for me.\" He was closely involved in his lifetime with several humanist groups. Einstein rejected a conflict between science and religion, and held that cosmic religion was necessary for science.\\n\\n\\n== Religious beliefs ==\\nAlbert Einstein himself stated \"I\\'m not an atheist, and I don\\'t think I can call myself a pantheist ... I believe in Spinoza\\'s God who reveals himself in the orderly harmony of what exists, not in a God who concerns himself with fates and actions of human beings\". Einstein believed the problem of God was the \"most difficult in the world\"—a question that could not be answered \"simply with yes or no\". He conceded that \"the problem involved is too vast for our limited minds\".\\nEinstein explained his view on the relationship between science, philosophy and religion in his lectures of 1939 and 1941:\\n\\n\"For science can only ascertain what is but not what should be, and outside of its domain value judgments of all kinds remain necessary. Religion, on the other hand, deals only with evaluations of human thought and action: it cannot justifiably speak of facts and relationships between facts.\"\\n\\n\\n=== Early childhood ===\\nEinstein was raised by secular Jewish parents and attended a local Catholic public elementary school in Munich. In his Autobiographical Notes, Einstein wrote that he had gradually lost his faith early in childhood:\\n\\n... I came—though the child of entirely irreligious (Jewish) parents—to a deep religiousness, which, however, reached an abrupt end at the age of twelve. Through the reading of popular scientific books I soon reached the conviction that much in the stories of the Bible could not be true. The consequence was a positively fanatic orgy of freethinking coupled with the impression that youth is intentionally being deceived by the state through lies; it was a crushing impression. Mistrust of every kind of authority grew out of this experience, a skeptical attitude toward the convictions that were alive in any specific social environment—an attitude that has never again left me, even though, later on, it has been tempered by a better insight into the causal connections.\\nIt is quite clear to me that the religious paradise of youth, which was thus lost, was a first attempt to free myself from the chains of the \\'merely personal,\\' from an existence dominated by wishes, hopes, and primitive feelings. Out yonder there was this huge world, which exists independently of us human beings and which stands before us like a great, eternal riddle, at least partially accessible to our inspection and thinking. The contemplation of this world beckoned as a liberation, and I soon noticed that many a man whom I had learned to esteem and to admire had found inner freedom and security in its pursuit. The mental grasp of this extra-personal world within the frame of our capabilities presented itself to my mind, half consciously, half unconsciously, as a supreme goal. Similarly motivated men of the present and of the past, as well as the insights they had achieved, were the friends who could not be lost. The road to this paradise was not as comfortable and alluring as the road to the religious paradise; but it has shown itself reliable, and I have never regretted having chosen it.\\n\\n\\n=== Personal God ===\\nEinstein expressed his skepticism regarding the existence of an anthropomorphic god, often describing this view as \"naïve\" and \"childlike\". In a'),\n",
       " Document(metadata={'title': 'Brain of Albert Einstein', 'summary': \"The brain of Albert Einstein has been a subject of much research and speculation. Albert Einstein's brain was removed within seven and a half hours of his death. His apparent regularities or irregularities in the brain have been used to support various ideas about correlations in neuroanatomy with general or mathematical intelligence. Studies have suggested an increased number of glial cells in Einstein's brain.\", 'source': 'https://en.wikipedia.org/wiki/Brain_of_Albert_Einstein'}, page_content='The brain of Albert Einstein has been a subject of much research and speculation. Albert Einstein\\'s brain was removed within seven and a half hours of his death. His apparent regularities or irregularities in the brain have been used to support various ideas about correlations in neuroanatomy with general or mathematical intelligence. Studies have suggested an increased number of glial cells in Einstein\\'s brain. \\n\\n\\n== Fate of the brain ==\\nEinstein\\'s autopsy was conducted in the lab of Thomas Stoltz Harvey. Shortly after Einstein died in 1955, Harvey removed and weighed the brain at 1230 g. Harvey then took the brain to a lab at the University of Pennsylvania where he dissected it into several pieces. He kept some of the pieces to himself while others were given to leading pathologists. He hoped that cytoarchitectonics, the study of brain cells under a microscope, would reveal useful information. Harvey injected 50% formalin through the internal carotid arteries and afterward suspended the intact brain in 10% formalin. He also photographed the brain from many angles.\\nHarvey dissected the brain into about 240 blocks (each about 1 cm3) and encased the segments in a plastic-like material called collodion. Harvey also removed Einstein\\'s eyes. He gave them to Henry Abrams, Einstein\\'s ophthalmologist.\\nWhether or not Einstein\\'s brain was preserved with his prior consent is a matter of dispute. Ronald Clark\\'s 1979 biography of Einstein states \"he had insisted that his brain should be used for research and that he be cremated.\" More recent research has suggested that the brain was removed and preserved without the permission of either Einstein or his close relatives. Hans Albert Einstein, the physicist\\'s elder son, endorsed the removal after the event. However, he insisted that his father\\'s brain should be used only for research to be published in scientific journals of high standing.\\nIn 1978, Einstein\\'s brain was rediscovered in Harvey\\'s possession by journalist Steven Levy. Its sections had been preserved in alcohol in two large mason jars within a cider box for over 20 years.\\nThe brain was driven across many U.S. states and to Hamilton, Ontario, accompanied by Harvey. Journalist and chauffeur Michael Paterniti wrote about some of the journeying that took place in 1997.\\nIn 2010, Harvey\\'s heirs transferred all of his holdings constituting the remains of Einstein\\'s brain to the National Museum of Health and Medicine. This included 14 photographs of the whole brain prior to sectioning, never before revealed to the public.\\nMore recently, 46 small portions of Einstein\\'s brain were acquired by the Mütter Museum in Philadelphia. In 2013, segments of the brain went on exhibit in the museum\\'s permanent galleries. The exhibit featured thin slices of Einstein\\'s brain, mounted on microscope slides.\\n\\n\\n== Scientific studies ==\\n\\n\\n=== Autopsy ===\\nHarvey had reported that Einstein had no parietal operculum in either hemisphere, but this finding has been disputed. Photographs of the brain show an enlarged Sylvian fissure.\\nIn 1999, further analysis by a team at McMaster University in Hamilton, Ontario revealed that his parietal operculum region in the inferior frontal gyrus in the frontal lobe of the brain was vacant. Also absent was part of a bordering region called the lateral sulcus (Sylvian fissure). Researchers at McMaster University speculated that the vacancy may have enabled neurons in this part of his brain to communicate better. \"This unusual brain anatomy...[missing part of the Sylvian fissure]... may explain why Einstein thought the way he did,\" said Professor Sandra Witelson who led the research published in The Lancet. This study was based on photographs of the whole brain made at autopsy in 1955 by Harvey and not a direct examination of the brain. Einstein himself claimed that he thought visually rather than verbally. Professor Laurie Hall of Cambridge University, commenting on the study, said, \"To say there is a definite link is one brid'),\n",
       " Document(metadata={'title': 'Political views of Albert Einstein', 'summary': 'German-born scientist Albert Einstein was best known during his lifetime for his development of the theory of relativity, his contributions to quantum mechanics, and many other notable achievements in modern physics. However, Einstein\\'s political views also garnered much public interest due to his fame and involvement in political, humanitarian, and academic projects around the world. Einstein was a peace activist and a firm advocate of global federalism and world law. He also wrote: “the population of Europe has grown from 113 million to almost 400 million during the last century… a terrible thought, which could almost make one reconciled to war!”. He favoured the principles of socialism, asserting that it was an ideological system that fixed what he perceived as the inherent societal shortcomings of capitalism. \\nThis became especially apparent in his later life, when he detailed his economic views in a 1949 article titled \"Why Socialism?\" for the independent socialist magazine Monthly Review. However, his view was not entirely uniform: he was critical of the methods employed by Vladimir Lenin and the Bolsheviks during the Russian Revolution, stating that they did not have a \"well-regulated system of government\" and had instead established a \"regime of terror\" over the fallen Russian Empire. His visible position in society allowed him to speak and write frankly, even provocatively, at a time when many people were being silenced across the European continent due to the swift rise of Nazism in Germany. \\nIn January 1933, Adolf Hitler assumed office as Germany\\'s leader while Einstein was visiting the United States. Einstein, an Ashkenazi Jew, was staunchly opposed to the policies of the Nazi government, and after his family was repeatedly harassed by the Gestapo, he renounced his German citizenship and permanently relocated to the United States, becoming an American citizen in 1940. Though he held a generally positive view of the country\\'s culture and values, he frequently objected to the systematic mistreatment of African Americans and became active in their civil rights movement. As a Labor Zionist, Einstein supported the Palestinian Jews of the Yishuv. However, he did not support the establishment of a Jewish state or an Arab state to replace Mandatory Palestine, instead asserting that he would \"much rather see a reasonable agreement reached with the Arabs on the basis of living together in peace\" under the framework of a binational Jewish–Arab state.', 'source': 'https://en.wikipedia.org/wiki/Political_views_of_Albert_Einstein'}, page_content='German-born scientist Albert Einstein was best known during his lifetime for his development of the theory of relativity, his contributions to quantum mechanics, and many other notable achievements in modern physics. However, Einstein\\'s political views also garnered much public interest due to his fame and involvement in political, humanitarian, and academic projects around the world. Einstein was a peace activist and a firm advocate of global federalism and world law. He also wrote: “the population of Europe has grown from 113 million to almost 400 million during the last century… a terrible thought, which could almost make one reconciled to war!”. He favoured the principles of socialism, asserting that it was an ideological system that fixed what he perceived as the inherent societal shortcomings of capitalism. \\nThis became especially apparent in his later life, when he detailed his economic views in a 1949 article titled \"Why Socialism?\" for the independent socialist magazine Monthly Review. However, his view was not entirely uniform: he was critical of the methods employed by Vladimir Lenin and the Bolsheviks during the Russian Revolution, stating that they did not have a \"well-regulated system of government\" and had instead established a \"regime of terror\" over the fallen Russian Empire. His visible position in society allowed him to speak and write frankly, even provocatively, at a time when many people were being silenced across the European continent due to the swift rise of Nazism in Germany. \\nIn January 1933, Adolf Hitler assumed office as Germany\\'s leader while Einstein was visiting the United States. Einstein, an Ashkenazi Jew, was staunchly opposed to the policies of the Nazi government, and after his family was repeatedly harassed by the Gestapo, he renounced his German citizenship and permanently relocated to the United States, becoming an American citizen in 1940. Though he held a generally positive view of the country\\'s culture and values, he frequently objected to the systematic mistreatment of African Americans and became active in their civil rights movement. As a Labor Zionist, Einstein supported the Palestinian Jews of the Yishuv. However, he did not support the establishment of a Jewish state or an Arab state to replace Mandatory Palestine, instead asserting that he would \"much rather see a reasonable agreement reached with the Arabs on the basis of living together in peace\" under the framework of a binational Jewish–Arab state.\\n\\n\\n== Germany ==\\n\\n\\n=== German Empire and World War I ===\\nBorn in Ulm, Einstein was a German citizen from birth. As he grew older, Einstein\\'s pacifism often clashed with the German Empire\\'s militant views at the time. At the age of 17, Einstein renounced his German citizenship and moved to Switzerland to attend college. The loss of Einstein\\'s citizenship allowed him to avoid service in the military, which suited his pacifist views. In response to a Manifesto of the Ninety-Three signed by 93 leading German intellectuals including Max Planck in support of the German war effort, Einstein and three others wrote a counter-manifesto.\\nEinstein accepted a position at the University of Berlin in 1914, returning to Germany where he spent his time during the rest of World War I. Einstein also reacquired his German citizenship. In the years after the war, Einstein was very vocal in his support for Germany. In 1918, Einstein was one of the signatories of the founding proclamation of the German Democratic Party, a liberal party. In 1921, Einstein refused to attend the third Solvay Congress in Belgium, as his German compatriots were excluded. In 1922, Einstein joined a committee sponsored by the League of Nations, but quickly left when the League refused to act on France\\'s occupation of the Ruhr in 1923. As a member of the German League of Human Rights, Einstein worked hard to repair relations between Germany and France.\\n\\n\\n=== Rise of Adolf Hitler\\'s Nazi Party ===\\n\\nEinstein moved to the United St'),\n",
       " Document(metadata={'title': 'Albert Einstein College of Medicine', 'summary': 'The Albert Einstein College of Medicine is a private medical school in New York City. Founded in 1953, Einstein operates as an independent degree-granting institution as part of the integrated healthcare Montefiore Health System (Montefiore Medicine) and also has affiliations with Jacobi Medical Center and Yeshiva University.\\nEinstein offers a M.D. program, a Ph.D. program in the biomedical sciences and clinical investigation, and two Master of Science (M.S.) degrees. Admission to Einstein\\'s MD program is amongst the most competitive in the United States, with an acceptance rate of 1.87% in 2024. Einstein was one of the original three Medical Scientist Training Programs—which award MD/PhD degrees—to be awarded funding from the National Institutes of Health in 1964, and has received continuous funding since.\\nThe college arose from plans by Samuel Belkin in the 1940s and was named for physicist Albert Einstein. The college was established expressly to provide medical training to \"students of all creeds and races\". Scientific feats achieved at Einstein include the first coronary artery bypass surgery. The Montefiore Health System acquired the school in 2015. Following a $1 billion donation to the school by Ruth Gottesman in 2024, the school became tuition-free for all MD students.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_College_of_Medicine'}, page_content='The Albert Einstein College of Medicine is a private medical school in New York City. Founded in 1953, Einstein operates as an independent degree-granting institution as part of the integrated healthcare Montefiore Health System (Montefiore Medicine) and also has affiliations with Jacobi Medical Center and Yeshiva University.\\nEinstein offers a M.D. program, a Ph.D. program in the biomedical sciences and clinical investigation, and two Master of Science (M.S.) degrees. Admission to Einstein\\'s MD program is amongst the most competitive in the United States, with an acceptance rate of 1.87% in 2024. Einstein was one of the original three Medical Scientist Training Programs—which award MD/PhD degrees—to be awarded funding from the National Institutes of Health in 1964, and has received continuous funding since.\\nThe college arose from plans by Samuel Belkin in the 1940s and was named for physicist Albert Einstein. The college was established expressly to provide medical training to \"students of all creeds and races\". Scientific feats achieved at Einstein include the first coronary artery bypass surgery. The Montefiore Health System acquired the school in 2015. Following a $1 billion donation to the school by Ruth Gottesman in 2024, the school became tuition-free for all MD students.\\n\\n\\n== History ==\\n\\nAs early as 1945, Yeshiva University president Samuel Belkin began planning a new medical school. Six years later, Belkin and New York City Mayor Vincent Impellitteri entered into an agreement to begin its construction with funding from Henry H. Minskoff and Phillip Stollman. Around the same time, physicist and humanitarian Albert Einstein sent a letter to Belkin. He remarked that such an endeavor would be \"unique\" in that the school would \"welcome students of all creeds and races\". Two years later, on his 74th birthday, March 14, 1953, Albert Einstein agreed to have his name attached to the medical school.\\nThe first classes began September 12, 1955, with 56 students. Irving London was the founding chair of the department of medicine. It was the first new medical school to open in New York City since 1897. The Sue Golding Graduate Division was established in 1957 to offer Doctor of Philosophy degrees in biomedical disciplines. The Medical Scientist Training Program, a combined MD–PhD program, was established in 1964. The Clinical Research Training Program, which confers Master of Science degrees in clinical research methods, began in July 1998.\\nThe world\\'s first coronary artery bypass surgery was performed May 2, 1960 at Einstein by a team led by Robert H. Goetz and the thoracic surgeon, Michael Rohman with the assistance of Jordan Haller and Ronald Dee.\\nIn February 2015, Yeshiva University announced the transfer of ownership of Einstein to the Montefiore Health System, in order to eliminate a large deficit from the university\\'s financial statements. The medical school accounted for approximately two-thirds of the university\\'s annual operating deficits, which had reached about $100 million before the announcement. On September 9, 2015, the agreement between Yeshiva and Montefiore was finalized, and financial and operational control of Albert Einstein College of Medicine was transferred to Montefiore. Yeshiva University continued to grant Einstein\\'s degrees until 2018, as the medical school achieved independent degree-granting authority in the spring of 2019.\\nIn February 2024, Ruth Gottesman, who had been a long-time professor at the medical school and is head of the board of trustees, donated $1 billion to the school to make free tuition available to all students in perpetuity.\\n\\n\\n== Student body ==\\nThere are 183 first-year medical students in the Class of 2025. 9,773 people applied for seats, and 1,200 were interviewed. 60% of the class identify as women and 20% identify with groups underrepresented in medicine. Ages range from 21 to 34 with an average age of 23.5. 16% of students were born outside the United States and students come fr'),\n",
       " Document(metadata={'title': 'Einstein field equations', 'summary': \"In the general theory of relativity, the Einstein field equations (EFE; also known as Einstein's equations) relate the geometry of spacetime to the distribution of matter within it.\\nThe equations were published by Albert Einstein in 1915 in the form of a tensor equation which related the local spacetime curvature (expressed by the Einstein tensor) with the local energy, momentum and stress within that spacetime (expressed by the stress–energy tensor).\\nAnalogously to the way that electromagnetic fields are related to the distribution of charges and currents via Maxwell's equations, the EFE relate the spacetime geometry to the distribution of mass–energy, momentum and stress, that is, they determine the metric tensor of spacetime for a given arrangement of stress–energy–momentum in the spacetime. The relationship between the metric tensor and the Einstein tensor allows the EFE to be written as a set of nonlinear partial differential equations when used in this way. The solutions of the EFE are the components of the metric tensor. The inertial trajectories of particles and radiation (geodesics) in the resulting geometry are then calculated using the geodesic equation.\\nAs well as implying local energy–momentum conservation, the EFE reduce to Newton's law of gravitation in the limit of a weak gravitational field and velocities that are much less than the speed of light.\\nExact solutions for the EFE can only be found under simplifying assumptions such as symmetry. Special classes of exact solutions are most often studied since they model many gravitational phenomena, such as rotating black holes and the expanding universe. Further simplification is achieved in approximating the spacetime as having only small deviations from flat spacetime, leading to the linearized EFE. These equations are used to study phenomena such as gravitational waves.\", 'source': 'https://en.wikipedia.org/wiki/Einstein_field_equations'}, page_content=\"In the general theory of relativity, the Einstein field equations (EFE; also known as Einstein's equations) relate the geometry of spacetime to the distribution of matter within it.\\nThe equations were published by Albert Einstein in 1915 in the form of a tensor equation which related the local spacetime curvature (expressed by the Einstein tensor) with the local energy, momentum and stress within that spacetime (expressed by the stress–energy tensor).\\nAnalogously to the way that electromagnetic fields are related to the distribution of charges and currents via Maxwell's equations, the EFE relate the spacetime geometry to the distribution of mass–energy, momentum and stress, that is, they determine the metric tensor of spacetime for a given arrangement of stress–energy–momentum in the spacetime. The relationship between the metric tensor and the Einstein tensor allows the EFE to be written as a set of nonlinear partial differential equations when used in this way. The solutions of the EFE are the components of the metric tensor. The inertial trajectories of particles and radiation (geodesics) in the resulting geometry are then calculated using the geodesic equation.\\nAs well as implying local energy–momentum conservation, the EFE reduce to Newton's law of gravitation in the limit of a weak gravitational field and velocities that are much less than the speed of light.\\nExact solutions for the EFE can only be found under simplifying assumptions such as symmetry. Special classes of exact solutions are most often studied since they model many gravitational phenomena, such as rotating black holes and the expanding universe. Further simplification is achieved in approximating the spacetime as having only small deviations from flat spacetime, leading to the linearized EFE. These equations are used to study phenomena such as gravitational waves.\\n\\n\\n== Mathematical form ==\\n\\nThe Einstein field equations (EFE) may be written in the form:\\n\\n  \\n    \\n      \\n        \\n          G\\n          \\n            μ\\n            ν\\n          \\n        \\n        +\\n        Λ\\n        \\n          g\\n          \\n            μ\\n            ν\\n          \\n        \\n        =\\n        κ\\n        \\n          T\\n          \\n            μ\\n            ν\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle G_{\\\\mu \\\\nu }+\\\\Lambda g_{\\\\mu \\\\nu }=\\\\kappa T_{\\\\mu \\\\nu }}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        \\n          G\\n          \\n            μ\\n            ν\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle G_{\\\\mu \\\\nu }}\\n  \\n is the Einstein tensor, \\n  \\n    \\n      \\n        \\n          g\\n          \\n            μ\\n            ν\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle g_{\\\\mu \\\\nu }}\\n  \\n is the metric tensor, \\n  \\n    \\n      \\n        \\n          T\\n          \\n            μ\\n            ν\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle T_{\\\\mu \\\\nu }}\\n  \\n is the stress–energy tensor, \\n  \\n    \\n      \\n        Λ\\n      \\n    \\n    {\\\\displaystyle \\\\Lambda }\\n  \\n is the cosmological constant and \\n  \\n    \\n      \\n        κ\\n      \\n    \\n    {\\\\displaystyle \\\\kappa }\\n  \\n is the Einstein gravitational constant.\\nThe Einstein tensor is defined as\\n\\n  \\n    \\n      \\n        \\n          G\\n          \\n            μ\\n            ν\\n          \\n        \\n        =\\n        \\n          R\\n          \\n            μ\\n            ν\\n          \\n        \\n        −\\n        \\n          \\n            1\\n            2\\n          \\n        \\n        R\\n        \\n          g\\n          \\n            μ\\n            ν\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle G_{\\\\mu \\\\nu }=R_{\\\\mu \\\\nu }-{\\\\frac {1}{2}}Rg_{\\\\mu \\\\nu },}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        \\n          R\\n          \\n            μ\\n            ν\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle R_{\\\\mu \\\\nu }}\\n  \\n is the Ricci curvature tensor, and \\n  \\n    \\n      \\n        R\\n      \\n    \\n    {\\\\displaystyle R}\\n  \\n is the scalar curvature.  This is a symmetric second-degree tensor that depends on only the metric tensor and its first and second derivatives.\\nThe Einstein gravitational constant is defin\"),\n",
       " Document(metadata={'title': 'Bose–Einstein condensate', 'summary': 'In condensed matter physics, a Bose–Einstein condensate (BEC) is a state of matter that is typically formed when a gas of bosons at very low densities is cooled to temperatures very close to absolute zero, i.e., 0 K (−273.15 °C; −459.67 °F). Under such conditions, a large fraction of bosons occupy the lowest quantum state, at which microscopic quantum-mechanical phenomena, particularly wavefunction interference, become apparent macroscopically. \\nMore generally, condensation refers to the appearance of macroscopic occupation of one or several states: for example, in BCS theory, a superconductor is a condensate of Cooper pairs. As such, condensation can be associated with phase transition, and the macroscopic occupation of the state is the order parameter.\\nBose–Einstein condensate was first predicted, generally, in 1924–1925 by Albert Einstein, crediting a pioneering paper by Satyendra Nath Bose on the new field now known as quantum statistics. In 1995, the Bose–Einstein condensate was created by Eric Cornell and Carl Wieman of the University of Colorado Boulder using rubidium atoms; later that year, Wolfgang Ketterle of MIT produced a BEC using sodium atoms. In 2001 Cornell, Wieman, and Ketterle shared the Nobel Prize in Physics \"for the achievement of Bose–Einstein condensation in dilute gases of alkali atoms, and for early fundamental studies of the properties of the condensates\".\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_condensate'}, page_content='In condensed matter physics, a Bose–Einstein condensate (BEC) is a state of matter that is typically formed when a gas of bosons at very low densities is cooled to temperatures very close to absolute zero, i.e., 0 K (−273.15 °C; −459.67 °F). Under such conditions, a large fraction of bosons occupy the lowest quantum state, at which microscopic quantum-mechanical phenomena, particularly wavefunction interference, become apparent macroscopically. \\nMore generally, condensation refers to the appearance of macroscopic occupation of one or several states: for example, in BCS theory, a superconductor is a condensate of Cooper pairs. As such, condensation can be associated with phase transition, and the macroscopic occupation of the state is the order parameter.\\nBose–Einstein condensate was first predicted, generally, in 1924–1925 by Albert Einstein, crediting a pioneering paper by Satyendra Nath Bose on the new field now known as quantum statistics. In 1995, the Bose–Einstein condensate was created by Eric Cornell and Carl Wieman of the University of Colorado Boulder using rubidium atoms; later that year, Wolfgang Ketterle of MIT produced a BEC using sodium atoms. In 2001 Cornell, Wieman, and Ketterle shared the Nobel Prize in Physics \"for the achievement of Bose–Einstein condensation in dilute gases of alkali atoms, and for early fundamental studies of the properties of the condensates\".\\n\\n\\n== History ==\\n\\nBose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons), in which he derived Planck\\'s quantum radiation law without any reference to classical physics. Einstein was impressed, translated the paper himself from English to German and submitted it for Bose to the Zeitschrift für Physik, which published it in 1924. (The Einstein manuscript, once believed to be lost, was found in a library at Leiden University in 2005.) Einstein then extended Bose\\'s ideas to matter in two other papers. The result of their efforts is the concept of a Bose gas, governed by Bose–Einstein statistics, which describes the statistical distribution of identical particles with integer spin, now called bosons. Bosons are allowed to share a quantum state. Einstein proposed that cooling bosonic atoms to a very low temperature would cause them to fall (or \"condense\") into the lowest accessible quantum state, resulting in a new form of matter. Bosons include the photon, polaritons, magnons, some atoms and molecules (depending on the number of nucleons, see #Isotopes) such as atomic hydrogen, helium-4, lithium-7, rubidium-87 or strontium-84.\\nIn 1938, Fritz London proposed the BEC as a mechanism for superfluidity in 4He and superconductivity.\\nThe quest to produce a Bose–Einstein condensate in the laboratory was stimulated by a paper published in 1976 by two program directors at the National Science Foundation (William Stwalley and Lewis Nosanow), proposing to use spin-polarized atomic hydrogen to produce a gaseous BEC. This led to the immediate pursuit of the idea by four independent research groups; these were led by Isaac Silvera (University of Amsterdam), Walter Hardy (University of British Columbia), Thomas Greytak (Massachusetts Institute of Technology) and David Lee (Cornell University). However, cooling atomic hydrogen turned out to be technically difficult, and Bose-Einstein condensation of atomic hydrogen was only realized in 1998.\\nOn 5 June 1995, the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST–JILA lab, in a gas of rubidium atoms cooled to 170 nanokelvins (nK). Shortly thereafter, Wolfgang Ketterle at MIT produced a Bose–Einstein Condensate in a gas of sodium atoms. For their achievements Cornell, Wieman, and Ketterle received the 2001 Nobel Prize in Physics. Bose-Einstein condensation of alkali gases is easier because they can be pre-cooled with laser cooling techniques, unlike atomic hydrogen at the time, which give a significant head start when'),\n",
       " Document(metadata={'title': 'Mileva Marić', 'summary': 'Mileva Marić (Serbian Cyrillic: Милева Марић, pronounced [milěːva mǎːritɕ]; 19 December 1875 – 4 August 1948), sometimes called Mileva Marić-Einstein (Милева Марић-Ајнштајн, Mileva Marić-Ajnštajn), was a Serbian physicist and mathematician. She showed intellectual aptitude from a young age and studied at Zürich Polytechnic in a highly male dominated field, after having studied medicine for one semester at Zürich University. Her studies included differential and integral calculus, descriptive and projective geometry, mechanics, theoretical physics, applied physics, experimental physics, and astronomy. One of her study colleagues at university was her future husband Albert Einstein, to whose early work Marić is thought by some to have contributed (in particular the Annus Mirabilis papers). \\nIn 1921 Mileva Marić received the Nobel Prize money her ex-husband was awarded as a part of their divorce agreement to support their sons; she had access to the interest.', 'source': 'https://en.wikipedia.org/wiki/Mileva_Mari%C4%87'}, page_content='Mileva Marić (Serbian Cyrillic: Милева Марић, pronounced [milěːva mǎːritɕ]; 19 December 1875 – 4 August 1948), sometimes called Mileva Marić-Einstein (Милева Марић-Ајнштајн, Mileva Marić-Ajnštajn), was a Serbian physicist and mathematician. She showed intellectual aptitude from a young age and studied at Zürich Polytechnic in a highly male dominated field, after having studied medicine for one semester at Zürich University. Her studies included differential and integral calculus, descriptive and projective geometry, mechanics, theoretical physics, applied physics, experimental physics, and astronomy. One of her study colleagues at university was her future husband Albert Einstein, to whose early work Marić is thought by some to have contributed (in particular the Annus Mirabilis papers). \\nIn 1921 Mileva Marić received the Nobel Prize money her ex-husband was awarded as a part of their divorce agreement to support their sons; she had access to the interest.\\n\\n\\n== Biography ==\\nOn 19 December 1875, Mileva Marić was born into a wealthy family in Titel in Austria-Hungary (today Serbia) as the eldest of three children of Miloš Marić (1846–1922) and Marija Ružić-Marić (1847–1935). \\nShe began her secondary education in 1886 at a high school for girls in Újvidék (today Novi Sad, Serbia), but changed the following year to the Mitrovica Gymnasium in Szávaszentdemeter (today Sremska Mitrovica). Beginning in 1890, Marić attended the Royal Serbian Grammar School in Šabac. In 1891, her father obtained special permission to enroll Marić as a private student at the all-male Royal Classical High School in Zagreb. Her mathematics teacher was Vladimir Varićak. She passed the entrance exam and entered the tenth grade in 1892. She won special permission to attend physics lectures in February 1894 and passed the final exams in September 1894. Her highest grades were in mathematics and physics, both \"very good\", one grade below the highest \"excellent\". That year she fell seriously ill and decided to move to Switzerland, where on 14 November, she started at the \"Girls High School\" in Zurich. In 1896, she passed her Matura-Exam, and started studying medicine at the University of Zurich for one semester.\\nIn the fall of 1896, Marić switched to the Zurich Polytechnic (later Eidgenössische Technische Hochschule, ETH), having passed the mathematics entrance examination with an average grade of 4.25 (scale 1–6). She enrolled for the diploma course to teach physics and mathematics in secondary schools (section VIA) at the same time as Albert Einstein. She was the only woman in her group of six students, and the fifth woman to enter that section. Marić and Einstein became close friends quite soon. In October, Marić went to Heidelberg to study at Heidelberg University for the winter semester 1897/98, attending physics and mathematics lectures as an auditor. She rejoined the Zurich Polytechnic in April 1898, where her studies included the following courses: differential and integral calculus, descriptive and projective geometry, mechanics, theoretical physics, applied physics, experimental physics, and astronomy.\\nShe sat for the intermediate diploma examinations in 1899, one year later than the other students in her group. Her grade average of 5.05 (scale 1–6) placed her fifth out of the six students taking the examinations that year. Marić\\'s grade in physics was 5.5 (the same as Einstein\\'s). In 1900, she failed the final teaching diploma examinations with a grade average of 4.00, having obtained only grade 2.5 in the mathematics component (theory of functions).\\nMarić\\'s academic career was disrupted in May 1901 on a short holiday in Italy when she became pregnant by Einstein. When three months pregnant, she resat the diploma examination, but failed for the second time without improving her grade. She discontinued work on her diploma dissertation that she had hoped to develop into a PhD thesis under the supervision of the physics professor Heinrich Weber.\\nShe went'),\n",
       " Document(metadata={'title': 'Albert Einstein Award', 'summary': 'The Albert Einstein Award was an award in theoretical physics, given periodically from 1951 to 1979, that was established to recognize high achievement in the natural sciences. It was endowed by the Lewis and Rosa Strauss Memorial Fund in honor of Albert Einstein\\'s 70th birthday. It was first awarded in 1951 and, in addition to a gold medal of Einstein by sculptor Gilroy Roberts, it also included a prize money of $15,000, which was later reduced to $5,000. The winner was selected by a committee (the first of which consisted of Einstein, Oppenheimer, von Neumann, and Weyl) of the Institute for Advanced Study, which administered the award. Lewis L. Strauss used to be one of the trustees of the institute.\\nThis award should not be confused with many others named after the famous physicist, such as the Albert Einstein World Award of Science given by the World Cultural Council (since 1984), the Albert Einstein Medal given by the Albert Einstein Society (since 1979), nor with the Hans Albert Einstein Award, named after his son and given by the American Society of Civil Engineers (since 1988). It was established much earlier than these, while Einstein was still alive and was a professor at the Institute for Advanced Study. It has been called \"the highest of its kind in the United States\" by The New York Times. Some considered it as \"the prestigious equivalent of a Nobel Prize\".', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_Award'}, page_content='The Albert Einstein Award was an award in theoretical physics, given periodically from 1951 to 1979, that was established to recognize high achievement in the natural sciences. It was endowed by the Lewis and Rosa Strauss Memorial Fund in honor of Albert Einstein\\'s 70th birthday. It was first awarded in 1951 and, in addition to a gold medal of Einstein by sculptor Gilroy Roberts, it also included a prize money of $15,000, which was later reduced to $5,000. The winner was selected by a committee (the first of which consisted of Einstein, Oppenheimer, von Neumann, and Weyl) of the Institute for Advanced Study, which administered the award. Lewis L. Strauss used to be one of the trustees of the institute.\\nThis award should not be confused with many others named after the famous physicist, such as the Albert Einstein World Award of Science given by the World Cultural Council (since 1984), the Albert Einstein Medal given by the Albert Einstein Society (since 1979), nor with the Hans Albert Einstein Award, named after his son and given by the American Society of Civil Engineers (since 1988). It was established much earlier than these, while Einstein was still alive and was a professor at the Institute for Advanced Study. It has been called \"the highest of its kind in the United States\" by The New York Times. Some considered it as \"the prestigious equivalent of a Nobel Prize\".\\n\\n\\n== Recipients ==\\n\\n\\n== See also ==\\nList of physics awards\\nList of awards named after people\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nInformation on the Albert Einstein Award from the Jewish-American Hall of Fame\\nInformation about and photos of the medal awarded to Kurt Godel on the website of the Institute for Advanced Study, last accessed 2020-04-18.'),\n",
       " Document(metadata={'title': 'Albert Einstein Institution', 'summary': \"The Albert Einstein Institution (AEI) is a non-profit organization specializing in the study of the methods of nonviolent resistance in conflict. It was founded by scholar Gene Sharp in 1983, and named after Albert Einstein.\\nUntil 2000, the institute provided funding for Einstein Institution Fellowships for scholars, sometimes referred to as Einstein Fellows, and was also the funding body for the Program on Nonviolent Sanctions in Conflict and Defense at Harvard's Center for International Affairs.\\nJamila Raqib has been executive director since 2005.\", 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_Institution'}, page_content='The Albert Einstein Institution (AEI) is a non-profit organization specializing in the study of the methods of nonviolent resistance in conflict. It was founded by scholar Gene Sharp in 1983, and named after Albert Einstein.\\nUntil 2000, the institute provided funding for Einstein Institution Fellowships for scholars, sometimes referred to as Einstein Fellows, and was also the funding body for the Program on Nonviolent Sanctions in Conflict and Defense at Harvard\\'s Center for International Affairs.\\nJamila Raqib has been executive director since 2005.\\n\\n\\n== History ==\\nThe research institute is named after the physicist Albert Einstein, who was a committed pacifist, although not an \"absolute pacifist\"; he recognized that pacifism would not work against Hitler in 1933. It was founded by political scientist Gene Sharp, whose first book, about the methods of Indian pacifist Gandhi, included an article on nonviolence signed by Einstein as a preface.\\nThe AEI was incorporated in July 1983, two months after the Program on Nonviolent Sanctions in Conflict and Defense was created at the Center for International Affairs (CFIA) (now the Weatherhead Center for International Affairs, or WCFIA) at Harvard University. This program operated as a research division under the framework and policies of the center, with its focus the use of nonviolent sanctions as a substitute for violent interventions. The Program provided grants or fellowships for scholars in residence, as well as conducting seminars and conferences. For the first few years, the Program at the CFIA lobbied for funding itself, as well as obtaining some funding from the AEI; after 1987 policy changes were made to reduce confusion and the AEI became solely responsible for raising the funds to support the CFIA Program as well as its own activities.\\nAround 2004, one of its major donors, former student of Sharp and co-founder of International Center on Nonviolent Conflict in 2002, businessman Peter Ackerman, withdrew his funding, and Sharp started running the institute out of his home in Boston.\\nJamila Raqib joined AEI in 2002, at first managing the promotion of its publications and translations. In 2005 she became its executive director, and in 2009  collaborated with Sharp to publish Self-Liberation: A Guide to Strategic Planning for Action to End a Dictatorship or Other Oppression, which has been translated into several languages.\\nSharp remained as senior scholar at AEI until his death in 2018.\\n\\n\\n== Governance ==\\nThe articles of incorporation stated that institution is an independent non-profit organization, to be publicly funded and to act as \"a grant-making and grant seeking organization\".\\nAs of April 2021, Jamila Raqib is executive director, and Cornelia Sargent is chair.\\n\\n\\n== Aims and work ==\\nThe institution \"is committed to the defense of freedom, democracy, and the reduction of political violence through the use of nonviolent action\", and looks at ways by which nonviolent means can be employed to deal with problems such as \"aggression, dictatorship, genocide and oppression.\\nIn order to achieve its aims, it encourages research and policy studies on the methods of nonviolent action; shares the results of this research with the public; and engages with groups that are in conflict, about the possible use of nonviolent action as a strategy. It prepares, translates, presents and publishes educational resources, and publishes books, pamphlets, conference proceedings and other materials.\\nIt has consulted with pro-democracy groups from countries such as the Baltic states, Burma, Equatorial Guinea, Iran, Iraq Serbia,  Thailand, Venezuela, and Zimbabwe. and the Occupied Palestinian Territories.\\n\\n\\n== Fellowships ==\\nThe Albert Einstein Institution provided a number of Einstein Institution Fellowships to scholars working on various aspects of nonviolent struggle. This was an honorary position, and the Einstein Institution Fellows were either paid only a modest stipend, or not at all in the fir'),\n",
       " Document(metadata={'title': 'Albert Einstein House', 'summary': 'The Albert Einstein House at 112 Mercer Street in Princeton, Mercer County, New Jersey, United States, was the home of Albert Einstein from 1935 until his death in 1955. His second wife, Elsa Einstein, died in 1936 while living in this house.', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_House'}, page_content='The Albert Einstein House at 112 Mercer Street in Princeton, Mercer County, New Jersey, United States, was the home of Albert Einstein from 1935 until his death in 1955. His second wife, Elsa Einstein, died in 1936 while living in this house.\\n\\n\\n== History ==\\nThe house was built in 1838, as it originally stood on Alexander Street where Stuart Hall of the Princeton Theological Seminary was built in that year, also displacing the house now at 108 Mercer. The home is a simple pattern-book cottage and not in itself of unusual significance.:\\u200a2\\u200a Elsa Einstein purchased the home from Mary Clark Marden on July 24, 1935, for an undisclosed sum according to the deed which was recorded by the Mercer County Clerk\\'s Office on August 1, 1935. For many years, Albert Einstein lived in the house with three women: his sister Maja, his step-daughter Margot Einstein-Marianoff (1899–1986), and his secretary Helen Dukas.\\nAlbert Einstein reportedly requested that this house not be made a museum, and the family did not want it to be recognized as such. Nonetheless, it was added to the National Register of Historic Places and further designated a U.S. National Historic Landmark in 1976.\\n\\nAfter Albert Einstein, the house was owned by his sculptor step-daughter Margot Einstein until her death in 1986.\\nThe house was owned by Eric Maskin and his family until 2012. He was the Albert O. Hirschman Professor in the School of Social Science at the Institute for Advanced Study in Princeton until 2011, and the 2007 Nobel Prize winner with two others. He is currently a professor of economics at Harvard University. Previously it was occupied by 2004 Nobel prize winner physicist Frank Wilczek when he was a professor in IAS between 1989 and 2001. Reportedly he requested the house from the IAS as his condition to move to Princeton, and he had been holding evening seminars in the house for graduate students. The house is now a private residence even though it is owned by IAS, and is not open to the public. There is no historical marker explaining the house\\'s significance; however, there are strategically placed \"Private Residence\" signs around the house.\\nThe house is 3,674 square feet, and includes only one bedroom and two baths. In 2012 it was purchased for $1,417,500 by the Institute for Advanced Study. It is on a half-acre parcel that extends 446 feet from the street.\\n\\n\\n== See also ==\\nNational Register of Historic Places listings in Mercer County, New Jersey\\nElsa Einstein, Albert Einstein\\'s wife, who lived here during her last days\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nMercer Hill Historic District Organization\\nPhotograph of Amanda Gefter standing in front of the Albert Einstein House\\nLützeler, Paul Michael (2011). \"Photo of 112 Mercer Street (photographs following page 240)\". Hermann Broch: eine Biographie. Suhrkamp Verlag. ISBN 9783518751015.'),\n",
       " Document(metadata={'title': 'Bose–Einstein statistics', 'summary': 'In quantum statistics, Bose–Einstein statistics (B–E statistics) describes one of two possible ways in which a collection of non-interacting identical particles may occupy a set of available discrete energy states at thermodynamic equilibrium. The aggregation of particles in the same state, which is a characteristic of particles obeying Bose–Einstein statistics, accounts for the cohesive streaming of laser light and the frictionless creeping of superfluid helium. The theory of this behaviour was developed (1924–25) by Satyendra Nath Bose, who recognized that a collection of identical and indistinguishable particles can be distributed in this way. The idea was later adopted and extended by Albert Einstein in collaboration with Bose.\\nBose–Einstein statistics apply only to particles that do not follow the Pauli exclusion principle restrictions. Particles that follow Bose-Einstein statistics are called bosons, which have integer values of spin. In contrast, particles that follow Fermi-Dirac statistics are called fermions and have half-integer spins.', 'source': 'https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_statistics'}, page_content='In quantum statistics, Bose–Einstein statistics (B–E statistics) describes one of two possible ways in which a collection of non-interacting identical particles may occupy a set of available discrete energy states at thermodynamic equilibrium. The aggregation of particles in the same state, which is a characteristic of particles obeying Bose–Einstein statistics, accounts for the cohesive streaming of laser light and the frictionless creeping of superfluid helium. The theory of this behaviour was developed (1924–25) by Satyendra Nath Bose, who recognized that a collection of identical and indistinguishable particles can be distributed in this way. The idea was later adopted and extended by Albert Einstein in collaboration with Bose.\\nBose–Einstein statistics apply only to particles that do not follow the Pauli exclusion principle restrictions. Particles that follow Bose-Einstein statistics are called bosons, which have integer values of spin. In contrast, particles that follow Fermi-Dirac statistics are called fermions and have half-integer spins.\\n\\n\\n== Bose–Einstein distribution ==\\nAt low temperatures, bosons behave differently from fermions (which obey the Fermi–Dirac statistics) in a way that an unlimited number of them can \"condense\" into the same energy state. This apparently unusual property also gives rise to the special state of matter – the Bose–Einstein condensate. Fermi–Dirac and Bose–Einstein statistics apply when quantum effects are important and the particles are \"indistinguishable\". Quantum effects appear if the concentration of particles satisfies\\n\\n  \\n    \\n      \\n        \\n          \\n            N\\n            V\\n          \\n        \\n        ≥\\n        \\n          n\\n          \\n            q\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {N}{V}}\\\\geq n_{\\\\text{q}},}\\n  \\n\\nwhere N is the number of particles, V is the volume, and nq is the quantum concentration, for which the interparticle distance is equal to the thermal de Broglie wavelength, so that the wavefunctions of the particles are barely overlapping.\\nFermi–Dirac statistics applies to fermions (particles that obey the Pauli exclusion principle), and Bose–Einstein statistics applies to bosons. As the quantum concentration depends on temperature, most systems at high temperatures obey the classical (Maxwell–Boltzmann) limit, unless they also have a very high density, as for a white dwarf. Both Fermi–Dirac and Bose–Einstein become Maxwell–Boltzmann statistics at high temperature or at low concentration.\\nBose–Einstein statistics was introduced for photons in 1924 by Bose and generalized to atoms by Einstein in 1924–25.\\nThe expected number of particles in an energy state i for Bose–Einstein statistics is:\\n\\nwith εi > μ and where ni is the occupation number (the number of particles) in state i, \\n  \\n    \\n      \\n        \\n          g\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle g_{i}}\\n  \\n is the degeneracy of energy level i, εi is the energy of the i-th state, μ is the chemical potential (zero for a photon gas), kB is the Boltzmann constant, and T is the absolute temperature.\\nThe variance of this distribution \\n  \\n    \\n      \\n        V\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle V(n)}\\n  \\n is calculated directly from the expression above for the average number.\\n\\n  \\n    \\n      \\n        V\\n        (\\n        n\\n        )\\n        =\\n        k\\n        T\\n        \\n          \\n            ∂\\n            \\n              ∂\\n              μ\\n            \\n          \\n        \\n        \\n          \\n            \\n              \\n                n\\n                ¯\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n        =\\n        ⟨\\n        n\\n        ⟩\\n        (\\n        1\\n        +\\n        ⟨\\n        n\\n        ⟩\\n        )\\n        =\\n        \\n          \\n            \\n              n\\n              ¯\\n            \\n          \\n        \\n        +\\n        \\n          \\n            \\n              \\n                n\\n   '),\n",
       " Document(metadata={'title': 'Albert Einstein Memorial', 'summary': 'The Albert Einstein Memorial is a monumental bronze statue by sculptor Robert Berks, depicting Albert Einstein seated with manuscript papers in hand. It is located in central Washington, D.C., United States, in a grove of trees at the southwest corner of the grounds of the National Academy of Sciences at 2101 Constitution Avenue N.W., near the Vietnam Veterans Memorial. Two replicas exist at the Israel Academy of Sciences and Humanities and the Georgia Institute of Technology.', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_Memorial'}, page_content='The Albert Einstein Memorial is a monumental bronze statue by sculptor Robert Berks, depicting Albert Einstein seated with manuscript papers in hand. It is located in central Washington, D.C., United States, in a grove of trees at the southwest corner of the grounds of the National Academy of Sciences at 2101 Constitution Avenue N.W., near the Vietnam Veterans Memorial. Two replicas exist at the Israel Academy of Sciences and Humanities and the Georgia Institute of Technology.\\n\\n\\n== Life ==\\nThe memorial, situated in an elm and holly grove in the southwest corner of the grounds of the National Academy of Sciences, was unveiled at the Academy\\'s annual meeting, April 22, 1979, in honor of the centennial of Einstein\\'s birth. At the dedication ceremony, physicist John Archibald Wheeler described the statue as \"a monument to the man who united space and time into space-time...a remembrance of the man who taught us...that the universe does not go on from everlasting to everlasting, but begins with a bang.\" The memorial is a popular spot for tourists visiting the national mall to pose for pictures.\\n\\nThe statue depicts Einstein seated in casual repose on a three-step bench of  Mount Airy (North Carolina) white granite. The bronze figure weighs approximately 4 tons and is 12 feet in height.  The monument is supported by three caissons, totaling 135 tons, sunk in bedrock to a depth of 23 to 25 feet., It was cast at Modern Art Foundry, Astoria Queens, NY.\\nThe sculptor, Robert Berks, known for his portrait busts and statues (John F. Kennedy at the Kennedy Center; Mary McLeod Bethune in Lincoln Park, Washington, D.C.), based the work on a bust of Einstein he sculpted from life in 1953 at Einstein\\'s Princeton home. Landscape architect James A. Van Sweden designed the monument landscaping.\\nEinstein was elected a foreign associate of the National Academy of Sciences in 1922, the year after he won the Nobel Prize in physics, and became a member of the Academy in 1942, two years after he became a naturalized American citizen.\\nBerks created two replicas of his 1979 monument. One of the replicas can presently be viewed in the academy garden of the Israel Academy of Sciences and Humanities; another on the campus of the Georgia Institute of Technology in Atlanta, Georgia.\\n\\n\\n== Platform ==\\nThe statue and bench are at one side of a circular dais, 28 feet (8.5 m) in diameter, made from emerald-pearl granite from Larvik, Norway.  Embedded in the dais are more than 2,700 metal studs representing the location of astronomical objects, including the sun, moon, planets, 4 asteroids, 5 galaxies, 10 quasars, and many stars at noon on April 22, 1979, when the memorial was dedicated.  The studs are different sizes to denote the apparent magnitude of the relevant object, and different studs denote binary stars, spectroscopic binaries, pulsars, globular clusters, open clusters, and quasars.  The celestial objects were accurately positioned by astronomers at the U.S. Naval Observatory. Familiar constellations are marked on the map for easy identification.\\nTo a visitor standing at the center of the dais, Einstein appears to be making direct eye contact, and any spoken words are notably amplified.\\n\\n\\n== Description ==\\nEngraved as though written on the papers held in the statue\\'s left hand are three equations, summarizing three of Einstein\\'s most important scientific advances:\\n\\n  \\n    \\n      \\n        \\n          R\\n          \\n            μ\\n            ν\\n          \\n        \\n        −\\n        \\n          \\n            1\\n            2\\n          \\n        \\n        \\n          g\\n          \\n            μ\\n            ν\\n          \\n        \\n        R\\n        =\\n        κ\\n        \\n          T\\n          \\n            μ\\n            ν\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle R_{\\\\mu \\\\nu }-{1 \\\\over 2}g_{\\\\mu \\\\nu }R=\\\\kappa T_{\\\\mu \\\\nu }}\\n  \\n  (the general theory of relativity)\\n\\n  \\n    \\n      \\n        e\\n        V\\n        =\\n        h\\n        ν\\n        −\\n        A\\n        \\n      \\n    \\n   '),\n",
       " Document(metadata={'title': 'Albert Einstein (album)', 'summary': 'Albert Einstein is the second and last collaborative studio album by American hip hop recording artist Prodigy of Mobb Deep and American record producer The Alchemist. The album was released on June 11, 2013, by Infamous Records. The album features guest appearances from Roc Marciano, Domo Genesis, Havoc, Raekwon and Action Bronson.', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_(album)'}, page_content='Albert Einstein is the second and last collaborative studio album by American hip hop recording artist Prodigy of Mobb Deep and American record producer The Alchemist. The album was released on June 11, 2013, by Infamous Records. The album features guest appearances from Roc Marciano, Domo Genesis, Havoc, Raekwon and Action Bronson.\\n\\n\\n== Background ==\\nOn November 27, 2012 Prodigy announced an upcoming collaboration album with producer The Alchemist via his Twitter account, along with the title of Albert Einstein. He also would say the album was scheduled for an early 2013 release. It would be their second collaboration album after 2007\\'s Return of the Mac. In November 2012, when speaking about the album with Complex Prodigy described it as a concept album.\\nOn April 7, 2013 The Alchemist revealed via Twitter that the album had been completed. Then on April 12, 2013, the album\\'s official track listing was revealed. The album contains 16 tracks and features guest appearances  by Roc Marciano, Domo Genesis, Havoc, Raekwon, and Action Bronson.\\n\\n\\n== Release and promotion ==\\nThe first single, \"Give ‘Em Hell\" was released on January 28, 2013 via Prodigy\\'s SoundCloud page. The song features vocals by Prodigy and was produced by The Alchemist. The album was originally scheduled to be released on May 14, 2013, but Amazon.com would later reveal a release date for the album of June 11, 2013. The second song released in promotion of the album would be \"Dough Pildin\\'\" on May 12, 2013. On June 4, 2013, the music video was released for \"Y.N.T.\" featuring Domo Genesis. The music video for \"Dough Pildin\\'\" was released on June 17, 2013. On June 25, 2013, the music video was released for \"Give ‘Em Hell\". On February 3, 2014, the music video was released for \"IMDKV\".\\n\\n\\n== Critical response ==\\n\\nAlbert Einstein was met with generally positive reviews from music critics. At Metacritic, which assigns a normalized rating out of 100 to reviews from mainstream critics, the album received an average score of 73, based on 7 reviews, indicating \"generally favorable reviews\". Reed Jackson of XXL gave the album an L, saying \"It’s the type of rough and witty realism people loved from P in the ’90s, and hopefully it’s here to stay for projects to come.\" Logan Smithson of PopMatters gave the album a seven out of ten, saying \"Albert Einstein has plenty of memorable beats, but the lyrics don’t match that same level of success. You might not be blown away by the album, but Prodigy and Alchemist have put together a highly enjoyable album.\" Jaroslav Lavick of RapReviews gave the album an 8.5 out of ten, saying \"This album is Prodigy and Alchemist giving long time fans exactly what they want.\"\\nPeter Marrack of Exclaim! gave the album a six out of ten, saying \"He\\'s casual, not always sincere, but dangerously convincing; his bars haven\\'t faltered since his early Mobb Deep days and Einstein is no exception.\" Nate Patrin of Pitchfork Media gave the album an 8.2 out of 10, saying \"Albert Einstein is so casually visceral and immediately gettable that it feels like a recent high point for both of them.\" Jesse Fairfax of HipHopDX gave the album three and a half stars out of five, saying \"Sharing his first name with the famed genius, Prodigy breaks his advanced expertise down to a science on Albert Einstein. Prodigy continues his largely consistent reign as a still thriving pioneer of New York’s once thugged-out era.\" Matthew Sanderson of AllHipHop gave the album an eight out of ten, saying \"This album is for the fans of the original Mobb sound of the late 90s. For the most part sick signature Alchemist beats, with the addition of some new sounds that are just right.\"\\n\\n\\n== Commercial performance ==\\nThe album debuted at number 175 on the Billboard 200 chart, with first-week sales of 3,000 copies in the United States.\\n\\n\\n== Track listing ==\\nAll songs solely produced by The Alchemist except track 13, co-produced by Adrian Younge.\\n\\n\\n== Personnel ==\\nCredits for Albert Einstein adapte'),\n",
       " Document(metadata={'title': 'List of scientific publications by Albert Einstein', 'summary': \"Albert Einstein (1879–1955) was a renowned theoretical physicist of the 20th century, best known for his special and general theories of relativity. He also made important contributions to statistical mechanics, especially his treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation.  Despite his reservations about its interpretation, Einstein also made seminal contributions to quantum mechanics and, indirectly, quantum field theory, primarily through his theoretical studies of the photon.\\nEinstein's writings, including his scientific publications, have been digitized and released on the Internet with English translations by a consortium of the Hebrew University of Jerusalem, Princeton University Press, and the California Institute of Technology, called the Einstein Papers Project.\\nEinstein's scientific publications are listed below in four tables: journal articles, book chapters, books and authorized translations.   Each publication is indexed in the first column by its number in the Schilpp bibliography (Albert Einstein: Philosopher–Scientist, pp. 694–730) and by its article number in Einstein's Collected Papers.  Complete references for these two bibliographies may be found below in the Bibliography section.  The Schilpp numbers are used for cross-referencing in the Notes (the final column of each table), since they cover a greater time period of Einstein's life at present.  The English translations of titles are generally taken from the published volumes of the Collected Papers.  For some publications, however, such official translations are not available; unofficial translations are indicated with a § superscript. Collaborative works by Einstein are highlighted in lavender, with the co-authors provided in the final column of the table.\\nThere were also five volumes of Einstein's Collected Papers (volumes 1, 5, 8–10) that are devoted to his correspondence, much of which is concerned with scientific questions, but were never prepared for publication.\", 'source': 'https://en.wikipedia.org/wiki/List_of_scientific_publications_by_Albert_Einstein'}, page_content='Albert Einstein (1879–1955) was a renowned theoretical physicist of the 20th century, best known for his special and general theories of relativity. He also made important contributions to statistical mechanics, especially his treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation.  Despite his reservations about its interpretation, Einstein also made seminal contributions to quantum mechanics and, indirectly, quantum field theory, primarily through his theoretical studies of the photon.\\nEinstein\\'s writings, including his scientific publications, have been digitized and released on the Internet with English translations by a consortium of the Hebrew University of Jerusalem, Princeton University Press, and the California Institute of Technology, called the Einstein Papers Project.\\nEinstein\\'s scientific publications are listed below in four tables: journal articles, book chapters, books and authorized translations.   Each publication is indexed in the first column by its number in the Schilpp bibliography (Albert Einstein: Philosopher–Scientist, pp. 694–730) and by its article number in Einstein\\'s Collected Papers.  Complete references for these two bibliographies may be found below in the Bibliography section.  The Schilpp numbers are used for cross-referencing in the Notes (the final column of each table), since they cover a greater time period of Einstein\\'s life at present.  The English translations of titles are generally taken from the published volumes of the Collected Papers.  For some publications, however, such official translations are not available; unofficial translations are indicated with a § superscript. Collaborative works by Einstein are highlighted in lavender, with the co-authors provided in the final column of the table.\\nThere were also five volumes of Einstein\\'s Collected Papers (volumes 1, 5, 8–10) that are devoted to his correspondence, much of which is concerned with scientific questions, but were never prepared for publication.\\n\\n\\n== Chronology and major themes ==\\n\\nThe following chronology of Einstein\\'s scientific discoveries provides a context for the publications listed below, and clarifies the major themes running through his work. Einstein\\'s scientific career can be broadly divided into to periods. During the first period (from 1901 to 1933), Einstein published mainly in German-language journals, notably the Annalen der Physik, and, after becoming a professional physicist, worked at various German-speaking institutions in Europe, including the Prussian Academy of Sciences in Berlin. Following his permanent relocation to the United States in 1933, Einstein spent most of his time at the Institute for Advanced Study in Princeton, New Jersey, where he remained till his death in 1955. During his second period, Einstein submitted his papers in English to North American journals, such as the Physical Review. Einstein first gained fame among physicists for the papers he submitted in 1905, his annus mirabilis or miraculous year in physics. His epochal contributions during this phase of his career stemmed from a single problem, the fluctuations of a delicately suspended mirror inside a radiation cavity. It led him to examine the nature of light, the statistical mechanics of fluctuations, and the electrodynamics of moving bodies.\\n\\nFrom 1901 to 1904, Einstein submitted his first scientific papers, dealing with problems in thermodynamics and statistical mechanics.\\nIn 1905, Einstein proposed that the existence of light quanta—dubbed photons by chemist Gilbert Lewis in 1926—could explain the photoelectric effect. He treated electromagnetic radiation as a gas and applied thermodynamic reasoning in his \"heuristic\" treatment, arguing that the energy \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n  \\n of a photon is given by Planck\\'s relation, \\n  \\n    \\n      \\n        E\\n        =\\n        h\\n        ν\\n      \\n    \\n    {\\\\displaystyle E=h\\\\n'),\n",
       " Document(metadata={'title': 'Elsa Einstein', 'summary': \"Elsa Einstein (18 January 1876 – 20 December 1936) was the second wife and cousin of Albert Einstein. Their mothers were sisters, thus making them maternal first cousins. The couple were also paternal second cousins (i.e. their fathers were first cousins). Born an Einstein, Elsa gave up the name when she took the surname of her first husband, Max Löwenthal; she and her daughters reverted to her maiden name after Elsa and Löwenthal's 1908 divorce.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Elsa_Einstein'}, page_content='Elsa Einstein (18 January 1876 – 20 December 1936) was the second wife and cousin of Albert Einstein. Their mothers were sisters, thus making them maternal first cousins. The couple were also paternal second cousins (i.e. their fathers were first cousins). Born an Einstein, Elsa gave up the name when she took the surname of her first husband, Max Löwenthal; she and her daughters reverted to her maiden name after Elsa and Löwenthal\\'s 1908 divorce.\\n\\n\\n== Early life ==\\nElsa, the daughter of Rudolf Einstein and Fanny Einstein (née Koch), was born in Hechingen on 18 January 1876.:\\u200a146\\u200a She had two sisters: Paula (1878–1955) and Hermine (1872–1942). Rudolf was a textile manufacturer in Hechingen. During the regular visits with the family in Munich, she often played with her cousin Albert. In her Swabian dialect, she called him \"Albertle\". The two parted ways in 1894, when Albert left Germany to follow his family to Milan.\\n\\n\\n== Married life ==\\n\\nIn 1896, Elsa married textile trader Max Löwenthal (1864–1914),:\\u200a146\\u200a from Berlin, with whom she had three children: daughters Ilse (1897–1934) and Margot (1899–1986), and a son who was born in 1903, but died shortly after birth.:\\u200a146,287\\u200a They lived together in Hechingen. In 1902, Max Löwenthal took a job in Berlin. His family stayed in Hechingen. She divorced Max on 11 May 1908,:\\u200a146\\u200a and moved with her two daughters to an apartment above her parents on Haberlandstrasse 5, in Berlin.:\\u200a146\\u200a She and her daughters reverted to her maiden name, Einstein, after her 1908 divorce.\\nShe began a relationship with her cousin Albert Einstein in April 1912,:\\u200a147\\u200a while Albert was still married to his first wife, the physicist and mathematician Mileva Marić. Einstein separated from Mileva in July 1914, sending her and their two sons back to Zürich. Their divorce was finalized on 10 February 1919. Elsa married him three and a half months later, on 2 June 1919.\\nWith stepdaughters Ilse and Margot, the Einsteins formed a close-knit family. Although Albert and Elsa did not have any children together, Albert treated Ilse and Margot as his own.:\\u200a193\\u200a They lived in the Berlin area and in 1929 acquired a summer house in Caputh in nearby  Potsdam.:\\u200a203\\u200a Ilse also served as Einstein\\'s secretary for a brief period.\\nElsa spent most of her marriage with Albert acting as his gatekeeper, protecting him from unwelcome visitors and charlatans.:\\u200a190,196\\u200a She also was the driving force behind building their summer house.\\n\\n\\n=== Later life ===\\nIn 1933, Albert and Elsa Einstein immigrated to Princeton, New Jersey, US. In autumn 1935, they moved to a house at 112 Mercer Street,:\\u200a216\\u200a bought that August, but shortly afterwards Elsa developed a swollen eye and was diagnosed with heart and kidney problems.:\\u200a216\\u200a When Elsa was diagnosed, Einstein decided to spend much of his time in his studies. It was stated in Walter Isaacson\\'s book, Einstein: His Life and Universe, that he believed \"strenuous intellectual work and looking at God\\'s nature are the reconciling, fortifying yet relentlessly strict angels that shall lead me through all of life\\'s troubles\". Thus did Einstein try to escape from his troubles by focusing on work that would distract him from Elsa\\'s dying. Elsa died after a painful illness on 20 December 1936, in the house on Mercer Street.:\\u200a216\\u200a\\n\\n\\n== See also ==\\nAlbert Einstein House, Princeton, New Jersey\\nList of coupled cousins\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Evelyn Einstein', 'summary': \"Evelyn Einstein (March 28, 1941 – April 13, 2011) was the adopted daughter of Hans Albert Einstein, the son of Albert Einstein. She graduated from University of California, Berkeley with a master's degree in literature, and had several jobs in her life including animal control officer, cult deprogrammer, and reserve police officer in Berkeley, California.\", 'source': 'https://en.wikipedia.org/wiki/Evelyn_Einstein'}, page_content='Evelyn Einstein (March 28, 1941 – April 13, 2011) was the adopted daughter of Hans Albert Einstein, the son of Albert Einstein. She graduated from University of California, Berkeley with a master\\'s degree in literature, and had several jobs in her life including animal control officer, cult deprogrammer, and reserve police officer in Berkeley, California.\\n\\n\\n== Biography ==\\nEinstein was born in Chicago; after her birth she was adopted by Hans Albert Einstein. Towards the end of her life, she asserted that she was an illegitimate daughter of Albert Einstein and a ballet dancer; however, she had no documentation supporting this claim. She obtained a Master\\'s degree in Medieval literature at University of California, Berkeley. She was married to Grover Krantz for 13 years from 1964 to about 1977. She then worked briefly as an animal control officer, as a cult deprogrammer, and as a Berkeley, California, reserve police officer.\\nAs an 18-year-old college student in 1960, Einstein was the only person with a recognized name among the dozens of people arrested in San Francisco at a peaceful protest against the House Un-American Activities Committee.\\nAfter her divorce, she stated that she was impoverished.  She claims that she was homeless, she slept in cars, scrounged for discarded food and described herself as a dumpster-diver for three months. From the mid-1990s up to her death in 2011, she lived in her own townhouse in Albany, California with a view of the San Francisco Bay.\\nEvelyn told CNN she was outraged she had not received a dime out of the millions of dollars earned annually from her grandfather\\'s likeness, with all profits going to Hebrew University of Jerusalem. While Albert Einstein bestowed the literary rights for the more than 75,000 papers and other items in his estate, Evelyn asked \"What does a bobblehead have to do with a literary estate?\" adding \"It\\'s hard for me to believe they would treat the family the way they have, which has been abysmally.\"  In 1996 she sued the trustee in charge of a collection of correspondence between Albert and Mileva Einstein, a suit that was settled privately.\\nShe left her entire estate valued over a million dollars to attorney Allen P. Wilkinson.\\n\\n\\n== Publications ==\\nWith Marfe Ferguson Delano: Genius: A Photobiography of Albert Einstein. National Geographic Children\\'s Books, 2005, ISBN 0-7922-9544-7.\\nIntroduction of: Alice Calaprice (editor); Robert Schulmann (contributor). Dear Professor Einstein: Albert Einstein\\'s Letters to and from Children. Prometheus Books, 2002. ISBN 978-1591020158\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\"Evelyn Einstein: Eine uneheliche Tochter? Evelyn Einstein, die Adoptiv-Tochter des Einstein-Sohnes Hans Albert, spielte eine wichtige Rolle bei der Beschaffung der frühen Liebesbriefe von Albert und Mileva. Ob sie selbst in Wahrheit ein uneheliches Kind von Albert Einstein ist, bleibt ungeklärt\". mensch–einstein.de. March 24, 2005. Archived from the original on January 5, 2013.\\n\"Personalien: Evelyn Einstein\". Der Spiegel. February 21, 2011. p. Nr. 8.\\nDinkelspiel, Frances (May 5, 2011). \"Saving the history of the Berkeley Police Department\". Berkeleyside. Archived from the original on April 2, 2015.\\nDonaldson James, Susan (April 20, 2011). \"Evelyn Einstein Died in Squalor, Despite Grandfather\\'s Riches\". ABC News.\\nHoller, Madeline (February 11, 2011). \"Granddaughter of Albert Einstein Wants a Cut in Bobblehead Profits\". blogs.babble.com.\\nMartin, Douglas (April 18, 2011). \"Evelyn Einstein Dies at 70; Shaped by a Link to Fame\". The New York Times.\\nRees, Mary (May 13, 2011). \"Granddaughter of Albert Einstein Remembered Fondly in Albany: Albany resident Evelyn Einstein passed away last month\". AlbanyPatch.\\nRees, Mary (June 18, 2011). \"Evelyn Einstein, R.I.P.? The death of Albany resident Evelyn Einstein, granddaughter of Albert Einstein, was followed by confusion and some conflict\". AlbanyPatch.\\nSanides, Silvia (December 20, 2004). \"Adoptiv-Enkelin ode'),\n",
       " Document(metadata={'title': 'Einstein notation', 'summary': 'In mathematics, especially the usage of linear algebra in mathematical physics and differential geometry, Einstein notation (also known as the  Einstein summation convention or Einstein summation notation) is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving brevity. As part of mathematics it is a notational subset of Ricci calculus; however, it is often used in physics applications that do not distinguish between tangent and cotangent spaces. It was introduced to physics by Albert Einstein in 1916.', 'source': 'https://en.wikipedia.org/wiki/Einstein_notation'}, page_content='In mathematics, especially the usage of linear algebra in mathematical physics and differential geometry, Einstein notation (also known as the  Einstein summation convention or Einstein summation notation) is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving brevity. As part of mathematics it is a notational subset of Ricci calculus; however, it is often used in physics applications that do not distinguish between tangent and cotangent spaces. It was introduced to physics by Albert Einstein in 1916.\\n\\n\\n== Introduction ==\\n\\n\\n=== Statement of convention ===\\nAccording to this convention, when an index variable appears twice in a single term and is not otherwise defined (see Free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set {1, 2, 3},\\n\\n  \\n    \\n      \\n        y\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            3\\n          \\n        \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\n          e\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        \\n          e\\n          \\n            1\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        \\n          e\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        \\n          e\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y=\\\\sum _{i=1}^{3}x^{i}e_{i}=x^{1}e_{1}+x^{2}e_{2}+x^{3}e_{3}}\\n  \\n\\nis simplified by the convention to:\\n\\n  \\n    \\n      \\n        y\\n        =\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\n          e\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y=x^{i}e_{i}}\\n  \\n\\nThe upper indices are not exponents but are indices of coordinates, coefficients or basis vectors.  That is, in this context x2 should be understood as the second component of x rather than the square of x (this can occasionally lead to ambiguity).  The upper index position in xi is because, typically, an index occurs once in an upper (superscript) and once in a lower (subscript) position in a term (see § Application below).  Typically, (x1 x2 x3) would be equivalent to the traditional (x y z).\\nIn general relativity, a common convention is that\\n\\nthe Greek alphabet is used for space and time components, where indices take on values 0, 1, 2, or 3 (frequently used letters are μ, ν, ...),\\nthe Latin alphabet is used for spatial components only, where indices take on values 1, 2, or 3 (frequently used letters are i, j, ...),\\nIn general, indices can range over any indexing set, including an infinite set. This should not be confused with a typographically similar convention used to distinguish between tensor index notation and the closely related but distinct basis-independent abstract index notation.\\nAn index that is summed over is a summation index, in this case \"i\\u200a\". It is also called a dummy index since any symbol can replace \"i\\u200a\" without changing the meaning of the expression (provided that it does not collide with other index symbols in the same term).\\nAn index that is not summed over is a free index and should appear only once per term.  If such an index does appear, it usually also appears in every other term in an equation. An example of a free index is the \"i\\u200a\" in the equation \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          a\\n          \\n            i\\n          \\n        \\n        \\n          b\\n          \\n            j\\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i}=a_{i}b_{j}x^{j}}\\n  \\n, which is equivalent to the equation \\n  \\n    \\n      \\n        \\n          v\\n          \\n     '),\n",
       " Document(metadata={'title': 'Albert Einstein in popular culture', 'summary': 'Albert Einstein has been the subject of (or inspiration for) many works of popular culture.\\n\\nEinstein is a favorite model for depictions of absent-minded professors; his expressive face and distinctive hairstyles have been widely copied and exaggerated. Time magazine\\'s Frederic Golden wrote that Einstein was \"a cartoonist\\'s dream come true\".\\n\"Einstein\" has become a byword for an extremely intelligent person. It may also be used ironically when someone states the obvious or demonstrates a lack of wisdom or intelligence (as in \"Way to go, Einstein!\")\\nMany quotes that have become popular via the Internet have been misattributed to him, including \"The definition of insanity is doing the same thing over and over and expecting a different result\".', 'source': 'https://en.wikipedia.org/wiki/Albert_Einstein_in_popular_culture'}, page_content='Albert Einstein has been the subject of (or inspiration for) many works of popular culture.\\n\\nEinstein is a favorite model for depictions of absent-minded professors; his expressive face and distinctive hairstyles have been widely copied and exaggerated. Time magazine\\'s Frederic Golden wrote that Einstein was \"a cartoonist\\'s dream come true\".\\n\"Einstein\" has become a byword for an extremely intelligent person. It may also be used ironically when someone states the obvious or demonstrates a lack of wisdom or intelligence (as in \"Way to go, Einstein!\")\\nMany quotes that have become popular via the Internet have been misattributed to him, including \"The definition of insanity is doing the same thing over and over and expecting a different result\".\\n\\n\\n== Recognition ==\\nIn 1999, leading physicists voted Einstein the \"greatest physicist ever\".\\nHis birthday, March 14, is also Pi Day, so called because 3/14 (March 14 in shorthand month-day format) corresponds to 3.14, the first three digits of the number Pi. The town of Princeton, New Jersey, where Einstein lived for more than 20 years, celebrates March 14 every year as \"Princeton Pi Day and Einstein Birthday Party\".\\n\\n\\n=== Usage of his name and image ===\\nThe children\\'s television show Little Einsteins and the educational toys and videos of the Baby Einstein series both use Einstein\\'s name, though not his image.\\nIranian cartoonist and humorist Javad Alizadeh publishes a column titled \"4D Humor\" in his Persian monthly Humor & Caricature, which features cartoons, caricatures and stories on Einstein-related topics. In 1991, he published in the column a comic book on Einstein\\'s life and work, inspired mainly by the theory of relativity.\\nOn Einstein\\'s 72nd birthday on March 14, 1951, United Press photographer Arthur Sasse was trying to persuade him to smile for the camera, but having smiled for photographers many times that day, Einstein stuck out his tongue instead. This photograph became one of the most popular ever taken of Einstein, often used in merchandise depicting him in a lighthearted sense. Einstein enjoyed this photo and requested UPI to give him nine copies for personal use, one of which he signed for a reporter. On June 19, 2009, the original signed photograph was sold at auction for $74,324, a record for an Einstein picture at the time. The photo was sold again on July 27, 2017 for $125,000.\\nSalesforce acquired a 20-year license in 2016 for $20 million to be the only business-oriented software company permitted to use his likeness. Einstein\\'s likeness is used as a company mascot and his name is used for the company\\'s artificial intelligence features.\\n\\n\\n=== Licensing ===\\nEinstein bequeathed his estate, as well as the use of his image (see personality rights), to the Hebrew University of Jerusalem, which from the mid-1980s has sponsored the Einstein Papers Project with the Princeton University Press. Einstein actively supported the university during his life and this support continues with the royalties received from licensing activities. GreenLight licences the commercial use of the name \"Albert Einstein\" and associated imagery and likenesses of Einstein, as agent for the Hebrew University of Jerusalem. As head licensee, the corporation can control commercial usage of Einstein\\'s name and theoretically ensure compliance with certain standards (e.g., when Einstein\\'s name is used as a trademark, the ™ symbol must be used).\\nIn 2012, Judge Howard Matz of a United States district court found that although a General Motors advertisement featuring Einstein\\'s image was \"tasteless\", it was not illegal. Judge Matz wrote that \"the obviously humorous ad for the 2010 Terrain having been published 55 years or more after Einstein\\'s death, it is unlikely that any viewer of it could reasonably infer that Einstein, or whoever succeeded to any right of publicity that Einstein may have had, was endorsing the GMC Terrain\".\\nHebrew University asked that publicity rights be extended to the 70 years associa'),\n",
       " Document(metadata={'title': 'Einstein refrigerator', 'summary': 'The Einstein–Szilard or Einstein refrigerator is an absorption refrigerator which has no moving parts, operates at constant pressure, and requires only a heat source to operate. It was jointly invented in 1926 by Albert Einstein and his former student Leó Szilárd, who patented it in the U.S. on November 11, 1930 (U.S. patent 1,781,541). The three working fluids in this design are water, ammonia, and butane. The Einstein refrigerator is a development of the original three-fluid patent by the Swedish inventors Baltzar von Platen and Carl Munters.', 'source': 'https://en.wikipedia.org/wiki/Einstein_refrigerator'}, page_content='The Einstein–Szilard or Einstein refrigerator is an absorption refrigerator which has no moving parts, operates at constant pressure, and requires only a heat source to operate. It was jointly invented in 1926 by Albert Einstein and his former student Leó Szilárd, who patented it in the U.S. on November 11, 1930 (U.S. patent 1,781,541). The three working fluids in this design are water, ammonia, and butane. The Einstein refrigerator is a development of the original three-fluid patent by the Swedish inventors Baltzar von Platen and Carl Munters.\\n\\n\\n== History ==\\nFrom 1926 until 1934 Einstein and Szilárd collaborated on ways to improve home refrigeration technology. The two were motivated by contemporary newspaper reports of a Berlin family who had been killed when a seal in their refrigerator failed and leaked toxic fumes into their home. Einstein and Szilárd proposed that a device without moving parts would eliminate the potential for seal failure, and explored practical applications for different refrigeration cycles. Einstein had worked in the Swiss Patent Office, and used his experience to apply for valid patents for their inventions in several countries. The two were eventually granted 45 patents in six countries for three different models.\\nIt has been suggested that most of the actual inventing was done by Szilárd, with Einstein merely acting as a consultant and helping with the patent-related paperwork, but others assert that Einstein contributed design work to the project.\\nThe refrigerator was less efficient than existing appliances, although having no moving parts made it more reliable; the introduction of Freon to replace refrigerant gases toxic to humans made it even less attractive commercially. The Great Depression of 1929 dried up funding for development, and the widespread political violence in Nazi Germany, where the inventors lived, particularly towards Jews such as Einstein and Szilard, contributed to the device\\'s lack of commercial success. (The inventors fled Germany in the early 1930s.) It was not immediately put into commercial production, although the most promising of the patents were quickly bought up by the Swedish company Electrolux. Einstein and Szilárd earned $750 (the equivalent of $10,000 in 2017). A few demonstration units were constructed from other patents.\\nOne variant, the Einstein–Szilard electromagnetic refrigerator used a Einstein–Szilard electromagnetic pump to compress a working gas, pentane. Although the refrigerator was not a commercial success, the Einstein–Szilard pump was later used for cooling breeder reactors, where its inherent reliability and safety were important.\\nIn 2008, electrical engineers at Oxford University\\'s Energy and Power Group, part of the university\\'s Department of Engineering Science, revived the Einstein refrigerator as an attempt to produce a refrigerator suitable for use in rural areas without electricity. The group, led by Malcolm McCulloch noted that the design was still \"nowhere near commercialised\", but might allow the efficiency of the original Einstein–Szilárd design to be quadrupled.\\n\\n\\n== See also ==\\nRudolf Goldschmidt (for the Einstein–Goldschmidt hearing aid)\\nIcyball\\nTimeline of low-temperature technology\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nEinstein, A., L. Szilárd, \"Refrigeration\" (Appl: 16 December 1927; Priority: Germany, 16 December 1926) U.S. patent 1,781,541, 11 November 1930.\\nEinstein, A., L. Szilárd, \"Accompanying notes and remarks for Pat. No. 1,781,541\". Mandeville Special Collections Library USC. Box 35, Folder 3, 1927; 52 pages.\\nEinstein, A., L. Szilárd, \"Improvements Relating to Refrigerating Apparatus.\" (Appl: 16 December. 1927; Priority: Germany, 16 December 1926). Patent Number 282,428 (United Kingdom). Complete accept.: 5 November 1928.\\n\\n\\n== External links ==\\nFlanigan, Allen, \"Einsteins \"Automatischer Beton-Volks-Kühlschrank\" (German site) Wolfgang Engels from the University of Oldenburg rebuilt the original concept—the housing is manufactu'),\n",
       " Document(metadata={'title': 'Einstein–Oppenheimer relationship', 'summary': 'Albert Einstein and J. Robert Oppenheimer were twentieth century physicists who made pioneering contributions to physics. From 1947 to 1955 they had been colleagues at the Institute for Advanced Study (IAS). Belonging to different generations, Einstein and Oppenheimer became representative figures for the relationship between \"science and power\", as well as for \"contemplation and utility\" in science.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Einstein%E2%80%93Oppenheimer_relationship'}, page_content='Albert Einstein and J. Robert Oppenheimer were twentieth century physicists who made pioneering contributions to physics. From 1947 to 1955 they had been colleagues at the Institute for Advanced Study (IAS). Belonging to different generations, Einstein and Oppenheimer became representative figures for the relationship between \"science and power\", as well as for \"contemplation and utility\" in science.\\n\\n\\n== Overview ==\\n\\nIn 1919, after the successful verification of the phenomenon of light from faraway stars gravitationally bending near the sun — as predicted earlier by Einstein\\'s theory of gravity — became an observable fact, Albert Einstein was acclaimed as “the most revolutionary innovator in physics” since Isaac Newton. J. Robert Oppenheimer, called the American physics community\\'s \"boy-wonder\" in the 1930s, became a popular figure from 1945 onwards after overseeing the first ever successful test of nuclear weapons.\\nBoth Einstein and Oppenheimer were born into nonobservant Jewish families.\\nBelonging to different generations, Einstein (1879–1955) and Oppenheimer (1904–1967), with the full development of quantum mechanics by 1925 marking a delineation, represented the shifted approach in being either a theoretical physicist or an experimental physicist since the mid-1920s when being both became rare due to the division of labor. \\nEinstein and Oppenheimer, who incorporated different modes of approach for their achievements, became emblematic for the relationship between \"science and power\", as well as for \"contemplation and utility\" in science. When in 1945 the first ever nuclear weapons were successfully tested, Oppenheimer was acknowledged for bringing forth to the world the astounding \"instrumental power of science\". Einstein, after facing criticism for having \"participated\" in the creation of the atomic bomb, answered in 1950 that, when he contemplated on the relationship between mass and energy in 1905, he had no idea that it could have been used for military purposes in anyway, and maintained that he had always been a \"convinced pacifist\". \\nWhile Einstein engaged in the pursuit of what he called as \"Unity\" in the complex phenomena of the Universe, Oppenheimer engaged in the establishment of an \"Unified\" framework at the Institute for Advanced Study, which would comprise all the academic disciplines of knowledge that can be pursued. Einstein was markedly individualistic in his approach to physics. He had only few students, and was disinterested if not adversarial in his relation with formal institutions and politics. Oppenheimer was more collaborative and embraced collective scientific work. He had been a better successful teacher and more immersed in political and institutional realms. Oppenheimer emerged as a powerful political \\'insider\\', a role that Einstein never embraced but instead wondered why Oppenheimer desired such power. Despite their differences in stances, both Oppenheimer and Einstein were regarded as \"deeply suspicious\" figures by the authorities, specifically by J. Edgar Hoover.\\nWith the advent of modern physics in the twentieth century changing the world radically, both Einstein and Oppenheimer grappled with metaphysics that can provide an ethical framework for human actions. Einstein turned to the philosophical works of Spinoza and Schopenhauer, along with an attachment to the European enlightenment heritage. Oppenheimer became engrossed in the eastern philosophy, with particular interest in the Bhagavad Gita, and an affinity with the American philosophical tradition of pragmatism.\\n\\n\\n== Association with each other ==\\nOppenheimer met Einstein for the first time in January 1932 when the latter visited Caltech as part of his round-the-world trip during 1931-32.\\nIn 1939, Einstein published a paper that argued against the existence of Black holes. Einstein used his own general theory of relativity to arrive at this conclusion. A few months after Einstein rejected the existence of Black holes, Oppenheimer and hi')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_wiki_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load arvix\n",
    "from langchain.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvix_loader = ArxivLoader(query=\"quantum computing\", load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2022-08-01', 'Title': 'The Rise of Quantum Internet Computing', 'Authors': 'Seng W. Loke', 'Summary': 'This article highlights quantum Internet computing as referring to\\ndistributed quantum computing over the quantum Internet, analogous to\\n(classical) Internet computing involving (classical) distributed computing over\\nthe (classical) Internet. Relevant to quantum Internet computing would be areas\\nof study such as quantum protocols for distributed nodes using quantum\\ninformation for computations, quantum cloud computing, delegated verifiable\\nblind or private computing, non-local gates, and distributed quantum\\napplications, over Internet-scale distances.'}, page_content='arXiv:2208.00733v1  [cs.ET]  1 Aug 2022\\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\\n1\\nThe Rise of Quantum Internet Computing\\nSeng W. Loke, Member, IEEE\\nAbstract—This article highlights quantum Internet computing as referring to distributed quantum computing over the quantum Internet,\\nanalogous to (classical) Internet computing involving (classical) distributed computing over the (classical) Internet. Relevant to\\nquantum Internet computing would be areas of study such as quantum protocols for distributed nodes using quantum information for\\ncomputations, quantum cloud computing, delegated veriﬁable blind or private computing, non-local gates, and distributed quantum\\napplications, over Internet-scale distances.\\nIndex Terms—quantum Internet computing, quantum Internet, distributed quantum computing, Internet computing, distributed\\nsystems, Internet\\n”This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this\\nversion may no longer be accessible.”\\n✦\\n1\\nINTRODUCTION\\nT\\nHERE have been tremendous developments in quantum\\ncomputing, quantum cryptography, quantum commu-\\nnications and the quantum Internet, and we have seen\\nincreased investments and intensive research in quantum\\ncomputing in recent years [1], [2]. The quantum Internet will\\nnot necessarily replace the (classical) Internet we know and\\nuse today, at least not in the near future, but can complement\\nthe current Internet. The quantum Internet aims to enable\\nrobust quantum teleportation (or transmission) of qubits,1\\nand entanglement among qubits,2 over long Internet-scale\\ndistances, which are key to many of the quantum protocols\\nincluding quantum key distribution, quantum voting, and\\nothers, as well as for non-local control of quantum gates.\\nThere have been efforts to build quantum computers,\\nand it remains to see if any one paradigm becomes the\\ndominant or best way of building such quantum comput-\\ners. At the same time, even as researchers develop more\\npowerful quantum computers (supporting more qubits for\\noperations, and at lower error rates), there is an opportunity\\nfor connecting multiple quantum computers from differ-\\nent sites to achieve much more complex quantum com-\\nputations, i.e., inter-linking multiple quantum computers\\non different sites to perform distributed computing with\\na distributed system of quantum computers (or quantum\\nprocessing units (QPUs) at different nodes), arriving at the\\nnotion of distributed quantum computing, e.g., [3].\\nWhile distributed quantum computing can involve mul-\\ntiple QPUs next to each other or at the same site, with the\\nquantum Internet, one can envision distributed quantum\\n•\\nSeng W. Loke is with the School of Information Technology, Deakin\\nUniversity, Melbourne, Australia.\\nE-mail: see https://www.deakin.edu.au/about-deakin/people/seng-loke.\\nManuscript received X XX, 20XX; revised X XX, 20XX.\\n1. A qubit is the basic unit of quantum information, and can be\\nthought of as a two-state, or two-levelled, quantum-mechanical system,\\nsuch as an electron’s spin, where the two levels are spin up and spin\\ndown, or a photon’s polarization, where the two states are the vertical\\npolarization and the horizontal polarization.\\n2. Multiple qubits at different sites can share an entangled state, a\\nsuperpositon of “specially correlated” states, to be used in distributed\\nalgorithms.\\ncomputing over nodes geographically far apart. As noted\\nin [4], the idea is the quantum Internet as the “underly-\\ning infrastructure of the Distributed Quantum Computing\\necosystem.”\\nThis article highlights the emerging area of distributed\\nquantum computing over the quantum Internet, which we\\nrefer to as quantum Internet computing, i.e., the idea of com-\\nputing using quantumly connected distributed quantum\\ncomputers over Internet-scale distances. Hence, quantum\\nInternet computing is not a new concept in itself but a\\nproposed “umbrella term” used here for the collection of\\ntopics (listed below), from an analogy to (classical) Internet\\ncomputing.\\nInternet computing, where one does distributed comput-\\ning but over Internet-scale distances and distributed sys-\\ntems involve nodes connected via the Internet, is at the inter-\\nsection of work in (classical) distributed computing and the\\n(classical) Internet. Analogous to Internet computing, one\\ncould ask the question of what would be at the intersection\\nof work in distributed quantum computing and work on the\\nquantum Internet, which brings us to the notion of quantum\\nInternet computing.\\nAlso, while the quantum Internet and distributed quan-\\ntum computing are still nascent research areas, there are at\\nleast three key topics which can be considered as relevant to\\nquantum Internet computing:\\n•\\ndistributed quantum computing, including quantum\\nprotocols from theoretical perspectives involving\\ncommunication complexity studies, and distributed\\nquantum computing via non-local or distributed\\nquantum gates,\\n•\\nquantum cloud computing with a focus on delegat-\\ning quantum computations, blind quantum comput-\\ning, and verifying delegated quantum computations,\\nand\\n•\\ncomputations and algorithms for the quantum Inter-\\nnet including key ideas such as quantum entangle-\\nment distillation, entanglement swapping, quantum\\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\\n2\\nrepeaters, and quantum Internet standards.3\\nWe brieﬂy discuss the above topics in the following sections.\\n2\\nDISTRIBUTED QUANTUM COMPUTING\\nDistributed quantum computing problems and quantum\\nprotocols have been well-studied for over two decades,\\nfrom a theoretical computer science perspective,4 many of\\nwhich have their inspiration from classical distributed com-\\nputing research. Quantum versions of classical distributed\\ncomputing problems and protocols, and new forms of dis-\\ntributed computing using quantum information, have been\\nexplored, e.g., the distributed three-party product problem,\\nthe distributed Deutsch-Jozsa promise problem and the\\ndistributed intersection problem, demonstrating how, for\\nsome problems, quantum information can enable fewer\\nbits of communication to be used for a solution, and how\\ncertain distributed computation problems can be solved\\nwith quantum information, but cannot be solved classically.\\nMany quantum protocols, including quantum coin ﬂipping,\\nquantum leader election, quantum anonymous broadcast-\\ning, quantum voting, quantum Byzantine Generals, quan-\\ntum secret sharing, and quantum oblivious transfer, can\\nbe viewed as “quantum versions” of classical distributed\\ncomputing problems, and have been studied extensively.\\nAnother area of study, which has also been considered\\nas distributed quantum computing, is non-local gates, or\\nthe non-local control of quantum gates, including early\\nwork nearly over two decades ago.5 Key to performing\\nsuch non-local control of quantum gates is the use of en-\\ntanglement, which can be viewed as a resource for such\\nnon-local computations. More recent work has looked at\\nhow to partition the computations of distributed quantum\\ncircuits over multiple QPUs, e.g., [3] as we mentioned earlier\\n- with considerations including distributing computations\\nin such a way as to optimize performance and to reduce the\\nrequirements on entanglement, since if the entanglements\\nrequired are generated at too low a rate, this will hold up\\ncomputations. The key motivation here is to inter-link a\\nset of quantum computers to form effectively a much more\\npowerful quantum computer.\\n3\\nQUANTUM CLOUD COMPUTING AND DELEGAT-\\nING QUANTUM COMPUTATIONS\\nWe have seen big tech companies and startups offering\\nquantum computing as a service similar to accessing other\\ncloud service offerings, which is a fantastic resource for\\nexperimentation and studies.\\nMore formally, studies into delegating quantum com-\\nputation from a client (which can be either classical, or\\nalmost classical, i.e., with minimal capability to perform\\n3. For example, see https://www.ietf.org/archive/id/draft-irtf-qirg-principles-10.html\\n[last accessed: 1/8/2022]\\n4. For example, see Buhrman and R¨ohrig’s paper dating back to\\n2003: https://link.springer.com/chapter/10.1007/978-3-540-45138-9 1\\n[last accessed: 1/8/2022]\\n5. For example, see the work by Yimsiriwattana and Lomonaco\\nJr.\\nin\\nhttps://arxiv.org/pdf/quant-ph/0402148.pdf\\nand\\na\\ndistributed\\nversion\\nof\\nShor’s\\nfamous\\nfactorization\\nalgorithm\\nhttps://arxiv.org/abs/2207.05976 [last accessed: 1/8/2022]\\ncomputations such as teleporting qubits, applying simple\\nPauli quantum operations, and doing basic measurements)\\nwhich is much more restricted than the server (assumed\\nto be a universal quantum computer) have been studied,\\ncalled delegated quantum computing. And when the server\\nis prevented from knowing the client’s inputs but still can\\nperform delegated computations, by a technique such as\\nthe quantum one-time pad (where the client applies Pauli\\noperations to add uncertainty from the server’s perspective,\\nthereby effectively encrypting the quantum inputs it sends\\nto the server, and keeps track of operations it later needs\\nto decrypt the outputs from the server), this is called blind\\nquantum computing.\\nIn order to be sure that the server does indeed perform\\nthe required quantum operations delegated to it by the\\nclient, the client can embed tests (or test runs) into the\\ndelegated computations, so that the server (not being able\\nto distinguish between tests and normal computations) can\\nbe caught out if it did not perform the required compu-\\ntations properly. That is, the client can verify if the server\\nperformed the required quantum computations.6 Further\\nabstractions for delegating quantum computations with\\nsupporting cloud services continues to be investigated.\\n4\\nTHE QUANTUM INTERNET\\nAs we mentioned earlier, work on the quantum Internet\\nfocuses on how to efﬁciently enable robust entanglement\\nshared among qubits over long geographical distances. If\\ntwo nodes in different continents share entangled states,\\nthen, this can be a resource to do non-local gates, i.e.,\\nto perform distributed quantum computations, and enable\\nquantum protocols over Internet-scale distances.\\nThere have been the use of satellites to enable long dis-\\ntance entanglement, as well as the use of optical ﬁbre cables\\nto demonstrate entanglement. Key to the quantum Internet\\nare ideas such as entanglement swapping and quantum\\nrepeaters, including ideas such quantum distillation, to\\nachieve high ﬁdelity distributed entangled states over long\\ndistances, and quantum error correction - this continues to\\nbe a research endeavour as mentioned earlier [2].\\nThere are other interesting distributed quantum appli-\\ncations to be considered including quantum cryptography,\\nquantum sensing, and quantum positioning systems.\\n5\\nDISTRIBUTED QUANTUM COMPUTING OVER THE\\nQUANTUM INTERNET: QUANTUM INTERNET COM-\\nPUTING AND THE QUANTUM IOT?\\nApart from the many quantum computers available over\\nthe cloud by big tech and startups which work at very\\nlow temperatures, room temperature quantum computers\\nhave also started to emerge.7 This could pave the way\\nfor quantum computers at the fog and at the edge, not\\njust in remote clouds, and perhaps even mobile quantum\\n6. An\\nexcellent\\nexample\\nis\\nthe\\nwork\\nby\\nBroadbent\\nat\\nhttps://theoryofcomputing.org/articles/v014a011/\\n[last\\naccessed:\\n1/8/2022]\\n7. See https://spectrum.ieee.org/nitrogen-vacancy-diamond-quantum-computer-\\nand also https://otd.harvard.edu/explore-innovation/technologies/scalable-room-t\\n[last accessed: 1/8/2022]\\nIEEE IOT MAGAZINE, VOL. XX, NO. X, X 2022\\n3\\ncomputers, or quantum computers embedded into every-\\nday devices and objects, if ever! Will we then have the\\nquantum Internet of Things (IoT)? The answer remains to\\nbe seen, and “quantum entangled things across the world”\\nwill likely complement the classical IoT. Future applications\\nand potential of quantum Internet computing remains to\\nbe investigated. Meanwhile, others have begun to look at\\nthe connection between 6G networking and the quantum\\nInternet [5].\\nREFERENCES\\n[1] W.\\nKozlowski\\nand\\nS.\\nWehner,\\n“Towards\\nlarge-scale\\nquantum\\nnetworks,”\\nin\\nProceedings\\nof\\nthe\\nSixth\\nAnnual\\nACM\\nInternational\\nConference\\non\\nNanoscale\\nComputing\\nand\\nCommunication, ser. NANOCOM ’19.\\nNew York, NY, USA:\\nAssociation for Computing Machinery, 2019. [Online]. Available:\\nhttps://doi.org/10.1145/3345312.3345497\\n[2] P. P. Rohde, The Quantum Internet: The Second Quantum Revolution.\\nCambridge University Press, 2021.\\n[3] R. Parekh, A. Ricciardi, A. Darwish, and S. DiAdamo, “Quantum\\nalgorithms and simulation for parallel and distributed quantum\\ncomputing,” in 2021 IEEE/ACM Second International Workshop on\\nQuantum Computing Software (QCS), 2021, pp. 9–19.\\n[4] D. Cuomo, M. Calefﬁ, and A. S. Cacciapuoti, “Towards a distributed\\nquantum computing ecosystem,” IET Quantum Communication,\\nvol. 1, no. 1, pp. 3–8, 2020.\\n[5] C. Wang and A. Rahman, “Quantum-enabled 6g wireless net-\\nworks: Opportunities and challenges,” IEEE Wireless Communica-\\ntions, vol. 29, no. 1, pp. 58–69, 2022.\\n'),\n",
       " Document(metadata={'Published': '2000-03-31', 'Title': 'Unconventional Quantum Computing Devices', 'Authors': 'Seth Lloyd', 'Summary': \"This paper investigates a variety of unconventional quantum computation\\ndevices, including fermionic quantum computers and computers that exploit\\nnonlinear quantum mechanics. It is shown that unconventional quantum computing\\ndevices can in principle compute some quantities more rapidly than\\n`conventional' quantum computers.\"}, page_content='arXiv:quant-ph/0003151v1  31 Mar 2000\\nUnconventional Quantum Computing Devices\\nSeth Lloyd\\nMechanical Engineering\\nMIT 3-160\\nCambridge, Mass. 02139\\nAbstract: This paper investigates a variety of unconventional quantum computation de-\\nvices, including fermionic quantum computers and computers that exploit nonlinear quan-\\ntum mechanics. It is shown that unconventional quantum computing devices can in prin-\\nciple compute some quantities more rapidly than ‘conventional’ quantum computers.\\nComputers are physical: what they can and cannot do is determined by the laws\\nof physics. When scientiﬁc progress augments or revises those laws, our picture of what\\ncomputers can do changes. Currently, quantum mechanics is generally accepted as the\\nfundamental dynamical theory of how physical systems behave. Quantum computers can\\nin principle exploit quantum coherence to perform computational tasks that classical com-\\nputers cannot [1-21]. If someday quantum mechanics should turn out to be incomplete\\nor faulty, then our picture of what computers can do will change. In addition, the set\\nof known quantum phenomena is constantly increasing: essentially any coherent quantum\\nphenomenon involving nonlinear interactions between quantum degrees of freedom can\\nin principle be exploited to perform quantum logic. This paper discusses how the revi-\\nsion of fundamental laws and the discovery of new quantum phenomena can lead to new\\ntechnologies and algorithms for quantum computers.\\nSince new quantum eﬀects are discovered seemingly every day, let’s ﬁrst discuss two\\nbasic tests that a phenomenon must pass to be able to function as a basis for quantum\\ncomputation. These are 1) The phenomenon must be nonlinear, and 2) It must be coherent.\\nTo support quantum logic, the phenomenon must involve some form of nonlinearity, e.g.,\\na nonlinear interaction between quantum degrees of freedom. Without such a nonlinearity\\nquantum devices, like linear classical devices, cannot perform even so simple a nonlinear\\noperation as an AND gate.\\nQuantum coherence is a prerequisite for performing tasks\\nsuch as factoring using Shor’s algorithm [10], quantum simulation a la Feynman [11] and\\nLloyd [12], or Grover’s data-base search algorithm [13], all of which require extended\\nmanipulations of coherent quantum superpositions.\\n1\\nThe requirements of nonlinearity and coherence are not only necessary for a phe-\\nnomenon to support quantum computation, they are also in principle suﬃcient. As shown\\nin [14-15], essentially any nonlinear interaction between quantum degrees of freedom suf-\\nﬁces to construct universal quantum logic gates that can be assembled into a quantum\\ncomputer. In addition, the work of Preskill et al. [18] on robust quantum computation\\nshows that an error rate of no more than 10−4 per quantum logic operation allows one to\\nperform arbitrarily long quantum computations in principle.\\nIn practice, of course, few if any quantum phenomena are likely to prove suﬃciently\\ncontrollable to provide extended quantum computation. Promising devices under current\\nexperimental investigation include ion traps [5,7], high ﬁnesse cavities for manipulating\\nlight and atoms using quantum electrodynamics [6], and molecular systems that can be\\nmade to compute using nuclear magnetic resonance [8-9]. Such devices store quantum\\ninformation on the states of quantum systems such as photons, atoms, or nuclei, and\\naccomplish quantum logic by manipulating the interactions between the systems via the\\napplication of semiclassical potentials such as microwave or laser ﬁelds. We will call such\\ndevices ‘conventional’ quantum computers, if only because such devices have actually been\\nconstructed.\\nThere is another sense in which such computers are conventional: although the de-\\nvices described above have already been used to explore new regimes in physics and to\\ncreate and investigate the properties of new and exotic quantum states of matter, they\\nfunction according to well established and well understood laws of physics. Perhaps the\\nmost striking examples of the ‘conventionality’ of current quantum logic devices are NMR\\nquantum microprocessors that are operated using techniques that have been reﬁned for\\nalmost half a century. Ion-trap and quantum electrodynamic quantum computers, though\\ncertainly cutting edge devices, operate in a quantum electrodynamic regime where the\\nfundamental physics has been understood for decades (that is not to say that new and\\nunexpected physics does not arise frequently in this regime, rather that there is general\\nagreement on how to model the dynamics of such devices).\\nMake no mistake about it: a conventional quantum logic device is the best kind of\\nquantum logic device to have around. It is exactly because the physics of nuclear magnetic\\nresonance and quantum electrodynamics are well understood that devices based on this\\nphysics can be used systematically to construct and manipulate the exotic quantum states\\nthat form the basis for quantum computation.\\nWith that recognition, let us turn to\\n2\\n‘unconventional’ quantum computers.\\nPerhaps the most obvious basis for an unconventional quantum computer is the use\\nof particles with non-Boltzmann statistics in a reﬁme where these statistics play a key role\\nin the dynamics of the device. For example, Lloyd [16] has proposed the use of fermions\\nas the fundamental carriers of quantum information, so that a site or state occupied by a\\nfermion represents a 1 and an unoccupied site or state represents a 0. It is straightforward\\nto design a universal quantum computer using a conditional hopping dynamics on an array\\nof sites, in which a fermion hops from one site to another if only if other sites are occupied.\\nIf the array is one-dimensional, then such a fermionic quantum computer is equivalent\\nto a conventional quantum computer via the well-known technique of bosonization. If the\\narray is two or more dimensional, however, a local operation involving fermions on the\\nlattice cannot be mocked up by a local operation on a conventional quantum computer,\\nwhich must explicitly keep track of the phases induced by Fermi statistics. As a result,\\nsuch a fermionic computer can perform certain operations more rapidly than a conventional\\nquantum computer. An obvious example of a problem that can be solved more rapidly on\\na fermionic quantum computer is the problem of simulating a lattice fermionic system in\\ntwo or more dimensions. To get the antisymmetrization right in second quantized form,\\na conventional ‘Boltzmann’ quantum computer takes time proportional to Tℓd−1 where T\\nis the time over which the simulation is to take place, ℓis the length of the lattice and\\nd is the dimension, while a fermionic quantum computer takes time proportional to T.\\n(Here we assume that the computations for both conventional and Fermionic quantum\\ncomputers can take advantage of the intrinsic parallelizability of such simulations: if the\\ncomputations are performed serially an additional factro of ℓd is required for both types\\nof computer to update each site sequentially.)\\nAs the lattice size ℓand the dimension d grow large, the diﬀerence between the two\\ntypes of computer also grows large. Indeed, the problem of simulating fermions hopping\\non a hypercube of dimension d as d →∞is evidently exponentially harder on a con-\\nventional quantum computer than a Fermionic quantum computer.\\nSince a variety of\\ndiﬃcult problems such as the travelling-salesman problem and data-base search problem\\ncan be mapped to particles hopping on a hypercube, it is interesting to speculate whether\\nfermionic computers might provide an exponential speed-up on problems of interest in ad-\\ndition to quantum simulation. No such problems are currently known, however. Fermionic\\ncomputers could be realized in principle by manipulating the ways in which electrons and\\n3\\nholes hop from site to site on a semiconductor lattice (though problems of decoherence are\\nlikely to be relatively severe for such systems).\\nIt might also be possible to construct bosonic computers using photons, phonons, or\\natoms in a Bose-Einstein condensate. Such systems can be highly coherent and support\\nnonlinear interactions: phonons and photons can interact in a nonlinear fshion via their\\ncommon nonlinear interaction with matter, and atoms in a Bose condensate can be made\\nto interact bia quantum electrodynamics (by introduction of a cavity) or by collisions. So\\nfar, however, the feature of Bose condensates that makes them so interesting from the point\\nof view of physics — all particles in the same state — makes them less interesting from the\\npoint of view of quantum computation. Many particles in the same state, which can be\\nmanipulated coherently by a variety of techniques, explore the same volume of Hilbert space\\nas a single particle in that state. As a result, it is unclear how such a bosonic system could\\nprovide a speed-up over conventional quantum computation. More promising than Bose\\ncondensates from the perspective of quantum computation and quantum communications,\\nis the use of cavity quantum electrodynamics to ‘dial up’ or synthesize arbitrary states\\nof the cavity ﬁeld. Such a use of bosonic states is important for the ﬁeld of quantum\\ncommunications, which requires the ability to create and manipulate entangled states of\\nthe electromagnetic ﬁeld.\\nA third unconventional design for a quantum computer relies on ‘exotic’ statistics\\nthat are neither fermionic nor bosonic. Kitaev has recently proposed a quantum computer\\narchitecture based on ‘anyons,’ particles that when exchanged acquuire an arbitrary phase.\\nExamples of anyons include two-dimensional topological defects in lattice systems of spins\\nwith various symmetries. Kitaev noted that such anyons could perform quantum logic via\\nAharonov-Bohm type interactions [19]. Preskill et al. have shown explicitly how anyonic\\nsystems could compute in principle [20], and Lloyd et al.\\nhave proposed methods of\\nrealizing anyons using superconducting circuits (they could also in principle be constructed\\nusing NMR quantum computers to mock up the anyonic dynamics in an eﬀectively two-\\ndimensional space of spins) [21]. The advantage of using anyons for quantum computation\\nis that their nonlocal topological nature can make them intrinsically error-correcting and\\nvirtually immune to the eﬀects of noise and interference.\\nAs the technologies of the microscale become better developed, more and more po-\\ntential designs for quantum computers, both conventional and unconventional, are likely\\nto arise. Additional technologies that could prove useful for the construction of quantum\\n4\\nlogic devices include photonic crystals, optical hole-burning techniques, electron spin res-\\nonance, quantum dots, superconducting circuits in the quantum regime, etc. Since every\\nquantum degree of freedom can in principle participate in a computation one cannot a\\npriori rule out the possibility of using currently hard to control degrees of freedom such as\\nquark and gluon in complex nuclei to process information. Needless to say, most if not all\\nof the designs inspired by these technologies are likely to fail. There is room for optimism\\nthat some such quantum computer designs will prove practicable, however.\\nThe preceding unconventional designs for quantum computers were based on existing,\\nexperimentally conﬁrmed physical phenomena (except in the case of non-abelian anyons).\\nLet us now turn to designs based on speculative, hypothetical, and not yet veriﬁed phenom-\\nena. (One of the most interesting of these phenomena is large-scale quantum computation\\nitself: can we create and systematically transform entangled states involving hundreds or\\nthousands of quantum variables?) A particularly powerful hypothesis from the point of\\nview of quantum computation is that of nonlinear quantum mechanics.\\nThe conventional picture of quantum mechanics is that it is linear in the sense that the\\nsuperposition principle is obeyed exactly. (Of course, quantum systems can still exhibit\\nnonlinear interactions between degrees of freedom while continuing to obey the superpo-\\nsition principle.) Experiment conﬁrms that the superposition principle is indeed obeyed\\nto a high degree of accuracy. Nonetheless, a number of scientists including Weinberg have\\nproposed nonlinear versions of quantum mechanics in which the superposition principle\\nis violated. Many of these proposals exhibit pathologies such as violations of the second\\nlaw of thermodynamics or the capacity for superluminal communication. Despite such\\ntheoretical diﬃculties, it is still possible that quantum mechanics does indeed possess a\\nsmall nonlinearity, even if it currently seems unlikely. If a nonlinear operation such as\\nthat proposed by Weinberg can be incorporated in a quantum logic operation, then the\\nconsequences are striking: NP-complete problems can be solved easily in polynomial time\\n[17]. Indeed, NP-oracle problems and all problems in #P can be solved in polynomial time\\non such a nonlinear quantum computer.\\nA general proof of this result is given in [17], however, a simple argument for why\\nthis is so can be seen as follows. Suppose that it is possible to perform a non-unitary\\noperation on a single qubit that has a positive Lyapunov exponent over some region: i.e.,\\nsomewhere on the unit sphere there exists a line of ﬁnite extent along which application of\\nthe operation causes nearby points to move apart exponentially at a rate eλ∆θ proportional\\n5\\nto their original angular separation δθ. Now consider a function f(x) from N bits to one\\nbit. We wish to determine whether or not there exists an x such that f(x) = 1, and if\\nso, how many such x’s there are. Using the nonlinear operation with positive Lyapunov\\nexponent, it is straightforward to construct a mapping leaves a point on the exponentially\\nexpanding line (call this point |0⟩) ﬁxed if their are no solutions to the equation f(x) = 1,\\nand that maps the point to a nearby point cos(n/2N)|0⟩+ sin(n/2N)|1⟩along the line\\nif there are exactly n solutions to the equation f(x) = 1. Repeated application of the\\nnonlinear map can be used to drive the points apart at an exponentional rate: eventually,\\nat a time determined by the number of qubits N, the number of solutions n, and the rate\\nof spreading λ, the two points will become macroscopically distinguishable, allowing one\\nto determine whether or not there is a solution and if there is, how many solutions there\\nare. The map f need only be applied once, and the amount of time it takes to reveal the\\nnumber of solutions is proportional to N.\\nThe fact that nonlinear quantum mechanics allows the straightforward solution of\\nNP-complete and #P problems should probably be regarded as yet another strike against\\nnonlinear quantum mechanics. Whether or not quantum mechanics is linear is a question\\nto be resolved experimentally, however. In the unlikely event that quantum mechanics\\ndoes turn out to be nonlinear, all our problems may be solved.\\nFinally, let us turn our attention to hypothetical quantum Theories of Everything,\\nsuch as string theory. Such a theory must clearly support quantum computation since it\\nsupports cavity quantum electrodynamics and nuclear magnetic resonance. The obvious\\nquestion to ask is then, does a Theory of Everything need to support anything more than\\nquantum computation? So far as experimental evidence is concerned the answer to this\\nquestion is apparently No: we have no evident reason to doubt that the universe is at\\nbottom anything more than a giant, parallel, quantum information processing machine,\\nand that the phenomena that we observe and attempt to characterize are simply outputs\\nof this machine’s ongoing computation. Of course, just how the universe is carrying out\\nthis computation is likely to remain a question of great interest for some time.\\nTo summarize: Computers are physical systems, and what they can do in practice and\\nin principle is circumscribed by the laws of physics. The laws of physics in turn permit a\\nwide variety of quantum computational devices including some based on nonconventional\\nstatistics and exotic eﬀects. Modiﬁcations made to the laws of physics have the consequence\\nthat what can be computed in practice and in principle changes. A particularly intriguing\\n6\\nvariation on conventional physics is nonlinear quantum mechanics which, if true, would\\nallow hard problems to be solved easily.\\n7\\nReferences\\n1. P. Benioﬀ, ‘Quantum Mechanical Models of Turing Machines that Dissipate No Energy,’\\nPhysical Review Letters, Vol. 48, No. 23, pp. 1581-1585 (1982)\\n2. D. Deutsch, ‘Quantum Theory, the Church-Turing Principle and the Universal Quantum\\nComputer,’ Proceedings of the Royal Society of London, A, Vol. 400, pp. 97-117 (1985).\\n3. R.P. Feynman, ‘Quantum Mechanical Computers,’ Optics News, Vol. 11, pp. 11-20\\n(1985); also in Foundations of Physics, Vol. 16, pp. 507-531 (1986).\\n4. S. Lloyd, ‘A Potentially Realizable Quantum Computer,’ Science, Vol. 261, pp. 1569-\\n1571 (1993).\\n5. J.I. Cirac and P. Zoller, ‘Quantum Computations with Cold Trapped Ions,’ Physical\\nReview Letters, Vol. 74, pp. 4091-4094 (1995).\\n6. Q.A. Turchette, C.J. Hood, W. Lange, H. Mabuchi, H.J. Kimble, ‘Measurement of\\nConditional Phase Shifts for Quantum Logic,’ Physical Review Letters, Vol. 75, pp. 4710-\\n4713 (1995).\\n7. C. Monroe, D.M. Meekhof, B.E. King, W.M. Itano, D.J. Wineland, ‘Demonstration of\\na Fundamental Quantum Logic Gate,’ Physical Review Letters, Vol. 75, pp. 4714-4717\\n(1995).\\n8. D.G. Cory, A.F. Fahmy, T.F. Havel, ‘Nuclear Magnetic Resonance Spectroscopy: an\\nexperimentally accessible paradigm for quantum computing,’ in PhysComp96, Proceedings\\nof the Fourth Workshop on Physics and Computation, T. Toﬀoli, M. Biafore, J. Le˜ao, eds.,\\nNew England Complex Systems Institute, 1996, pp. 87-91.\\n9.\\nN.A. Gershenfeld and I.L. Chuang, ‘Bulk Spin-Resonance Quantum Computation,’\\nScience, Vol. 275, pp. 350-356 (1997).\\n10.\\nP. Shor, ‘Algorithms for Quantum Computation: Discrete Log and Factoring,’ in\\nProceedings of the 35th Annual Symposium on Foundations of Computer Science, S. Gold-\\nwasser, Ed., IEEE Computer Society, Los Alamitos, CA, 1994, pp. 124-134.\\n11. R.P. Feynman, ‘Simulating Physics with Computers,’ International Journal of Theo-\\nretical Physics, Vol. 21, pp. 467-488 (1982).\\n12. S. Lloyd, ‘Universal Quantum Simulators,’ Science, Vol. 273, pp. 1073-1078 (1996).\\n13. L.K. Grover, ‘Quantum Mechanics Helps in Searching for a Needle in a Haystack,’\\nPhysical Review Letters, Vol. 79, pp. 325-328 (1997).\\n14. D. Deutsch, A. Barenco, A. Ekert, ‘Universality in Quantum Computation,’ Proceed-\\nings of the Royal Society of London A, Vol. 449, pp. 669-677 (1995).\\n8\\n15. S. Lloyd, ‘Almost Any Quantum Logic Gate is Universal,’ Physical Review Letters,\\nVol. 75, pp. 346-349 (1995).\\n16. S. Lloyd, ‘Fermionic Quantum Computers,’ talk delivered at the Santa Barbara work-\\nshop on Physics of Information, November 1996.\\n17. D. Abrams and S. Lloyd, to be published.\\n18. J. Preskill et al., to be published.\\n19. Yu. Kitaev, to be published.\\n20. J. Preskill et al., to be published.\\n21. S. Lloyd et al. to be published.\\n9\\n'),\n",
       " Document(metadata={'Published': '2013-11-20', 'Title': 'Geometrical perspective on quantum states and quantum computation', 'Authors': 'Zeqian Chen', 'Summary': \"We interpret quantum computing as a geometric evolution process by\\nreformulating finite quantum systems via Connes' noncommutative geometry. In\\nthis formulation, quantum states are represented as noncommutative connections,\\nwhile gauge transformations on the connections play a role of unitary quantum\\noperations. Thereby, a geometrical model for quantum computation is presented,\\nwhich is equivalent to the quantum circuit model. This result shows a geometric\\nway of realizing quantum computing and as such, provides an alternative\\nproposal of building a quantum computer.\"}, page_content='arXiv:1311.4939v1  [quant-ph]  20 Nov 2013\\nGeometrical perspective on quantum states and quantum computation\\nZeqian Chen\\nState Key Laboratory of Resonances and Atomic and Molecular Physics,\\nWuhan Institute of Physics and Mathematics, Chinese Academy of Sciences,\\n30 West District, Xiao-Hong-Shan, Wuhan 430071, China\\nWe interpret quantum computing as a geometric evolution process by reformulating ﬁnite quantum\\nsystems via Connes’ noncommutative geometry. In this formulation, quantum states are represented\\nas noncommutative connections, while gauge transformations on the connections play a role of\\nunitary quantum operations. Thereby, a geometrical model for quantum computation is presented,\\nwhich is equivalent to the quantum circuit model. This result shows a geometric way of realizing\\nquantum computing and as such, provides an alternative proposal of building a quantum computer.\\nPACS numbers: 03.67.Lx, 03.65.Aa\\nQuantum computation has the advantage of solving\\neﬃciently some problems that are considered intractable\\nby using conventional classical computation [1]. In this\\ncontext, there are two remarkable algorithms found:\\nShor’s factoring algorithm [2] and Grove’s search algo-\\nrithm [3].\\nBut it remains a challenge to ﬁnd eﬃcient\\nquantum circuits that can perform these complicated\\ntasks in practice, due to quantum decoherence. A cru-\\ncial step in the theory of quantum computer has been\\nthe discovery of error-correcting quantum codes [4] and\\nfault-tolerant quantum computation [5, 6], which estab-\\nlished a threshold theorem that proves that quantum de-\\ncoherence can be corrected as long as the decoherence is\\nsuﬃciently weak. To tackle this barrier, a revolutionary\\nstrategy, topological quantum computation (see [7] and\\nreferences therein), is to make the system immune to the\\nusual sources of quantum decoherence, by involving the\\nglobally robust topological nature of the computation.\\nRecently, substantial progress in this ﬁeld has been made\\non both theoretical and experimental fronts [8].\\nIn this paper, we provide an alternative approach to\\nquantum computation from a geometrical view of point.\\nTo this end, we need to reformulate quantum mechanics\\nvia Connes’ noncommutative geometry [9]. In this for-\\nmulation, quantum states are represented as noncommu-\\ntative connections, while gauge transformations on the\\nconnections play a role of unitary quantum operations.\\nIn this way, we present a geometrical model for quan-\\ntum computation, which is equivalent to the quantum\\ncircuit model. In this computational model, information\\nis encoded in gauge states instead of quantum states and\\nimplementing on gauge states is played by gauge transfor-\\nmations. Therefore, our scheme shows a geometric way\\nof realizing quantum computing and as such, provides an\\nalternative proposal of building a quantum computer.\\nLet H be a N dimensional Hilbert space associated\\nwith a ﬁnite quantum system. Let A be the algebra of\\nall (bounded) linear operators on H, and let U(A) = {u ∈\\nA : uu∗= u∗u = I} with I being the unit operator on\\nH. Given a selfadjoint operator D on H, (A, H, D) is a\\nspectral triple in the sense of noncommutative geometry\\n[9, 10]. A (noncommutative) connection on (A, H, D) is\\ndeﬁned to be a selfadjoint operator V on H of the form\\nthat follows\\nV =\\nX\\nj\\naj[D, bj]\\n(1)\\nwhere aj, bj ∈A and [a, b] = ab −ba. A gauge transform\\non a connection V under u ∈U(A) is deﬁned as\\nV 7−→Gu(V ) = uV u∗+ u[D, u∗].\\n(2)\\nFor avoiding triviality, we always assume that D ̸= 0 or\\nI in what follows.\\nFor any (pure) quantum state |ψ⟩⟨ψ| with ψ being a\\nunit vector in H, we have\\n|ψ⟩⟨ψ| = |ψ⟩⟨ϕ|i[D, b]|ϕ⟩⟨ψ|\\nwhere i = √−1 and, b is a selfjoint operator on H such\\nthat i[D, b] has eigenvalue 1 at |ϕ⟩. Such a selfjoint oper-\\nator b always exists because D ̸= 0 or I. In this case,\\n|ψ⟩⟨ψ| = ia∗[D, ba] −ia∗b[D, a]\\n(3)\\nwith a = |ϕ⟩⟨ψ|. Thus, every quantum state |ψ⟩⟨ψ| can\\nbe represented as a connection, denoted by Vψ, i.e.,\\nVψ = ia∗[D, ba] −ia∗b[D, a].\\n(4)\\nLet GD(H) be the set of all connections V which can\\nbe written as V = Vψ + uDu∗−D with ψ being a unit\\nvector in H and u ∈U(A). An element in GD(H) is said\\nto be a gauge state on (A, H, D). Any quantum state is\\nnecessarily a gauge state, but a gauge state need not to\\nbe a quantum state. However, any gauge state V can be\\nobtained from a quantum state by performing a gauge\\ntransform. Indeed, if V = Vψ + uDu∗−D then V =\\nGu(Vu∗ψ). Moreover, for any gauge state V on (A, H, D)\\nwe have (see [11])\\n• for any u ∈U(A), Gu(V ) is again a gauge state;\\n• Guv(V ) = Gu(Gv(V )) for all u, v ∈U(A).\\nTherefore, a gauge transform preserves gauge states.\\nLet V be a gauge state which is prepared from a quan-\\ntum state |ψ⟩⟨ψ| by operating a gauge transform Gu, i.e.,\\n2\\nV = Gu(Vψ). For any event E, the probability of E oc-\\ncurring on V is\\n⟨E⟩V = ⟨ψ|u∗Eu|ψ⟩.\\n(5)\\nNote that a gauge state may be prepared in several ways.\\nHence, the probability of a event E occurring on a gauge\\nstate V depends on the quantum state from which V is\\nprepared.\\nLet H be a selfadjoint operator on H. Assuming ut =\\neitH for t ∈R, we have that the gauge transforms Vt =\\nGt(V ) on a ﬁxed gauge state V under ut form a group\\n(see [11]), that is,\\nGt+s(V ) = Gt(Gs(V )).\\n(6)\\nThis yields a dynamical equation governed by the Hamil-\\ntonian H for gauge states on (A, H, D) as follows [12]\\nidVt\\ndt = [Vt, H] + [D, H]\\n(7)\\nwith V0 = V. In particular, for a unit vector ψ we have\\nVt = Gt(Vψ) = Vutψ + utDu∗\\nt −D.\\n(8)\\nWe now turn to product of two spectral triples. Sup-\\npose (Ai, Hi, Di), i = 1, 2, are two spectral triple associ-\\nated with ﬁnite quantum systems. Put\\nD = D1 ⊗I2 + I1 ⊗D2\\n(9)\\nwith Ii being the unit operator on Hi (i = 1, 2). Then\\nD is a selfjoint operator on H1 ⊗H2. The spectral triple\\n(A1⊗A2, H1⊗H2, D) is called the product of two spectral\\ntriples (Ai, Hi, Di), i = 1, 2.\\nNow we illustrate our scheme by using a qubit. Let\\nH = C2 and\\nσx =\\n\\x14\\n0 1\\n1 0\\n\\x15\\n, σy =\\n\\x14\\n0 −i\\ni\\n0\\n\\x15\\n, σz =\\n\\x14\\n1\\n0\\n0 −1\\n\\x15\\n.\\n(10)\\nThen (M2, C2, D) is a spectral triple with D = σx, where\\nM2 is the set of all 2×2 complex matrices. For |0⟩=\\n\\x14\\n1\\n0\\n\\x15\\n,\\nwe have\\nV|0⟩=\\n\\x14\\n1 0\\n0 0\\n\\x15\\n,\\nGσx(V|0⟩) =\\n\\x14\\n0 0\\n0 1\\n\\x15\\n,\\nand\\nGσy(V|0⟩) =\\n\\x14\\n0\\n−2\\n−2\\n1\\n\\x15\\n,\\nGσz(V|0⟩) =\\n\\x14\\n1\\n−2\\n−2\\n0\\n\\x15\\n.\\nFor |1⟩=\\n\\x14\\n0\\n1\\n\\x15\\n, we have\\nV|1⟩=\\n\\x14\\n0 0\\n0 1\\n\\x15\\nand\\nGσy(V|1⟩) =\\n\\x14\\n1\\n−2\\n−2\\n0\\n\\x15\\n.\\nHence Gσy(V|1⟩) = Gσz(V|0⟩) and so, the gauge state\\nV =\\n\\x14\\n1\\n−2\\n−2\\n0\\n\\x15\\ncan be prepared in two diﬀerent ways.\\nWe are now ready to interpret quantum computation\\nfrom a geometrical view of point.\\nBut let us take a\\nstep backward and discuss the standard quantum circuit\\nmodel for computation [13]. Let H = (C2)⊗n, the tensor\\nproduct of n copies of C2. A quantum circuit model on\\nn qubits consists of\\n• a initial state |ψ⟩, represented by a unit vector ψ ∈\\nH;\\n• a quantum circuit Γ = UNUN−1 · · · U1, where quan-\\ntum “gates” Uk 1 ≤k ≤N, are unitary transfor-\\nmations on either C2\\ni or C2\\ni ⊗C2\\nj, 1 ≤i, j ≤n, the\\nidentity on all remaining factors;\\n• reading the output of the circuit Γ|ψ⟩by measuring\\nthe ﬁrst qubit; the probability of observing |1⟩is\\nP(Γ) = ⟨ψ|Γ∗Π1Γ|ψ⟩, where Π1 = |1⟩⟨1| ⊗I · · · ⊗I\\nis the projection to |1⟩in the ﬁrst qubit.\\nLet A = M2n. Put\\nD =\\nn\\nX\\ni=1\\nI ⊗· · · ⊗I\\n|\\n{z\\n}\\ni−1\\n⊗σx ⊗I · · · ⊗I\\nwhere I is the identity on C2. A computational model\\nbased on the spectral triple (A, H, D) is as follows:\\n• Initialization of a gauge state Vψ in the spectral\\ntriple (A, H, D), where ψ is a unit vector in H;\\n• Gauge implementation of the computational pro-\\ngram\\nG(Γ) = GUN GUN−1 · · · GU1\\nwhere “gates” GUk, 1 ≤k ≤N, are gauge transfor-\\nmations induced by Uk;\\n• Application of the projection operator Π1 for read-\\ning the output of the computation G(Γ)(Vψ);\\nthe probability of observing |1⟩is P(GΓ)\\n=\\n⟨ψ|Γ∗Π1Γ|ψ⟩because G(Γ)(Vψ) = GΓ(Vψ) (see\\n[11]), i.e., G(Γ)(Vψ) = Γ|ψ⟩⟨ψ|Γ∗+ ΓDΓ∗−D.\\nThus, we obtain a geometrical model on n qubits for\\nquantum computation, which is evidently equivalent to\\nthe quantum circuit model as described above. Due to\\nthe essential role of gauge transformations played in this\\ncomputational model, we call this scheme gauge quantum\\ncomputation.\\nAs illustration, we give the Deutsch-Jozsa algorithm\\n[14] in gauge quantum computation. Let f : {0, 1}n 7→\\n{0, 1} be a function that takes an n-bit into a bit. We\\ncall f balanced if f(x) = 1 for exactly half of all possible\\n3\\nx and f(x) = 0 for the other half.\\nGiven a function\\nf that is either constant or balanced, we want to ﬁnd\\nout which it is with certainty. More precisely, we select\\none x ∈{0, 1}n and calculate f(x) with the result being\\neither 0 or 1. What is the fewest number of queries that\\nwe can make to determine whether or not f is constant?\\nIn the classical case, at worst we will need to calculate f\\n2n−1 + 1 times, because we may ﬁrst obtain 2n−1 zeros\\nand will need one more query to decide.\\nHowever, in\\nthe setting of quantum computation we could achieve the\\ngoal in just one query using the Deutsch-Jozsa algorithm.\\nIn the sequel, we give a realization of the Deutsch-Jozsa\\nalgorithm in gauge quantum computation.\\nLet H = (C2)⊗(n+1) and A = M2n+1. Given a selfad-\\njoint operator D on H that is not 0 or I, we get the desired\\nspectral triple (A, H, D). For a given f, we deﬁne the as-\\nsociated operator Uf on H as Uf|x, y⟩= |x, y ⊕f(x)⟩for\\nx ∈{0, 1}n and y ∈{0, 1}. Recall that the Hadamard\\noperator H on C2 is\\nH =\\n1\\n√\\n2\\nX\\nx,y∈{0,1}\\n(−1)x·y|x⟩⟨y|\\nwhere x·y signiﬁes ordinary multiplication. The following\\nis the Deutsch-Jozsa algorithm in the setting of gauge\\nquantum computation:\\n• Initialization of a gauge state Vψ with ψ = |0⟩⊗n ⊗\\n|1⟩;\\n• Gauge implementation of the computational pro-\\ngram G(Γ) = GH⊗n⊗IGUf GH⊗(n+1);\\n• Application of the projection operator Π|0⟩⊗n for\\nreading the output of the computation G(Γ)(Vψ),\\nwhere Π|0⟩⊗n is the projection to |0⟩⊗n in the ﬁrst\\nn qubits.\\nThe ﬁnal gauge state is V = VΓψ + ΓDΓ∗−D with Γ =\\n(H⊗n ⊗I)UfH⊗(n+1), where\\nΓψ =\\nX\\nx,y∈{0,1}n\\n(−1)x·y+f(x)\\n2n\\n|y⟩⊗|0⟩−|1⟩\\n√\\n2\\n.\\nSince the amplitude for the state |0⟩⊗n in the ﬁrst n\\nqubits is P\\nx(−1)f(x)/2n, the probability of observing 0\\nis 1 if f is constant, or 0 if f is balanced. Thus we have\\ntwo possibilities of obtaining the outcome zero or the\\noutcome nonzero. In the ﬁrst case, f is certainly constant\\nand in the second case f must be balanced. Therefore,\\nwe only need to perform three times gauge transforms for\\ndetermining whether or not f is constant.\\nIn conclusion, we present a geometrical description of\\nquantum computation via noncommutative geometry. In\\nthis geometrical model, information is encoded in gauge\\nstates and computational operation is implemented by\\ngauge transforms instead of unitary transforms. In prin-\\nciple, gauge transforms are easier to perform than uni-\\ntary quantum operation [15]. Therefore, gauge quantum\\ncomputation should be more accessible than the usual\\nquantum circuit computation and as such, this provides\\nan alternative proposal of building a quantum computer.\\nThis work was supported in part by the NSFC under\\nGrant No. 11171338 and National Basic Research Pro-\\ngram of China under Grant No. 2012CB922102.\\n[1] M. A. Nielsen, I. L. Chuang, Quantum Computation\\nand Quantum Information (Cambridge University Press,\\nCambridge, 2000).\\n[2] P. Shor, Algorithms for quantum computation, discrete\\nlogarithms and factoring, Proc. 35th Annual Symposium\\non Foundations of Computer Science (IEEE Computer\\nSociety Press, Los Alamitos, CA, 1994, 124-134).\\n[3] L. Grover, Quantum mechanics helps in search for a nee-\\ndle in a haystack, Phys. Rev. Lett. 79 (1997), 325-328.\\n[4] P. Shor, Scheme for reducing decoherence in quantum\\ncomputer memory, Phys. Rev. A 52 (1995), 2493-2496.\\n[5] J. Preskill, Fault-tolerant quantum computation, arXiv:\\nquant-ph/9712048, 1997.\\n[6] P. Shor, Fault-tolerant quantum computation, Proc. 37th\\nAnnual Symposium on Foundations of Computer Science\\n(IEEE Computer Society Press, Los Alamitos, CA, 1996,\\n56-65).\\n[7] C. Nayak, S. H. Simon, A. Stern, M. Freedman, S. Das\\nSarma, Non-Abelian anyons and topological quantum\\ncomputation, Rev. Mod. Phys. 80 (2008), 1083-1159.\\n[8] A.\\nStern,\\nN.\\nH.\\nLindner,\\nTopological\\nquantum\\ncomputation–from basic concepts to ﬁrst experiments,\\nScience 339 (2013), 1179-1184.\\n[9] A. Connes, Noncommutative Geometry (Academic Press,\\nLondon, 1994).\\n[10] G. Landi, An Introduction to Noncommutative Spaces\\nand Their Geometries (Springer, Berlin, 1997).\\n[11] Since Gu(V ) = uV u∗+ uDu∗−D, it follows that\\nGu(Gv(V )) = u(vV v∗+ vDv∗−D)u∗+ uDu∗−D\\n= uvV (uv)∗+ uvD(uv)∗−D = Guv(V ).\\n[12] Since\\ndGt(V )\\ndt\\n= d(utV u∗\\nt )\\ndt\\n+ d(utDu∗\\nt )\\ndt\\n= i[H, utV u∗\\nt ] + i[H, utDu∗\\nt ] = i[H, Vt + D],\\nit follows (7).\\n[13] S. Gudder, Quantum computation, The American math-\\nematical monthly 110 (2003), 181-201.\\n[14] D. Deutsch, R. Jozsa, Rapid solutions of problems by\\nquantum computation, Proceedings of the Royal Society\\nof London A 439 (1992), 553-558.\\n4\\n[15] J. Madore, An Introduction to Noncommutative Diﬀer-\\nential Geometry and its Physical Applications (Second\\nedition, Cambridge University Press, Cambridge, 1999).\\n')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arvix_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# load web\n",
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = WebBaseLoader(\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)', 'title': 'Transformer (deep learning architecture) - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nTransformer (deep learning architecture) - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usSpecial pages\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nHistory\\n\\n\\n\\n\\nToggle History subsection\\n\\n\\n\\n\\n\\n1.1\\nPredecessors\\n\\n\\n\\n\\n\\n\\n\\n\\n1.2\\nAttention with seq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n1.3\\nParallelizing attention\\n\\n\\n\\n\\n\\n\\n\\n\\n1.4\\nAI boom era\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nTraining\\n\\n\\n\\n\\nToggle Training subsection\\n\\n\\n\\n\\n\\n2.1\\nMethods for stabilizing training\\n\\n\\n\\n\\n\\n\\n\\n\\n2.2\\nPretrain-finetune\\n\\n\\n\\n\\n\\n\\n\\n\\n2.3\\nTasks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nArchitecture\\n\\n\\n\\n\\nToggle Architecture subsection\\n\\n\\n\\n\\n\\n3.1\\nTokenization\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2\\nEmbedding\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3\\nUn-embedding\\n\\n\\n\\n\\n\\n\\n\\n\\n3.4\\nPositional encoding\\n\\n\\n\\n\\n\\n\\n\\n\\n3.5\\nEncoder-decoder (overview)\\n\\n\\n\\n\\n\\n\\n\\n\\n3.6\\nFeedforward network\\n\\n\\n\\n\\n\\n\\n\\n\\n3.7\\nScaled dot-product attention\\n\\n\\n\\n\\n\\n\\n3.7.1\\nAttention head\\n\\n\\n\\n\\n\\n\\n\\n\\n3.7.2\\nMultiheaded attention\\n\\n\\n\\n\\n\\n\\n\\n\\n3.7.3\\nMasked attention\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.8\\nEncoder\\n\\n\\n\\n\\n\\n\\n\\n\\n3.9\\nDecoder\\n\\n\\n\\n\\n\\n\\n\\n\\n3.10\\nAdapted architectures\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nFull transformer architecture\\n\\n\\n\\n\\nToggle Full transformer architecture subsection\\n\\n\\n\\n\\n\\n4.1\\nSublayers\\n\\n\\n\\n\\n\\n\\n\\n\\n4.2\\nPseudocode\\n\\n\\n\\n\\n\\n\\n\\n\\n4.3\\nTerminology\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nSubsequent work\\n\\n\\n\\n\\nToggle Subsequent work subsection\\n\\n\\n\\n\\n\\n5.1\\nAlternative activation functions\\n\\n\\n\\n\\n\\n\\n\\n\\n5.2\\nAlternative normalizations\\n\\n\\n\\n\\n\\n\\n\\n\\n5.3\\nAlternative positional encodings\\n\\n\\n\\n\\n\\n\\n5.3.1\\nRoPE\\n\\n\\n\\n\\n\\n\\n\\n\\n5.3.2\\nALiBi\\n\\n\\n\\n\\n\\n\\n\\n\\n5.3.3\\nRelative Position Encodings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5.4\\nEfficient implementation\\n\\n\\n\\n\\n\\n\\n5.4.1\\nKV caching\\n\\n\\n\\n\\n\\n\\n\\n\\n5.4.2\\nFlashAttention\\n\\n\\n\\n\\n\\n\\n\\n\\n5.4.3\\nMulti-Query Attention\\n\\n\\n\\n\\n\\n\\n\\n\\n5.4.4\\nSpeculative decoding\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5.5\\nSub-quadratic transformers\\n\\n\\n\\n\\n\\n\\n5.5.1\\nAlternative attention graphs\\n\\n\\n\\n\\n\\n\\n\\n\\n5.5.2\\nRandom Feature Attention\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5.6\\nMultimodality\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nApplications\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n8\\nNotes\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n10\\nFurther reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nTransformer (deep learning architecture)\\n\\n\\n\\n28 languages\\n\\n\\n\\n\\nالعربيةCatalàČeštinaDeutschEestiEspañolEuskaraفارسیFrançaisGaeilgeGalego한국어ՀայերենItalianoעברית日本語PolskiQaraqalpaqshaРусскийSimple EnglishکوردیСрпски / srpskiSvenskaไทยУкраїнськаTiếng Việt粵語中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nDeep learning architecture for modelling sequential data\\n\\nPart of a series onMachine learningand data mining\\nParadigms\\nSupervised learning\\nUnsupervised learning\\nSemi-supervised learning\\nSelf-supervised learning\\nReinforcement learning\\nMeta-learning\\nOnline learning\\nBatch learning\\nCurriculum learning\\nRule-based learning\\nNeuro-symbolic AI\\nNeuromorphic engineering\\nQuantum machine learning\\n\\nProblems\\nClassification\\nGenerative modeling\\nRegression\\nClustering\\nDimensionality reduction\\nDensity estimation\\nAnomaly detection\\nData cleaning\\nAutoML\\nAssociation rules\\nSemantic analysis\\nStructured prediction\\nFeature engineering\\nFeature learning\\nLearning to rank\\nGrammar induction\\nOntology learning\\nMultimodal learning\\n\\nSupervised learning(classification\\xa0• regression) \\nApprenticeship learning\\nDecision trees\\nEnsembles\\nBagging\\nBoosting\\nRandom forest\\nk-NN\\nLinear regression\\nNaive Bayes\\nArtificial neural networks\\nLogistic regression\\nPerceptron\\nRelevance vector machine (RVM)\\nSupport vector machine (SVM)\\n\\nClustering\\nBIRCH\\nCURE\\nHierarchical\\nk-means\\nFuzzy\\nExpectation–maximization (EM)\\nDBSCAN\\nOPTICS\\nMean shift\\n\\nDimensionality reduction\\nFactor analysis\\nCCA\\nICA\\nLDA\\nNMF\\nPCA\\nPGD\\nt-SNE\\nSDL\\n\\nStructured prediction\\nGraphical models\\nBayes net\\nConditional random field\\nHidden Markov\\n\\nAnomaly detection\\nRANSAC\\nk-NN\\nLocal outlier factor\\nIsolation forest\\n\\nArtificial neural network\\nAutoencoder\\nDeep learning\\nFeedforward neural network\\nRecurrent neural network\\nLSTM\\nGRU\\nESN\\nreservoir computing\\nBoltzmann machine\\nRestricted\\nGAN\\nDiffusion model\\nSOM\\nConvolutional neural network\\nU-Net\\nLeNet\\nAlexNet\\nDeepDream\\nNeural radiance field\\nTransformer\\nVision\\nMamba\\nSpiking neural network\\nMemtransistor\\nElectrochemical RAM (ECRAM)\\n\\nReinforcement learning\\nQ-learning\\nSARSA\\nTemporal difference (TD)\\nMulti-agent\\nSelf-play\\n\\nLearning with humans\\nActive learning\\nCrowdsourcing\\nHuman-in-the-loop\\nRLHF\\n\\nModel diagnostics\\nCoefficient of determination\\nConfusion matrix\\nLearning curve\\nROC curve\\n\\nMathematical foundations\\nKernel machines\\nBias–variance tradeoff\\nComputational learning theory\\nEmpirical risk minimization\\nOccam learning\\nPAC learning\\nStatistical learning\\nVC theory\\nTopological deep learning\\n\\nJournals and conferences\\nECML PKDD\\nNeurIPS\\nICML\\nICLR\\nIJCAI\\nML\\nJMLR\\n\\nRelated articles\\nGlossary of artificial intelligence\\nList of datasets for machine-learning research\\nList of datasets in computer vision and image processing\\nOutline of machine learning\\nvte\\nA standard Transformer architecture, showing on the left an encoder, and on the right a decoder. Note: it uses the pre-LN convention, which is different from the post-LN convention used in the original 2017 Transformer.\\nThe transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\".[1] Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.[3]\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers).\\nHistory[edit]\\nSee also: Timeline of machine learning\\nPredecessors[edit]\\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.\\nA key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.[16] One of its two networks has \"fast weights\" or \"dynamic links\" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This was later shown to be equivalent to the unnormalized linear Transformer.[20][21]\\n\\nAttention with seq2seq[edit]\\nMain article: Seq2seq §\\xa0History\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers[22][23]). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]\\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM).[23] Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]\\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.[26]\\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".[4]\\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]\\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]\\n\\nParallelizing attention[edit]\\nMain article: Attention (machine learning) §\\xa0History\\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\".[31] That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]\\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.[33]\\n\\n\\nAI boom era[edit]\\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.[34] Transformer architecture is now used in many generative models that contribute to the ongoing AI boom.\\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model.[35] In 2019 October, Google started using BERT to process search queries.[36] In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.[37]\\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular,[38] triggering a boom around large language models.[39][40]\\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer,[41] speech recognition,[42] robotics,[6] and multimodal.[43] The vision transformer, in turn, stimulated new developments in convolutional neural networks.[44] Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024),[45] and Sora (2024), are based on the Transformer architecture.\\n\\nTraining[edit]\\nMethods for stabilizing training[edit]\\nThe plain transformer architecture had difficulty converging. In the original paper[1] the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\\nA 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.[46]\\n\\nPretrain-finetune[edit]\\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\\n\\nlanguage modeling[12]\\nnext-sentence prediction[12]\\nquestion answering[3]\\nreading comprehension\\nsentiment analysis[1]\\nparaphrasing[1]\\nThe T5 transformer report[47] documents a large number of natural language pretraining tasks. Some examples are:\\n\\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you\\u202f~~\\u202fme to your party\\u202f~~\\u202fweek\", might generate the output, \"Thank you for inviting me to your party last week\".\\ntranslation between natural languages (machine translation)\\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\",[48] because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\\n\\nTasks[edit]\\nSee also: Large language model §\\xa0Evaluation\\nIn general, there are 3 classes of language modelling tasks: \"masked\",[49] \"autoregressive\",[50] and \"prefixLM\".[51] These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\\nIn a masked task,[49] one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \\n\\n\\n\\n\\nLoss\\n\\n=\\n−\\n\\n∑\\n\\nt\\n∈\\n\\nmasked tokens\\n\\n\\n\\nln\\n\\u2061\\n(\\n\\nprobability of\\xa0\\n\\nt\\n\\n\\xa0conditional on its context\\n\\n)\\n\\n\\n{\\\\displaystyle {\\\\text{Loss}}=-\\\\sum _{t\\\\in {\\\\text{masked tokens}}}\\\\ln({\\\\text{probability of }}t{\\\\text{ conditional on its context}})}\\n\\nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\\nIn an autoregressive task,[50] the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\\nIn a prefixLM task,[51] the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\\n\\nArchitecture[edit]\\nAll transformers have the same primary components:\\n\\nTokenizers, which convert text into tokens.\\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\\nThe following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\\nBy convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \\n\\n\\n\\nx\\nW\\n\\n\\n{\\\\displaystyle xW}\\n\\n.\\n\\nTokenization[edit]\\nMain article: Lexical analysis\\nAs the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.\\nThe set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \\n\\n\\n\\n\\nn\\n\\nvocabulary\\n\\n\\n\\n\\n{\\\\displaystyle n_{\\\\text{vocabulary}}}\\n\\n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\\nSome commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\\n\\nEmbedding[edit]\\nFurther information: Word embedding\\nEach token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \\n\\n\\n\\nM\\n\\n\\n{\\\\displaystyle M}\\n\\n. For example, if the input token is \\n\\n\\n\\n3\\n\\n\\n{\\\\displaystyle 3}\\n\\n, then the one-hot representation is \\n\\n\\n\\n[\\n0\\n,\\n0\\n,\\n0\\n,\\n1\\n,\\n0\\n,\\n0\\n,\\n…\\n]\\n\\n\\n{\\\\displaystyle [0,0,0,1,0,0,\\\\dots ]}\\n\\n, and its embedding vector is\\n\\n\\n\\n\\nE\\nm\\nb\\ne\\nd\\n\\n(\\n3\\n)\\n=\\n[\\n0\\n,\\n0\\n,\\n0\\n,\\n1\\n,\\n0\\n,\\n0\\n,\\n…\\n]\\nM\\n\\n\\n{\\\\displaystyle \\\\mathrm {Embed} (3)=[0,0,0,1,0,0,\\\\dots ]M}\\n\\nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. \\nThe number of dimensions in an embedding vector is called hidden size or embedding size and written as \\n\\n\\n\\n\\nd\\n\\nemb\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{emb}}}\\n\\n.[35] This size is written as \\n\\n\\n\\n\\nd\\n\\nmodel\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{model}}}\\n\\n in the original Transformer paper.[1]\\n\\nUn-embedding[edit]\\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\\nThe un-embedding layer is a linear-softmax layer:\\n\\n\\n\\n\\nU\\nn\\nE\\nm\\nb\\ne\\nd\\n\\n(\\nx\\n)\\n=\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n(\\nx\\nW\\n+\\nb\\n)\\n\\n\\n{\\\\displaystyle \\\\mathrm {UnEmbed} (x)=\\\\mathrm {softmax} (xW+b)}\\n\\nThe matrix has shape \\n\\n\\n\\n(\\n\\nd\\n\\nemb\\n\\n\\n,\\n\\nn\\n\\nvocabulary\\n\\n\\n)\\n\\n\\n{\\\\displaystyle (d_{\\\\text{emb}},n_{\\\\text{vocabulary}})}\\n\\n. The embedding matrix \\n\\n\\n\\nM\\n\\n\\n{\\\\displaystyle M}\\n\\n and the un-embedding matrix \\n\\n\\n\\nW\\n\\n\\n{\\\\displaystyle W}\\n\\n are sometimes required to be transposes of each other, a practice called weight tying.[52]\\n\\nPositional encoding[edit]\\nA diagram of a sinusoidal positional encoding with parameters \\n\\n\\n\\nN\\n=\\n10000\\n,\\nd\\n=\\n100\\n\\n\\n{\\\\displaystyle N=10000,d=100}\\n\\n\\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\\nThe positional encoding is defined as a function of type \\n\\n\\n\\nf\\n:\\n\\nR\\n\\n→\\n\\n\\nR\\n\\n\\nd\\n\\n\\n;\\nd\\n∈\\n\\nZ\\n\\n,\\nd\\n>\\n0\\n\\n\\n{\\\\displaystyle f:\\\\mathbb {R} \\\\to \\\\mathbb {R} ^{d};d\\\\in \\\\mathbb {Z} ,d>0}\\n\\n, where \\n\\n\\n\\nd\\n\\n\\n{\\\\displaystyle d}\\n\\n is a positive even integer. The full positional encoding defined in the original paper[1] is:\\n\\n\\n\\n(\\nf\\n(\\nt\\n\\n)\\n\\n2\\nk\\n\\n\\n,\\nf\\n(\\nt\\n\\n)\\n\\n2\\nk\\n+\\n1\\n\\n\\n)\\n=\\n(\\nsin\\n\\u2061\\n(\\nθ\\n)\\n,\\ncos\\n\\u2061\\n(\\nθ\\n)\\n)\\n\\n∀\\nk\\n∈\\n{\\n0\\n,\\n1\\n,\\n…\\n,\\nd\\n\\n/\\n\\n2\\n−\\n1\\n}\\n\\n\\n{\\\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\\\sin(\\\\theta ),\\\\cos(\\\\theta ))\\\\quad \\\\forall k\\\\in \\\\{0,1,\\\\ldots ,d/2-1\\\\}}\\n\\nwhere \\n\\n\\n\\nθ\\n=\\n\\n\\nt\\n\\nr\\n\\nk\\n\\n\\n\\n\\n,\\nr\\n=\\n\\nN\\n\\n2\\n\\n/\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle \\\\theta ={\\\\frac {t}{r^{k}}},r=N^{2/d}}\\n\\n.\\nHere, \\n\\n\\n\\nN\\n\\n\\n{\\\\displaystyle N}\\n\\n is a free parameter that should be significantly larger than the biggest \\n\\n\\n\\nk\\n\\n\\n{\\\\displaystyle k}\\n\\n that would be input into the positional encoding function. The original paper uses \\n\\n\\n\\nN\\n=\\n10000\\n\\n\\n{\\\\displaystyle N=10000}\\n\\n.\\nThe function is in a simpler form when written as a complex function of type \\n\\n\\n\\nf\\n:\\n\\nR\\n\\n→\\n\\n\\nC\\n\\n\\nd\\n\\n/\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle f:\\\\mathbb {R} \\\\to \\\\mathbb {C} ^{d/2}}\\n\\n\\n\\n\\n\\nf\\n(\\nt\\n)\\n=\\n\\n\\n(\\n\\ne\\n\\ni\\nt\\n\\n/\\n\\n\\nr\\n\\nk\\n\\n\\n\\n\\n)\\n\\n\\nk\\n=\\n0\\n,\\n1\\n,\\n…\\n,\\n\\n\\nd\\n2\\n\\n\\n−\\n1\\n\\n\\n\\n\\n{\\\\displaystyle f(t)=\\\\left(e^{it/r^{k}}\\\\right)_{k=0,1,\\\\ldots ,{\\\\frac {d}{2}}-1}}\\n\\nwhere \\n\\n\\n\\nr\\n=\\n\\nN\\n\\n2\\n\\n/\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle r=N^{2/d}}\\n\\n.\\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\\n\\n\\n\\nf\\n(\\nt\\n+\\nΔ\\nt\\n)\\n=\\n\\nd\\ni\\na\\ng\\n\\n(\\nf\\n(\\nΔ\\nt\\n)\\n)\\nf\\n(\\nt\\n)\\n\\n\\n{\\\\displaystyle f(t+\\\\Delta t)=\\\\mathrm {diag} (f(\\\\Delta t))f(t)}\\n\\nwhere \\n\\n\\n\\nΔ\\nt\\n∈\\n\\nR\\n\\n\\n\\n{\\\\displaystyle \\\\Delta t\\\\in \\\\mathbb {R} }\\n\\n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\n\\nc\\n\\nj\\n\\n\\nf\\n(\\nt\\n+\\nΔ\\n\\nt\\n\\nj\\n\\n\\n)\\n=\\n\\n(\\n\\n\\n∑\\n\\nj\\n\\n\\n\\nc\\n\\nj\\n\\n\\n\\n\\nd\\ni\\na\\ng\\n\\n(\\nf\\n(\\nΔ\\n\\nt\\n\\nj\\n\\n\\n)\\n)\\n\\n)\\n\\nf\\n(\\nt\\n)\\n\\n\\n{\\\\displaystyle \\\\sum _{j}c_{j}f(t+\\\\Delta t_{j})=\\\\left(\\\\sum _{j}c_{j}\\\\,\\\\mathrm {diag} (f(\\\\Delta t_{j}))\\\\right)f(t)}\\n\\nfor any constants \\n\\n\\n\\n\\nc\\n\\nj\\n\\n\\n\\n\\n{\\\\displaystyle c_{j}}\\n\\n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author\\'s words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\\n\\nEncoder-decoder (overview)[edit]\\nOne encoder-decoder block\\nA Transformer is composed of stacked encoder layers and decoder layers.\\nLike earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder\\'s output and the decoder\\'s output tokens so far.\\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).[53][54]\\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.[54] These feed-forward layers contain most of the parameters in a Transformer model.\\n\\nFeedforward network[edit]\\nThe feedforward network module. It is a two-layered network that maps \\n\\n\\n\\n\\nd\\n\\nemb\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{emb}}}\\n\\n-dimensional vectors into \\n\\n\\n\\n\\nd\\n\\nemb\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{emb}}}\\n\\n-dimensional vectors.\\nThe feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\\n\\n\\n\\n\\nF\\nF\\nN\\n\\n(\\nx\\n)\\n=\\nϕ\\n(\\nx\\n\\nW\\n\\n(\\n1\\n)\\n\\n\\n+\\n\\nb\\n\\n(\\n1\\n)\\n\\n\\n)\\n\\nW\\n\\n(\\n2\\n)\\n\\n\\n+\\n\\nb\\n\\n(\\n2\\n)\\n\\n\\n\\n\\n{\\\\displaystyle \\\\mathrm {FFN} (x)=\\\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\\n\\nwhere \\n\\n\\n\\nϕ\\n\\n\\n{\\\\displaystyle \\\\phi }\\n\\n is its activation function. The original Transformer used ReLU activation.\\nThe number of neurons in the middle layer is called intermediate size (GPT),[55] filter size (BERT),[35] or feedforward size (BERT).[35] It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \\n\\n\\n\\n\\nd\\n\\nffn\\n\\n\\n=\\n4\\n\\nd\\n\\nemb\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{ffn}}=4d_{\\\\text{emb}}}\\n\\n.\\n\\nScaled dot-product attention[edit]\\nMain article: Dot-product attention\\nAttention head[edit]\\nScaled dot-product attention, block diagram\\nExact dimension counts within an attention head module \\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q}}\\n\\n, the key weights \\n\\n\\n\\n\\nW\\n\\nK\\n\\n\\n\\n\\n{\\\\displaystyle W^{K}}\\n\\n, and the value weights \\n\\n\\n\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W^{V}}\\n\\n.\\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \\n\\n\\n\\n\\nℓ\\n\\nseq, query\\n\\n\\n\\n\\n{\\\\displaystyle \\\\ell _{\\\\text{seq, query}}}\\n\\n, and each entry is a vector of dimension \\n\\n\\n\\n\\nd\\n\\nemb, query\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{emb, query}}}\\n\\n. Similarly for the key and value sequences.\\nFor each vector \\n\\n\\n\\n\\nx\\n\\ni\\n,\\n\\nquery\\n\\n\\n\\n\\n\\n{\\\\displaystyle x_{i,{\\\\text{query}}}}\\n\\n in the query sequence, it is multiplied by a matrix \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q}}\\n\\n to produce a query vector \\n\\n\\n\\n\\nq\\n\\ni\\n\\n\\n=\\n\\nx\\n\\ni\\n,\\n\\nquery\\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle q_{i}=x_{i,{\\\\text{query}}}W^{Q}}\\n\\n. The matrix of all query vectors is the query matrix:\\n\\n\\n\\nQ\\n=\\n\\nX\\n\\nquery\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle Q=X_{\\\\text{query}}W^{Q}}\\n\\nSimilarly, we construct the key matrix \\n\\n\\n\\nK\\n=\\n\\nX\\n\\nkey\\n\\n\\n\\nW\\n\\nK\\n\\n\\n\\n\\n{\\\\displaystyle K=X_{\\\\text{key}}W^{K}}\\n\\n and the value matrix \\n\\n\\n\\nV\\n=\\n\\nX\\n\\nvalue\\n\\n\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle V=X_{\\\\text{value}}W^{V}}\\n\\n.\\nIt is usually the case that all \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n,\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q},W^{K},W^{V}}\\n\\n are square matrices, meaning \\n\\n\\n\\n\\nd\\n\\nemb, query\\n\\n\\n=\\n\\nd\\n\\nquery\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{emb, query}}=d_{\\\\text{query}}}\\n\\n, etc.\\nAttention weights are calculated using the query and key vectors: the attention weight \\n\\n\\n\\n\\na\\n\\ni\\nj\\n\\n\\n\\n\\n{\\\\displaystyle a_{ij}}\\n\\n from token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n to token \\n\\n\\n\\nj\\n\\n\\n{\\\\displaystyle j}\\n\\n is the dot product between \\n\\n\\n\\n\\nq\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle q_{i}}\\n\\n and \\n\\n\\n\\n\\nk\\n\\nj\\n\\n\\n\\n\\n{\\\\displaystyle k_{j}}\\n\\n. The attention weights are divided by the square root of the dimension of the key vectors, \\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\sqrt {d_{k}}}}\\n\\n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q}}\\n\\n and \\n\\n\\n\\n\\nW\\n\\nK\\n\\n\\n\\n\\n{\\\\displaystyle W^{K}}\\n\\n are different matrices allows attention to be non-symmetric: if token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n attends to token \\n\\n\\n\\nj\\n\\n\\n{\\\\displaystyle j}\\n\\n (i.e. \\n\\n\\n\\n\\nq\\n\\ni\\n\\n\\n⋅\\n\\nk\\n\\nj\\n\\n\\n\\n\\n{\\\\displaystyle q_{i}\\\\cdot k_{j}}\\n\\n is large), this does not necessarily mean that token \\n\\n\\n\\nj\\n\\n\\n{\\\\displaystyle j}\\n\\n will attend to token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n (i.e. \\n\\n\\n\\n\\nq\\n\\nj\\n\\n\\n⋅\\n\\nk\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle q_{j}\\\\cdot k_{i}}\\n\\n could be small). The output of the attention unit for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n is the weighted sum of the value vectors of all tokens, weighted by \\n\\n\\n\\n\\na\\n\\ni\\nj\\n\\n\\n\\n\\n{\\\\displaystyle a_{ij}}\\n\\n, the attention from token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n to each token.\\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \\n\\n\\n\\nQ\\n\\n\\n{\\\\displaystyle Q}\\n\\n, \\n\\n\\n\\nK\\n\\n\\n{\\\\displaystyle K}\\n\\n and \\n\\n\\n\\nV\\n\\n\\n{\\\\displaystyle V}\\n\\n are defined as the matrices where the \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\nth rows are vectors \\n\\n\\n\\n\\nq\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle q_{i}}\\n\\n, \\n\\n\\n\\n\\nk\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle k_{i}}\\n\\n, and \\n\\n\\n\\n\\nv\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle v_{i}}\\n\\n respectively. Then we can represent the attention as\\n\\n\\n\\n\\n\\n\\n\\n\\nAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\nsoftmax\\n\\n\\n(\\n\\n\\n\\nQ\\n\\nK\\n\\n\\nT\\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n)\\n\\nV\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{aligned}{\\\\text{Attention}}(Q,K,V)={\\\\text{softmax}}\\\\left({\\\\frac {QK^{\\\\mathrm {T} }}{\\\\sqrt {d_{k}}}}\\\\right)V\\\\end{aligned}}}\\n\\n\\nwhere the softmax is applied over each of the rows of the matrix.\\nThe number of dimensions in a query vector is query size \\n\\n\\n\\n\\nd\\n\\nquery\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{query}}}\\n\\n and similarly for the key size \\n\\n\\n\\n\\nd\\n\\nkey\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{key}}}\\n\\n and value size \\n\\n\\n\\n\\nd\\n\\nvalue\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{value}}}\\n\\n. The output dimension of an attention head is its head dimension \\n\\n\\n\\n\\nd\\n\\nhead\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{head}}}\\n\\n. The attention mechanism requires the following three equalities to hold:\\n\\n\\n\\n\\nℓ\\n\\nseq, key\\n\\n\\n=\\n\\nℓ\\n\\nseq, value\\n\\n\\n,\\n\\n\\nd\\n\\nquery\\n\\n\\n=\\n\\nd\\n\\nkey\\n\\n\\n,\\n\\n\\nd\\n\\nvalue\\n\\n\\n=\\n\\nd\\n\\nhead\\n\\n\\n\\n\\n{\\\\displaystyle \\\\ell _{\\\\text{seq, key}}=\\\\ell _{\\\\text{seq, value}},\\\\;d_{\\\\text{query}}=d_{\\\\text{key}},\\\\;d_{\\\\text{value}}=d_{\\\\text{head}}}\\n\\nbut is otherwise unconstrained.\\nIf the attention head is used in a self-attention fashion, then \\n\\n\\n\\n\\nX\\n\\nquery\\n\\n\\n=\\n\\nX\\n\\nkey\\n\\n\\n=\\n\\nX\\n\\nvalue\\n\\n\\n\\n\\n{\\\\displaystyle X_{\\\\text{query}}=X_{\\\\text{key}}=X_{\\\\text{value}}}\\n\\n. If the attention head is used in a cross-attention fashion, then usually \\n\\n\\n\\n\\nX\\n\\nquery\\n\\n\\n≠\\n\\nX\\n\\nkey\\n\\n\\n=\\n\\nX\\n\\nvalue\\n\\n\\n\\n\\n{\\\\displaystyle X_{\\\\text{query}}\\\\neq X_{\\\\text{key}}=X_{\\\\text{value}}}\\n\\n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\\n\\nMultiheaded attention[edit]\\nMultiheaded attention, block diagram\\nExact dimension counts within a multiheaded attention module\\nOne set of \\n\\n\\n\\n\\n(\\n\\n\\nW\\n\\nQ\\n\\n\\n,\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nW\\n\\nV\\n\\n\\n\\n)\\n\\n\\n\\n{\\\\displaystyle \\\\left(W^{Q},W^{K},W^{V}\\\\right)}\\n\\n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q}}\\n\\n and \\n\\n\\n\\n\\nW\\n\\nK\\n\\n\\n\\n\\n{\\\\displaystyle W^{K}}\\n\\n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \\n\\n\\n\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W^{V}}\\n\\n, in combination with the part of the output projection matrix \\n\\n\\n\\n\\nW\\n\\nO\\n\\n\\n\\n\\n{\\\\displaystyle W^{O}}\\n\\n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.[56] The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\\nConcretely, let the multiple attention heads be indexed by \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n, then we have\\n\\n\\n\\n\\nMultiheadedAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\n\\nConcat\\n\\n\\ni\\n∈\\n[\\n\\nn\\n\\nheads\\n\\n\\n]\\n\\n\\n(\\n\\nAttention\\n\\n(\\nQ\\n\\nW\\n\\ni\\n\\n\\nQ\\n\\n\\n,\\nK\\n\\nW\\n\\ni\\n\\n\\nK\\n\\n\\n,\\nV\\n\\nW\\n\\ni\\n\\n\\nV\\n\\n\\n)\\n)\\n\\nW\\n\\nO\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{MultiheadedAttention}}(Q,K,V)={\\\\text{Concat}}_{i\\\\in [n_{\\\\text{heads}}]}({\\\\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}))W^{O}}\\n\\n where the matrix \\n\\n\\n\\nX\\n\\n\\n{\\\\displaystyle X}\\n\\n is the concatenation of word embeddings, and the matrices \\n\\n\\n\\n\\nW\\n\\ni\\n\\n\\nQ\\n\\n\\n,\\n\\nW\\n\\ni\\n\\n\\nK\\n\\n\\n,\\n\\nW\\n\\ni\\n\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\\n\\n are \"projection matrices\" owned by individual attention head \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n, and \\n\\n\\n\\n\\nW\\n\\nO\\n\\n\\n\\n\\n{\\\\displaystyle W^{O}}\\n\\n is a final projection matrix owned by the whole multi-headed attention head.\\nIt is theoretically possible for each attention head to have a different head dimension \\n\\n\\n\\n\\nd\\n\\nhead\\n\\n\\n\\n\\n{\\\\displaystyle d_{\\\\text{head}}}\\n\\n, but that is rarely the case in practice.\\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\\n\\n\\n\\n\\nd\\n\\nemb\\n\\n\\n=\\n768\\n,\\n\\nn\\n\\nhead\\n\\n\\n=\\n12\\n,\\n\\nd\\n\\nhead\\n\\n\\n=\\n64\\n\\n\\n{\\\\displaystyle d_{\\\\text{emb}}=768,n_{\\\\text{head}}=12,d_{\\\\text{head}}=64}\\n\\nSince \\n\\n\\n\\n12\\n×\\n64\\n=\\n768\\n\\n\\n{\\\\displaystyle 12\\\\times 64=768}\\n\\n, its output projection matrix \\n\\n\\n\\n\\nW\\n\\nO\\n\\n\\n∈\\n\\n\\nR\\n\\n\\n(\\n12\\n×\\n64\\n)\\n×\\n768\\n\\n\\n\\n\\n{\\\\displaystyle W^{O}\\\\in \\\\mathbb {R} ^{(12\\\\times 64)\\\\times 768}}\\n\\n is a square matrix.\\n\\nMasked attention[edit]\\nThe Transformer architecture is constructed to calculate output tokens iteratively. Assuming \\n\\n\\n\\nt\\n=\\n0\\n\\n\\n{\\\\displaystyle t=0}\\n\\n refers to the calculation of the first output token \\n\\n\\n\\ni\\n=\\n0\\n\\n\\n{\\\\displaystyle i=0}\\n\\n, for step \\n\\n\\n\\nt\\n>\\n0\\n\\n\\n{\\\\displaystyle t>0}\\n\\n, the output token \\n\\n\\n\\ni\\n=\\n0\\n\\n\\n{\\\\displaystyle i=0}\\n\\n shall remain constant. This ensures properties of the model similar to autoregressive models.[1] Therefore, at every time step \\n\\n\\n\\nt\\n\\n\\n{\\\\displaystyle t}\\n\\n, the calculation for all outputs \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n should not have access to tokens at position \\n\\n\\n\\nj\\n\\n\\n{\\\\displaystyle j}\\n\\n for \\n\\n\\n\\nj\\n>=\\ni\\n\\n\\n{\\\\displaystyle j>=i}\\n\\n (as it naturally is the case for time step \\n\\n\\n\\nt\\n=\\ni\\n\\n\\n{\\\\displaystyle t=i}\\n\\n, when tokens \\n\\n\\n\\nj\\n>\\nt\\n\\n\\n{\\\\displaystyle j>t}\\n\\n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \\n\\n\\n\\nM\\n\\n\\n{\\\\displaystyle M}\\n\\n that is \\n\\n\\n\\n−\\n∞\\n\\n\\n{\\\\displaystyle -\\\\infty }\\n\\n at entries where the attention link must be cut, and \\n\\n\\n\\n0\\n\\n\\n{\\\\displaystyle 0}\\n\\n at other places:\\n\\n\\n\\n\\n\\n\\n\\n\\nMaskedAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\nsoftmax\\n\\n\\n(\\n\\nM\\n+\\n\\n\\n\\nQ\\n\\nK\\n\\n\\nT\\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n\\n)\\n\\nV\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{aligned}{\\\\text{MaskedAttention}}(Q,K,V)={\\\\text{softmax}}\\\\left(M+{\\\\frac {QK^{\\\\mathrm {T} }}{\\\\sqrt {d_{k}}}}\\\\right)V\\\\end{aligned}}}\\n\\n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\\n\\n\\n\\n\\nM\\n\\ncausal\\n\\n\\n=\\n\\n\\n[\\n\\n\\n\\n0\\n\\n\\n−\\n∞\\n\\n\\n−\\n∞\\n\\n\\n…\\n\\n\\n−\\n∞\\n\\n\\n\\n\\n0\\n\\n\\n0\\n\\n\\n−\\n∞\\n\\n\\n…\\n\\n\\n−\\n∞\\n\\n\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n…\\n\\n\\n−\\n∞\\n\\n\\n\\n\\n⋮\\n\\n\\n⋮\\n\\n\\n⋮\\n\\n\\n⋱\\n\\n\\n⋮\\n\\n\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n…\\n\\n\\n0\\n\\n\\n\\n]\\n\\n\\n\\n\\n{\\\\displaystyle M_{\\\\text{causal}}={\\\\begin{bmatrix}0&-\\\\infty &-\\\\infty &\\\\dots &-\\\\infty \\\\\\\\0&0&-\\\\infty &\\\\dots &-\\\\infty \\\\\\\\0&0&0&\\\\dots &-\\\\infty \\\\\\\\\\\\vdots &\\\\vdots &\\\\vdots &\\\\ddots &\\\\vdots \\\\\\\\0&0&0&\\\\dots &0\\\\end{bmatrix}}}\\n\\n\\nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \\n\\n\\n\\nP\\n\\nM\\n\\ncausal\\n\\n\\n\\nP\\n\\n−\\n1\\n\\n\\n\\n\\n{\\\\displaystyle PM_{\\\\text{causal}}P^{-1}}\\n\\n, where \\n\\n\\n\\nP\\n\\n\\n{\\\\displaystyle P}\\n\\n is a random permutation matrix.[57]\\n\\nEncoder[edit]\\nOne encoder layer\\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\\n\\n\\n\\n\\n\\n\\n\\n\\ngiven input vectors\\xa0\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n\\n\\n\\ncombine them into a matrix\\xa0\\n\\nH\\n\\n\\n\\n=\\n\\n\\n[\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n\\n\\n\\n\\n\\nh\\n\\n1\\n\\n\\n\\n\\n\\n\\n⋮\\n\\n\\n\\n]\\n\\n\\n\\n\\n\\n\\n\\nEncoderLayer\\n\\n(\\nH\\n)\\n\\n\\n\\n=\\n\\n\\n[\\n\\n\\n\\n\\nFFN\\n\\n(\\n\\nMultiheadedAttention\\n\\n(\\nH\\n,\\nH\\n,\\nH\\n\\n)\\n\\n0\\n\\n\\n)\\n\\n\\n\\n\\n\\nFFN\\n\\n(\\n\\nMultiheadedAttention\\n\\n(\\nH\\n,\\nH\\n,\\nH\\n\\n)\\n\\n1\\n\\n\\n)\\n\\n\\n\\n\\n⋮\\n\\n\\n\\n]\\n\\n\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{aligned}{\\\\text{given input vectors }}&h_{0},h_{1},\\\\dots \\\\\\\\{\\\\text{combine them into a matrix }}H&={\\\\begin{bmatrix}h_{0}\\\\\\\\h_{1}\\\\\\\\\\\\vdots \\\\end{bmatrix}}\\\\\\\\{\\\\text{EncoderLayer}}(H)&={\\\\begin{bmatrix}{\\\\text{FFN}}({\\\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\\\\\{\\\\text{FFN}}({\\\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\\\\\\\vdots \\\\end{bmatrix}}\\\\\\\\\\\\end{aligned}}}\\n\\n\\nwhere \\n\\n\\n\\n\\nFFN\\n\\n\\n\\n{\\\\displaystyle {\\\\text{FFN}}}\\n\\n stands for \"feed-forward network\". We can more succinctly write it as\\n\\n\\n\\n\\nEncoderLayer\\n\\n(\\nH\\n)\\n=\\n\\nFFN\\n\\n(\\n\\nMultiheadedAttention\\n\\n(\\nH\\n,\\nH\\n,\\nH\\n)\\n)\\n\\n\\n{\\\\displaystyle {\\\\text{EncoderLayer}}(H)={\\\\text{FFN}}({\\\\text{MultiheadedAttention}}(H,H,H))}\\n\\nwith the implicit convention that the \\n\\n\\n\\n\\nFFN\\n\\n\\n\\n{\\\\displaystyle {\\\\text{FFN}}}\\n\\n is applied to each row of the matrix individually.\\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\\n\\nDecoder[edit]\\nOne decoder layer\\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.[1][54]\\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.[1] This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\\nSchematically, we have:\\n\\n\\n\\n\\n\\n\\n\\n\\nH\\n′\\n\\n\\n\\n\\n=\\n\\nMaskedMultiheadedAttention\\n\\n(\\nH\\n,\\nH\\n,\\nH\\n)\\n\\n\\n\\n\\n\\nDecoderLayer\\n\\n(\\nH\\n)\\n\\n\\n\\n=\\n\\nFFN\\n\\n(\\n\\nMultiheadedAttention\\n\\n(\\n\\nH\\n′\\n\\n,\\n\\nH\\n\\nE\\n\\n\\n,\\n\\nH\\n\\nE\\n\\n\\n)\\n)\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{aligned}H\\'&={\\\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\\\\\{\\\\text{DecoderLayer}}(H)&={\\\\text{FFN}}({\\\\text{MultiheadedAttention}}(H\\',H^{E},H^{E}))\\\\end{aligned}}}\\n\\nwhere \\n\\n\\n\\n\\nH\\n\\nE\\n\\n\\n\\n\\n{\\\\displaystyle H^{E}}\\n\\n is the matrix with rows being the output vectors from the encoder.\\nThe last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\\n\\nAdapted architectures[edit]\\nMany large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence.[58] BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.[35]\\n\\nFull transformer architecture[edit]\\nSublayers[edit]\\n(a) One encoder layer and one decoder layer. (b) Two encoder layers and two decoder layers. The sublayers are labelled as well.Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\\nTransformer encoder with norm-first and norm-last\\nTransformer decoder with norm-first and norm-last\\nBlock diagram for the full Transformer architectureSchematic object hierarchy for the full Transformer architecture, in object-oriented programming styleThe final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. \\nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \\n\\n\\n\\n\\nL\\na\\ny\\ne\\nr\\nN\\no\\nr\\nm\\n\\n(\\nx\\n+\\n\\nS\\nu\\nb\\nl\\na\\ny\\ne\\nr\\n\\n(\\nx\\n)\\n)\\n\\n\\n{\\\\displaystyle \\\\mathrm {LayerNorm} (x+\\\\mathrm {Sublayer} (x))}\\n\\nwhere \\n\\n\\n\\n\\nS\\nu\\nb\\nl\\na\\ny\\ne\\nr\\n\\n(\\nx\\n)\\n\\n\\n{\\\\displaystyle \\\\mathrm {Sublayer} (x)}\\n\\n is the function implemented by the sublayer itself.\\nIn the pre-LN convention, the output of each sublayer is\\n\\n\\n\\nx\\n+\\n\\nS\\nu\\nb\\nl\\na\\ny\\ne\\nr\\n\\n(\\n\\nL\\na\\ny\\ne\\nr\\nN\\no\\nr\\nm\\n\\n(\\nx\\n)\\n)\\n\\n\\n{\\\\displaystyle x+\\\\mathrm {Sublayer} (\\\\mathrm {LayerNorm} (x))}\\n\\nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018,[59] was found to be easier to train, requiring no warm-up, leading to faster convergence.[46]\\n\\nPseudocode[edit]\\nThe following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from[60]\\n\\ninput: Encoder input t_e\\n       Decoder input t_d\\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\\n\\n/* encoder */\\nz_e ← encoder.tokenizer(t_e)\\n\\nfor each t in 1:length(z_e) do\\n    z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\\n\\nfor each l in 1:length(encoder.layers) do\\n    layer ← encoder.layers[l]\\n\\n    /* first sublayer */\\n    z_e_copy ← copy(z_e)\\n    for each t in 1:length(z_e) do\\n        z_e[t] ← layer.layer_norm(z_e[t])\\n    z_e ← layer.multiheaded_attention(z_e, z_e, z_e)\\n    for each t in 1:length(z_e) do\\n        z_e[t] ← z_e[t] + z_e_copy[t]\\n\\n    /* second sublayer */\\n    z_e_copy ← copy(z_e)\\n    for each t in 1:length(z_e) do\\n        z_e[t] ← layer.layer_norm(z_e[t])\\n    z_e ← layer.feedforward(z_e)\\n    for each t in 1:length(z_e) do\\n        z_e[t] ← z_e[t] + z_e_copy[t]\\n\\nfor each t in 1:length(z_e) do\\n    z_e[t] ← encoder.final_layer_norm(z_e[t])\\n\\n/* decoder */\\nz_d ← decoder.tokenizer(t_d)\\n\\nfor each t in 1:length(z_d) do\\n    z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\\n\\nfor each l in 1:length(decoder.layers) do\\n        layer ← decoder.layers[l]\\n\\n        /* first sublayer */\\n        z_d_copy ← copy(z_d)\\n        for each t in 1:length(z_d) do\\n            z_d[t] ← layer.layer_norm(z_d[t])\\n        z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d)\\n        for each t in 1:length(z_d) do\\n            z_d[t] ← z_d[t] + z_d_copy[t]\\n\\n        /* second sublayer */\\n        z_d_copy ← copy(z_d)\\n        for each t in 1:length(z_d) do\\n            z_d[t] ← layer.layer_norm(z_d[t])\\n        z_d ← layer.multiheaded_attention(z_d, z_e, z_e) \\n        for each i in 1:length(z_d) do\\n            z_d[t] ← z_d[t] + z_d_copy[t]\\n\\n        /* third sublayer */\\n        z_d_copy ← copy(z_d)\\n        for each t in 1:length(z_d) do\\n            z_d[t] ← layer.layer_norm(z_d[t])\\n        z_d ← layer.feedforward(z_d)\\n        for each t in 1:length(z_d) do\\n            z_d[t] ← z_d[t] + z_d_copy[t]\\n\\nz_d ← decoder.final_layer_norm(z_d)\\n\\noutput_distributions ← []\\nfor each t in 1:length(z_d) do\\n    output_distributions.append(decoder.unembed(z_d[t]))\\n\\nreturn output_distributions\\n\\nTerminology[edit]\\nThe Transformer architecture, being modular, allows variations. Several common variations are described here.[61]\\nAn \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.[51]\\nA \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\\nAn \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.[61]\\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form[61]:\\u200aFigure 3\\u200a\\n\\n\\n\\n\\nM\\n\\nprefixLM\\n\\n\\n=\\n\\n\\n[\\n\\n\\n\\n\\n0\\n\\n\\n\\n−\\n∞\\n\\n\\n\\n\\n\\n0\\n\\n\\n\\n\\nM\\n\\ncausal\\n\\n\\n\\n\\n\\n]\\n\\n\\n\\n\\n{\\\\displaystyle M_{\\\\text{prefixLM}}={\\\\begin{bmatrix}\\\\mathbf {0} &-\\\\infty \\\\\\\\\\\\mathbf {0} &M_{\\\\text{causal}}\\\\end{bmatrix}}}\\n\\nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.[51]\\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.[62]\\n\\nSubsequent work[edit]\\nAlternative activation functions[edit]\\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU;[63] both GPT-1 and BERT[35] used GELU.[64]\\nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.[65]\\n\\nAlternative normalizations[edit]\\nThe normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm[66] which is used in the Llama series. Other examples include CapsuleNorm[67] ScaleNorm,[68] or FixNorm.[68]\\n\\nAlternative positional encodings[edit]\\nTransformers may use other positional encoding methods than sinusoidal.[69]\\nThe original Transformer paper reported using a learned positional encoding,[70] but finding it not superior to the sinusoidal one.[1] Later, [71] found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\\n\\nRoPE[edit]\\nRoPE (rotary positional embedding),[72] is best explained by considering a list of 2-dimensional vectors \\n\\n\\n\\n[\\n(\\n\\nx\\n\\n1\\n\\n\\n(\\n1\\n)\\n\\n\\n,\\n\\nx\\n\\n1\\n\\n\\n(\\n2\\n)\\n\\n\\n)\\n,\\n(\\n\\nx\\n\\n2\\n\\n\\n(\\n1\\n)\\n\\n\\n,\\n\\nx\\n\\n2\\n\\n\\n(\\n2\\n)\\n\\n\\n)\\n,\\n(\\n\\nx\\n\\n3\\n\\n\\n(\\n1\\n)\\n\\n\\n,\\n\\nx\\n\\n3\\n\\n\\n(\\n2\\n)\\n\\n\\n)\\n,\\n.\\n.\\n.\\n]\\n\\n\\n{\\\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\\n\\n. Now pick some angle \\n\\n\\n\\nθ\\n\\n\\n{\\\\displaystyle \\\\theta }\\n\\n. Then RoPE encoding is\\n\\n\\n\\n\\nRoPE\\n\\n\\n\\n(\\n\\n\\n\\nx\\n\\nm\\n\\n\\n(\\n1\\n)\\n\\n\\n,\\n\\nx\\n\\nm\\n\\n\\n(\\n2\\n)\\n\\n\\n,\\nm\\n\\n\\n)\\n\\n\\n=\\n\\n\\n(\\n\\n\\n\\ncos\\n\\u2061\\nm\\nθ\\n\\n\\n−\\nsin\\n\\u2061\\nm\\nθ\\n\\n\\n\\n\\nsin\\n\\u2061\\nm\\nθ\\n\\n\\ncos\\n\\u2061\\nm\\nθ\\n\\n\\n\\n)\\n\\n\\n\\n\\n(\\n\\n\\n\\n\\nx\\n\\nm\\n\\n\\n(\\n1\\n)\\n\\n\\n\\n\\n\\n\\n\\nx\\n\\nm\\n\\n\\n(\\n2\\n)\\n\\n\\n\\n\\n\\n)\\n\\n\\n=\\n\\n\\n(\\n\\n\\n\\n\\nx\\n\\nm\\n\\n\\n(\\n1\\n)\\n\\n\\ncos\\n\\u2061\\nm\\nθ\\n−\\n\\nx\\n\\nm\\n\\n\\n(\\n2\\n)\\n\\n\\nsin\\n\\u2061\\nm\\nθ\\n\\n\\n\\n\\n\\nx\\n\\nm\\n\\n\\n(\\n2\\n)\\n\\n\\ncos\\n\\u2061\\nm\\nθ\\n+\\n\\nx\\n\\nm\\n\\n\\n(\\n1\\n)\\n\\n\\nsin\\n\\u2061\\nm\\nθ\\n\\n\\n\\n)\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{RoPE}}{\\\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\\\big )}={\\\\begin{pmatrix}\\\\cos m\\\\theta &-\\\\sin m\\\\theta \\\\\\\\\\\\sin m\\\\theta &\\\\cos m\\\\theta \\\\end{pmatrix}}{\\\\begin{pmatrix}x_{m}^{(1)}\\\\\\\\x_{m}^{(2)}\\\\\\\\\\\\end{pmatrix}}={\\\\begin{pmatrix}x_{m}^{(1)}\\\\cos m\\\\theta -x_{m}^{(2)}\\\\sin m\\\\theta \\\\\\\\x_{m}^{(2)}\\\\cos m\\\\theta +x_{m}^{(1)}\\\\sin m\\\\theta \\\\\\\\\\\\end{pmatrix}}}\\n\\nEquivalently, if we write the 2-dimensional vectors as complex numbers \\n\\n\\n\\n\\nz\\n\\nm\\n\\n\\n:=\\n\\nx\\n\\nm\\n\\n\\n(\\n1\\n)\\n\\n\\n+\\ni\\n\\nx\\n\\nm\\n\\n\\n(\\n2\\n)\\n\\n\\n\\n\\n{\\\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\\n\\n, then RoPE encoding is just multiplication by an angle:\\n\\n\\n\\n\\nRoPE\\n\\n\\n\\n(\\n\\n\\n\\nz\\n\\nm\\n\\n\\n,\\nm\\n\\n\\n)\\n\\n\\n=\\n\\ne\\n\\ni\\nm\\nθ\\n\\n\\n\\nz\\n\\nm\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{RoPE}}{\\\\big (}z_{m},m{\\\\big )}=e^{im\\\\theta }z_{m}}\\n\\nFor a list of \\n\\n\\n\\n2\\nn\\n\\n\\n{\\\\displaystyle 2n}\\n\\n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \\n\\n\\n\\n\\nθ\\n\\n(\\n1\\n)\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nθ\\n\\n(\\nn\\n)\\n\\n\\n\\n\\n{\\\\displaystyle \\\\theta ^{(1)},...,\\\\theta ^{(n)}}\\n\\n. Then the RoPE encoding is applied to each pair of coordinates.\\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\\n\\n\\n\\n\\nRoPE\\n\\n\\n\\n(\\n\\n\\nx\\n,\\nm\\n\\n\\n\\n)\\n\\n\\n\\nT\\n\\n\\n\\nRoPE\\n\\n\\n\\n(\\n\\n\\ny\\n,\\nn\\n\\n\\n)\\n\\n\\n=\\n\\nRoPE\\n\\n\\n\\n(\\n\\n\\nx\\n,\\nm\\n+\\nk\\n\\n\\n\\n)\\n\\n\\n\\nT\\n\\n\\n\\nRoPE\\n\\n\\n\\n(\\n\\n\\ny\\n,\\nn\\n+\\nk\\n\\n\\n)\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{RoPE}}{\\\\big (}x,m{\\\\big )}^{T}{\\\\text{RoPE}}{\\\\big (}y,n{\\\\big )}={\\\\text{RoPE}}{\\\\big (}x,m+k{\\\\big )}^{T}{\\\\text{RoPE}}{\\\\big (}y,n+k{\\\\big )}}\\n\\n\\nfor any integer \\n\\n\\n\\nk\\n\\n\\n{\\\\displaystyle k}\\n\\n.\\n\\nALiBi[edit]\\nALiBi (Attention with Linear Biases)[73] is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\\n\\n\\n\\n\\n\\n\\n\\n\\nAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\nsoftmax\\n\\n\\n(\\n\\n\\n\\n\\nQ\\n\\nK\\n\\n\\nT\\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n+\\ns\\nB\\n\\n)\\n\\nV\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{aligned}{\\\\text{Attention}}(Q,K,V)={\\\\text{softmax}}\\\\left({\\\\frac {QK^{\\\\mathrm {T} }}{\\\\sqrt {d_{k}}}}+sB\\\\right)V\\\\end{aligned}}}\\n\\nHere, \\n\\n\\n\\ns\\n\\n\\n{\\\\displaystyle s}\\n\\n is a real number (\"scalar\"), and \\n\\n\\n\\nB\\n\\n\\n{\\\\displaystyle B}\\n\\n is the linear bias matrix defined by\\n\\n\\n\\nB\\n=\\n\\n\\n(\\n\\n\\n\\n0\\n\\n\\n1\\n\\n\\n2\\n\\n\\n3\\n\\n\\n⋯\\n\\n\\n\\n\\n−\\n1\\n\\n\\n0\\n\\n\\n1\\n\\n\\n2\\n\\n\\n⋯\\n\\n\\n\\n\\n−\\n2\\n\\n\\n−\\n1\\n\\n\\n0\\n\\n\\n1\\n\\n\\n⋯\\n\\n\\n\\n\\n−\\n3\\n\\n\\n−\\n2\\n\\n\\n−\\n1\\n\\n\\n0\\n\\n\\n⋯\\n\\n\\n\\n\\n⋮\\n\\n\\n⋮\\n\\n\\n⋮\\n\\n\\n⋮\\n\\n\\n⋱\\n\\n\\n\\n)\\n\\n\\n\\n\\n{\\\\displaystyle B={\\\\begin{pmatrix}0&1&2&3&\\\\cdots \\\\\\\\-1&0&1&2&\\\\cdots \\\\\\\\-2&-1&0&1&\\\\cdots \\\\\\\\-3&-2&-1&0&\\\\cdots \\\\\\\\\\\\vdots &\\\\vdots &\\\\vdots &\\\\vdots &\\\\ddots \\\\\\\\\\\\end{pmatrix}}}\\n\\nin other words, \\n\\n\\n\\n\\nB\\n\\ni\\n,\\nj\\n\\n\\n=\\nj\\n−\\ni\\n\\n\\n{\\\\displaystyle B_{i,j}=j-i}\\n\\n. The idea being that the linear bias matrix is a softened mask. Just as \\n\\n\\n\\n0\\n\\n\\n{\\\\displaystyle 0}\\n\\n represent full attention paid, and \\n\\n\\n\\n−\\n∞\\n\\n\\n{\\\\displaystyle -\\\\infty }\\n\\n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\\nALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\\n\\nRelative Position Encodings[edit]\\nRelative Position Encodings[74] is similar to ALiBi, but more generic:\\n\\n\\n\\n\\n\\n\\n\\n\\nAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\nsoftmax\\n\\n\\n(\\n\\n\\n\\n\\nQ\\n\\nK\\n\\n\\nT\\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n+\\nB\\n\\n)\\n\\nV\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{aligned}{\\\\text{Attention}}(Q,K,V)={\\\\text{softmax}}\\\\left({\\\\frac {QK^{\\\\mathrm {T} }}{\\\\sqrt {d_{k}}}}+B\\\\right)V\\\\end{aligned}}}\\n\\nwhere \\n\\n\\n\\nB\\n\\n\\n{\\\\displaystyle B}\\n\\n is a Toeplitz matrix, that is, \\n\\n\\n\\n\\nB\\n\\ni\\n,\\nj\\n\\n\\n=\\n\\nB\\n\\n\\ni\\n′\\n\\n,\\n\\nj\\n′\\n\\n\\n\\n\\n\\n{\\\\displaystyle B_{i,j}=B_{i\\',j\\'}}\\n\\n whenever \\n\\n\\n\\ni\\n−\\nj\\n=\\n\\ni\\n′\\n\\n−\\n\\nj\\n′\\n\\n\\n\\n{\\\\displaystyle i-j=i\\'-j\\'}\\n\\n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".[75]\\n\\nEfficient implementation[edit]\\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.[11]\\n\\nKV caching[edit]\\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.[76][77][78]\\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\\n\\nFlashAttention[edit]\\nFlashAttention[79] is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on sofmax for details.\\nAn improved version, FlashAttention-2,[80][81][82] was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).[83]\\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\\n\\nMulti-Query Attention[edit]\\n\\n\\nComparison between several different forms of attention mechanism and the amount of KV caching necessary for each.\\nMulti-Query Attention changes the multiheaded attention mechanism.[84] Whereas normally,\\n\\n\\n\\n\\n\\nMultiheadedAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\n\\nConcat\\n\\n\\ni\\n∈\\n[\\n\\nn\\n\\nheads\\n\\n\\n]\\n\\n\\n\\n(\\n\\n\\nAttention\\n\\n(\\nX\\n\\nW\\n\\ni\\n\\n\\nQ\\n\\n\\n,\\nX\\n\\nW\\n\\ni\\n\\n\\nK\\n\\n\\n,\\nX\\n\\nW\\n\\ni\\n\\n\\nV\\n\\n\\n)\\n\\n)\\n\\n\\nW\\n\\nO\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{MultiheadedAttention}}(Q,K,V)={\\\\text{Concat}}_{i\\\\in [n_{\\\\text{heads}}]}\\\\left({\\\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\\\right)W^{O}}\\n\\nwith Multi-Query Attention, there is just one \\n\\n\\n\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W^{K},W^{V}}\\n\\n, thus:\\n\\n\\n\\n\\n\\nMultiQueryAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\n\\nConcat\\n\\n\\ni\\n∈\\n[\\n\\nn\\n\\nheads\\n\\n\\n]\\n\\n\\n\\n(\\n\\n\\nAttention\\n\\n(\\nX\\n\\nW\\n\\ni\\n\\n\\nQ\\n\\n\\n,\\nX\\n\\nW\\n\\nK\\n\\n\\n,\\nX\\n\\nW\\n\\nV\\n\\n\\n)\\n\\n)\\n\\n\\nW\\n\\nO\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{MultiQueryAttention}}(Q,K,V)={\\\\text{Concat}}_{i\\\\in [n_{\\\\text{heads}}]}\\\\left({\\\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\\\right)W^{O}}\\n\\n\\nThis has a neutral effect on model quality and training speed, but increases inference speed. \\nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.[85]\\n\\nThe architecture of V2, showing both MLA and a variant of mixture of experts.[86]:\\u200aFigure 2\\u200a\\n\\nMultihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.[86]\\n\\nSpeculative decoding[edit]\\nSpeculative decoding[87][88] is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.\\nThe key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.\\nSuppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token \\n\\n\\n\\n\\nx\\n\\n1\\n\\n\\n,\\n\\nx\\n\\n2\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nx\\n\\n512\\n\\n\\n\\n\\n{\\\\displaystyle x_{1},x_{2},...,x_{512}}\\n\\n, taking time \\n\\n\\n\\n512\\n\\nT\\n\\nGPT-3\\n\\n\\n\\n\\n{\\\\displaystyle 512T_{\\\\text{GPT-3}}}\\n\\n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \\n\\n\\n\\n\\nx\\n\\nt\\n\\n\\n\\n\\n{\\\\displaystyle x_{t}}\\n\\n is indeed the token with the largest log-likelihood in the \\n\\n\\n\\nt\\n\\n\\n{\\\\displaystyle t}\\n\\n-th output.\\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: \\n\\n\\n\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n1\\n\\n\\n,\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n2\\n\\n\\n,\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n3\\n\\n\\n,\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n4\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\tilde {x}}_{1},{\\\\tilde {x}}_{2},{\\\\tilde {x}}_{3},{\\\\tilde {x}}_{4}}\\n\\n. This only takes \\n\\n\\n\\n4\\n\\nT\\n\\nGPT-3-small\\n\\n\\n\\n\\n{\\\\displaystyle 4T_{\\\\text{GPT-3-small}}}\\n\\n. These tokens are then run through the larger GPT-3 in one go. Suppose that \\n\\n\\n\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\tilde {x}}_{1}}\\n\\n and \\n\\n\\n\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\tilde {x}}_{2}}\\n\\n are verified by GPT-3 as what it would have picked, then those are kept, but \\n\\n\\n\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n3\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\tilde {x}}_{3}}\\n\\n is not, so \\n\\n\\n\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n3\\n\\n\\n,\\n\\n\\n\\n\\nx\\n~\\n\\n\\n\\n\\n4\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\tilde {x}}_{3},{\\\\tilde {x}}_{4}}\\n\\n are discarded, and GPT-3 is run on those. This would take \\n\\n\\n\\n4\\n\\nT\\n\\nGPT-3-small\\n\\n\\n+\\n3\\n\\nT\\n\\nGPT-3\\n\\n\\n\\n\\n{\\\\displaystyle 4T_{\\\\text{GPT-3-small}}+3T_{\\\\text{GPT-3}}}\\n\\n, which might be shorter than \\n\\n\\n\\n4\\n\\nT\\n\\nGPT-3\\n\\n\\n\\n\\n{\\\\displaystyle 4T_{\\\\text{GPT-3}}}\\n\\n.\\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.[87][89]\\n\\nMulti-Token Prediction.\\nIn Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.[90][91]\\n\\nSub-quadratic transformers[edit]\\nTraining transformer-based architectures can be expensive, especially for long inputs.[92] Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows.[93] In the audio domain, SepTr decouples the attention in time and frequency domains.[94] Long Range Arena (2020)[95] is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\\n\\nAlternative attention graphs[edit]\\nThe standard attention graph is either all-to-all or causal, both of which scales as \\n\\n\\n\\nO\\n(\\n\\nN\\n\\n2\\n\\n\\n)\\n\\n\\n{\\\\displaystyle O(N^{2})}\\n\\n where \\n\\n\\n\\nN\\n\\n\\n{\\\\displaystyle N}\\n\\n is the number of tokens in a sequence.\\nReformer (2020)[92][96] reduces the computational load from \\n\\n\\n\\nO\\n(\\n\\nN\\n\\n2\\n\\n\\n)\\n\\n\\n{\\\\displaystyle O(N^{2})}\\n\\n to \\n\\n\\n\\nO\\n(\\nN\\nln\\n\\u2061\\nN\\n)\\n\\n\\n{\\\\displaystyle O(N\\\\ln N)}\\n\\n by using locality-sensitive hashing and reversible layers.[97]\\nSparse attention[98] uses attention graphs that grows slower than \\n\\n\\n\\nO\\n(\\n\\nN\\n\\n2\\n\\n\\n)\\n\\n\\n{\\\\displaystyle O(N^{2})}\\n\\n. For example, BigBird (2020)[99] uses random small-world networks which grows as \\n\\n\\n\\nO\\n(\\nN\\n)\\n\\n\\n{\\\\displaystyle O(N)}\\n\\n.\\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers[100] reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\\n\\nRandom Feature Attention[edit]\\nRandom Feature Attention (2021)[101] uses Fourier random features:\\n\\n\\n\\nφ\\n(\\nx\\n)\\n=\\n\\n\\n1\\n\\nD\\n\\n\\n\\n[\\ncos\\n\\u2061\\n⟨\\n\\nw\\n\\n1\\n\\n\\n,\\nx\\n⟩\\n,\\nsin\\n\\u2061\\n⟨\\n\\nw\\n\\n1\\n\\n\\n,\\nx\\n⟩\\n,\\n⋯\\ncos\\n\\u2061\\n⟨\\n\\nw\\n\\nD\\n\\n\\n,\\nx\\n⟩\\n,\\nsin\\n\\u2061\\n⟨\\n\\nw\\n\\nD\\n\\n\\n,\\nx\\n⟩\\n\\n]\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle \\\\varphi (x)={\\\\frac {1}{\\\\sqrt {D}}}[\\\\cos \\\\langle w_{1},x\\\\rangle ,\\\\sin \\\\langle w_{1},x\\\\rangle ,\\\\cdots \\\\cos \\\\langle w_{D},x\\\\rangle ,\\\\sin \\\\langle w_{D},x\\\\rangle ]^{T}}\\n\\nwhere \\n\\n\\n\\n\\nw\\n\\n1\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nw\\n\\nD\\n\\n\\n\\n\\n{\\\\displaystyle w_{1},...,w_{D}}\\n\\n are independent samples from the normal distribution \\n\\n\\n\\nN\\n(\\n0\\n,\\n\\nσ\\n\\n2\\n\\n\\nI\\n)\\n\\n\\n{\\\\displaystyle N(0,\\\\sigma ^{2}I)}\\n\\n. This choice of parameters satisfy \\n\\n\\n\\n\\nE\\n\\n[\\n⟨\\nφ\\n(\\nx\\n)\\n,\\nφ\\n(\\ny\\n)\\n⟩\\n]\\n=\\n\\ne\\n\\n−\\n\\n\\n\\n‖\\nx\\n−\\ny\\n\\n‖\\n\\n2\\n\\n\\n\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle \\\\mathbb {E} [\\\\langle \\\\varphi (x),\\\\varphi (y)\\\\rangle ]=e^{-{\\\\frac {\\\\|x-y\\\\|^{2}}{2\\\\sigma ^{2}}}}}\\n\\n, or \\n\\n\\n\\n\\ne\\n\\n⟨\\nx\\n,\\ny\\n⟩\\n\\n/\\n\\n\\nσ\\n\\n2\\n\\n\\n\\n\\n=\\n\\nE\\n\\n[\\n⟨\\n\\ne\\n\\n‖\\nx\\n\\n‖\\n\\n2\\n\\n\\n\\n/\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\nφ\\n(\\nx\\n)\\n,\\n\\ne\\n\\n‖\\ny\\n\\n‖\\n\\n2\\n\\n\\n\\n/\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\nφ\\n(\\ny\\n)\\n⟩\\n]\\n≈\\n⟨\\n\\ne\\n\\n‖\\nx\\n\\n‖\\n\\n2\\n\\n\\n\\n/\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\nφ\\n(\\nx\\n)\\n,\\n\\ne\\n\\n‖\\ny\\n\\n‖\\n\\n2\\n\\n\\n\\n/\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\nφ\\n(\\ny\\n)\\n⟩\\n\\n\\n{\\\\displaystyle e^{\\\\langle x,y\\\\rangle /\\\\sigma ^{2}}=\\\\mathbb {E} [\\\\langle e^{\\\\|x\\\\|^{2}/2\\\\sigma ^{2}}\\\\varphi (x),e^{\\\\|y\\\\|^{2}/2\\\\sigma ^{2}}\\\\varphi (y)\\\\rangle ]\\\\approx \\\\langle e^{\\\\|x\\\\|^{2}/2\\\\sigma ^{2}}\\\\varphi (x),e^{\\\\|y\\\\|^{2}/2\\\\sigma ^{2}}\\\\varphi (y)\\\\rangle }\\n\\nConsequently, the one-headed attention, with one query, can be written as \\n\\n\\n\\n\\nAttention\\n\\n(\\nq\\n,\\nK\\n,\\nV\\n)\\n=\\n\\nsoftmax\\n\\n\\n(\\n\\n\\n\\nq\\n\\nK\\n\\n\\nT\\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n)\\n\\nV\\n≈\\n\\n\\n\\nφ\\n(\\nq\\n\\n)\\n\\nT\\n\\n\\n\\n∑\\n\\ni\\n\\n\\n\\ne\\n\\n‖\\n\\nk\\n\\ni\\n\\n\\n\\n‖\\n\\n2\\n\\n\\n\\n/\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\nφ\\n(\\n\\nk\\n\\ni\\n\\n\\n)\\n\\nv\\n\\ni\\n\\n\\nT\\n\\n\\n\\n\\nφ\\n(\\nq\\n\\n)\\n\\nT\\n\\n\\n\\n∑\\n\\ni\\n\\n\\n\\ne\\n\\n‖\\n\\nk\\n\\ni\\n\\n\\n\\n‖\\n\\n2\\n\\n\\n\\n/\\n\\n2\\n\\nσ\\n\\n2\\n\\n\\n\\n\\nφ\\n(\\n\\nk\\n\\ni\\n\\n\\n)\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\text{Attention}}(q,K,V)={\\\\text{softmax}}\\\\left({\\\\frac {qK^{\\\\mathrm {T} }}{\\\\sqrt {d_{k}}}}\\\\right)V\\\\approx {\\\\frac {\\\\varphi (q)^{T}\\\\sum _{i}e^{\\\\|k_{i}\\\\|^{2}/2\\\\sigma ^{2}}\\\\varphi (k_{i})v_{i}^{T}}{\\\\varphi (q)^{T}\\\\sum _{i}e^{\\\\|k_{i}\\\\|^{2}/2\\\\sigma ^{2}}\\\\varphi (k_{i})}}}\\n\\nwhere \\n\\n\\n\\nσ\\n=\\n\\nd\\n\\nK\\n\\n\\n1\\n\\n/\\n\\n4\\n\\n\\n\\n\\n{\\\\displaystyle \\\\sigma =d_{K}^{1/4}}\\n\\n. Similarly for multiple queries, and for multiheaded attention.\\nThis approximation can be computed in linear time, as we can compute the matrix \\n\\n\\n\\nφ\\n(\\n\\nk\\n\\ni\\n\\n\\n)\\n\\nv\\n\\ni\\n\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle \\\\varphi (k_{i})v_{i}^{T}}\\n\\n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \\n\\n\\n\\n\\nAttention\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\n\\nsoftmax\\n\\n\\n(\\n\\n\\n\\nQ\\n\\nK\\n\\n\\nT\\n\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n)\\n\\nV\\n≈\\nQ\\n(\\n\\nK\\n\\nT\\n\\n\\nV\\n\\n/\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n)\\n\\n\\n{\\\\displaystyle {\\\\text{Attention}}(Q,K,V)={\\\\text{softmax}}\\\\left({\\\\frac {QK^{\\\\mathrm {T} }}{\\\\sqrt {d_{k}}}}\\\\right)V\\\\approx Q(K^{T}V/{\\\\sqrt {d_{k}}})}\\n\\nPerformer (2022)[102] uses the same Random Feature Attention, but \\n\\n\\n\\n\\nw\\n\\n1\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nw\\n\\nD\\n\\n\\n\\n\\n{\\\\displaystyle w_{1},...,w_{D}}\\n\\n are first independently sampled from the normal distribution \\n\\n\\n\\nN\\n(\\n0\\n,\\n\\nσ\\n\\n2\\n\\n\\nI\\n)\\n\\n\\n{\\\\displaystyle N(0,\\\\sigma ^{2}I)}\\n\\n, then they are Gram-Schmidt processed.\\n\\nMultimodality[edit]\\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.[103] The LLaVA was a vision-language model composed of a language model (Vicuna-13B)[104] and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.[105]\\nVision transformers[41] adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\\nConformer[42] and later Whisper[106] follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\\nPerceivers[107][108] are a variant of Transformers designed for multimodality.\\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022),[109] Phenaki (2023),[110] and Muse (2023).[111] Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.[112] Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.[113] Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.[111] Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.[110]\\n\\nApplications[edit]\\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, AlbertAGPT, Claude, BERT, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\\n\\nmachine translation\\ntime series prediction\\ndocument summarization\\ndocument generation\\nnamed entity recognition (NER)[114]\\nwriting computer code based on requirements expressed in natural language.\\nspeech-to-text\\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\\n\\nbiological sequence analysis\\nvideo understanding\\nprotein folding (such as AlphaFold)\\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.[10]\\nSee also[edit]\\nseq2seq\\xa0– Family of machine learning approaches\\nPerceiver\\xa0– Variant of Transformer designed for multimodal data\\nVision transformer\\xa0– Machine learning model for vision processing\\nLarge language model\\xa0– Type of machine learning model\\nBERT (language model)\\xa0– Series of language models developed by Google AI\\nGenerative pre-trained transformer\\xa0– Type of large language model\\nT5 (language model)\\xa0– Series of large language models developed by Google AI\\nNotes[edit]\\n\\n\\n^ Gated recurrent units (2014) further reduced its complexity.\\n\\n^ Some architectures, such as RWKV or state space models, avoid the issue.\\n\\n\\nReferences[edit]\\n\\n\\n^ a b c d e f g h i j k l Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). \"Attention is All you Need\" (PDF). Advances in Neural Information Processing Systems. 30. Curran Associates, Inc.\\n\\n^ Hochreiter, Sepp; Schmidhuber, Jürgen (1 November 1997). \"Long Short-Term Memory\". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. ISSN\\xa00899-7667. PMID\\xa09377276. S2CID\\xa01915014.\\n\\n^ a b \"Better Language Models and Their Implications\". OpenAI. 2019-02-14. Archived from the original on 2020-12-19. Retrieved 2019-08-25.\\n\\n^ a b Bahdanau; Cho, Kyunghyun; Bengio, Yoshua (September 1, 2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n\\n^ Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (August 17, 2015). \"Effective Approaches to Attention-based Neural Machine Translation\". arXiv:1508.04025 [cs.CL].\\n\\n^ a b Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor (2021-06-24), Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345\\n\\n^ Parisotto, Emilio; Song, Francis; Rae, Jack; Pascanu, Razvan; Gulcehre, Caglar; Jayakumar, Siddhant; Jaderberg, Max; Kaufman, Raphaël Lopez; Clark, Aidan; Noury, Seb; Botvinick, Matthew; Heess, Nicolas; Hadsell, Raia (2020-11-21). \"Stabilizing Transformers for Reinforcement Learning\". Proceedings of the 37th International Conference on Machine Learning. PMLR: 7487–7498.\\n\\n^ Radford, Alec; Jong Wook Kim; Xu, Tao; Brockman, Greg; McLeavey, Christine; Sutskever, Ilya (2022). \"Robust Speech Recognition via Large-Scale Weak Supervision\". arXiv:2212.04356 [eess.AS].\\n\\n^ Monastirsky, Maxim; Azulay, Osher; Sintov, Avishai (February 2023). \"Learning to Throw With a Handful of Samples Using Decision Transformers\". IEEE Robotics and Automation Letters. 8 (2): 576–583. doi:10.1109/LRA.2022.3229266. ISSN\\xa02377-3766.\\n\\n^ a b Ruoss, Anian; Delétang, Grégoire; Medapati, Sourabh; Grau-Moya, Jordi; Wenliang, Li; Catt, Elliot; Reid, John; Genewein, Tim (2024-02-07). \"Grandmaster-Level Chess Without Search\". arXiv:2402.04494v1 [cs.LG].\\n\\n^ a b Wolf, Thomas; Debut, Lysandre; Sanh, Victor; Chaumond, Julien; Delangue, Clement; Moi, Anthony; Cistac, Pierric; Rault, Tim; Louf, Remi; Funtowicz, Morgan; Davison, Joe; Shleifer, Sam; von Platen, Patrick; Ma, Clara; Jernite, Yacine; Plu, Julien; Xu, Canwen; Le Scao, Teven; Gugger, Sylvain; Drame, Mariama; Lhoest, Quentin; Rush, Alexander (2020). \"Transformers: State-of-the-Art Natural Language Processing\". Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp.\\xa038–45. doi:10.18653/v1/2020.emnlp-demos.6. S2CID\\xa0208117506.\\n\\n^ a b c \"Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing\". Google AI Blog. 2 November 2018. Archived from the original on 2021-01-13. Retrieved 2019-08-25.\\n\\n^ Feldman, J. A.; Ballard, D. H. (1982-07-01). \"Connectionist models and their properties\". Cognitive Science. 6 (3): 205–254. doi:10.1016/S0364-0213(82)80001-3. ISSN\\xa00364-0213.\\n\\n^ Rumelhart, David E.; McClelland, James L.; Hinton, Geoffrey E. (1987-07-29). Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations, Chapter 2 (PDF). Cambridge, Mass: Bradford Books. ISBN\\xa0978-0-262-68053-0.\\n\\n^ Giles, C. Lee; Maxwell, Tom (1987-12-01). \"Learning, invariance, and generalization in high-order neural networks\". Applied Optics. 26 (23): 4972–4978. doi:10.1364/AO.26.004972. ISSN\\xa00003-6935. PMID\\xa020523475.\\n\\n^ a b Schmidhuber, Jürgen (1992). \"Learning to control fast-weight memories: an alternative to recurrent nets\" (PDF). Neural Computation. 4 (1): 131–139. doi:10.1162/neco.1992.4.1.131. S2CID\\xa016683347.\\n\\n^ Christoph von der Malsburg: The correlation theory of brain function. Internal Report 81-2, MPI Biophysical Chemistry, 1981. http://cogprints.org/1380/1/vdM_correlation.pdf See Reprint in Models of Neural Networks II, chapter 2, pages 95-119. Springer, Berlin, 1994.\\n\\n^ Jerome A. Feldman, \"Dynamic connections in neural networks,\" Biological Cybernetics, vol. 46, no. 1, pp. 27-39, Dec. 1982.\\n\\n^ Hinton, Geoffrey E.; Plaut, David C. (1987). \"Using Fast Weights to Deblur Old Memories\". Proceedings of the Annual Meeting of the Cognitive Science Society. 9.\\n\\n^ Katharopoulos, Angelos; Vyas, Apoorv; Pappas, Nikolaos; Fleuret, François (2020). \"Transformers are RNNs: Fast autoregressive Transformers with linear attention\". ICML 2020. PMLR. pp.\\xa05156–5165.\\n\\n^ Schlag, Imanol; Irie, Kazuki; Schmidhuber, Jürgen (2021). \"Linear Transformers Are Secretly Fast Weight Programmers\". ICML 2021. Springer. pp.\\xa09355–9366.\\n\\n^ a b c Cho, Kyunghyun; van Merriënboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (October 2014). \"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\". In Moschitti, Alessandro; Pang, Bo; Daelemans, Walter (eds.). Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics. pp.\\xa01724–1734. arXiv:1406.1078. doi:10.3115/v1/D14-1179.\\n\\n^ a b c Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (14 Dec 2014). \"Sequence to sequence learning with neural networks\". arXiv:1409.3215 [cs.CL]. [first version posted to arXiv on 10 Sep 2014]\\n\\n^ Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". arXiv:1412.3555 [cs.NE].\\n\\n^ Gruber, N.; Jockisch, A. (2020), \"Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?\", Frontiers in Artificial Intelligence, 3: 40, doi:10.3389/frai.2020.00040, PMC\\xa07861254, PMID\\xa033733157, S2CID\\xa0220252321\\n\\n^ Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V (2014). \"Sequence to Sequence Learning with Neural Networks\". Advances in Neural Information Processing Systems. 27. Curran Associates, Inc. arXiv:1409.3215.\\n\\n^ Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (2015). \"Effective Approaches to Attention-based Neural Machine Translation\". arXiv:1508.04025 [cs.CL].\\n\\n^ Wu, Yonghui; et\\xa0al. (2016-09-01). \"Google\\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL].\\n\\n^ Lewis-Kraus, Gideon (2016-12-14). \"The Great A.I. Awakening\". The New York Times. ISSN\\xa00362-4331. Archived from the original on 24 May 2023. Retrieved 2023-06-22.\\n\\n^ Parikh, Ankur P.; Täckström, Oscar; Das, Dipanjan; Uszkoreit, Jakob (2016-09-25). \"A Decomposable Attention Model for Natural Language Inference\". arXiv:1606.01933 [cs.CL].\\n\\n^ a b Levy, Steven. \"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\". Wired. ISSN\\xa01059-1028. Archived from the original on 20 Mar 2024. Retrieved 2024-08-06.\\n\\n^ Cheng, Jianpeng; Dong, Li; Lapata, Mirella (November 2016). \"Long Short-Term Memory-Networks for Machine Reading\". In Su, Jian; Duh, Kevin; Carreras, Xavier (eds.). Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: Association for Computational Linguistics. pp.\\xa0551–561. doi:10.18653/v1/D16-1053.\\n\\n^ Peng, Bo; Alcaide, Eric; Anthony, Quentin; Albalak, Alon; Arcadinho, Samuel; Biderman, Stella; Cao, Huanqi; Cheng, Xin; Chung, Michael (2023-12-10), RWKV: Reinventing RNNs for the Transformer Era, arXiv:2305.13048\\n\\n^ Marche, Stephen (2024-08-23). \"Was Linguistic A.I. Created by Accident?\". The New Yorker. ISSN\\xa00028-792X. Retrieved 2024-08-27.\\n\\n^ a b c d e f Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805v2 [cs.CL].\\n\\n^ \"Google: BERT now used on almost every English query\". Search Engine Land. 2020-10-15. Retrieved 2020-11-24.\\n\\n^ \"Recent Advances in Google Translate\". research.google. Retrieved 2024-05-08.\\n\\n^ \"The inside story of how ChatGPT was built from the people who made it\". MIT Technology Review. Retrieved 2024-08-06.\\n\\n^ \"Improving language understanding with unsupervised learning\". openai.com. June 11, 2018. Archived from the original on 2023-03-18. Retrieved 2023-03-18.\\n\\n^ finetune-transformer-lm, OpenAI, June 11, 2018, retrieved 2023-05-01\\n\\n^ a b Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob (2021-06-03). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". arXiv:2010.11929 [cs.CV].\\n\\n^ a b Gulati, Anmol; Qin, James; Chiu, Chung-Cheng; Parmar, Niki; Zhang, Yu; Yu, Jiahui; Han, Wei; Wang, Shibo; Zhang, Zhengdong; Wu, Yonghui; Pang, Ruoming (2020). \"Conformer: Convolution-augmented Transformer for Speech Recognition\". arXiv:2005.08100 [eess.AS].\\n\\n^ Choromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Mohiuddin, Afroz (2022-11-19), Rethinking Attention with Performers, arXiv:2009.14794\\n\\n^ Liu, Zhuang; Mao, Hanzi; Wu, Chao-Yuan; Feichtenhofer, Christoph; Darrell, Trevor; Xie, Saining (2022). A ConvNet for the 2020s. Conference on Computer Vision and Pattern Recognition. pp.\\xa011976–11986.\\n\\n^ Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (2024-03-05), Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, arXiv:2403.03206\\n\\n^ a b Xiong, Ruibin; Yang, Yunchang; He, Di; Zheng, Kai; Zheng, Shuxin; Xing, Chen; Zhang, Huishuai; Lan, Yanyan; Wang, Liwei; Liu, Tie-Yan (2020-06-29). \"On Layer Normalization in the Transformer Architecture\". arXiv:2002.04745 [cs.LG].\\n\\n^ Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020-01-01). \"Exploring the limits of transfer learning with a unified text-to-text transformer\". The Journal of Machine Learning Research. 21 (1): 140:5485–140:5551. arXiv:1910.10683. ISSN\\xa01532-4435.\\n\\n^ Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2019). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\". arXiv:1910.10683 [cs.LG].\\n\\n^ a b \"Masked language modeling\". huggingface.co. Retrieved 2023-10-05.\\n\\n^ a b \"Causal language modeling\". huggingface.co. Retrieved 2023-10-05.\\n\\n^ a b c d Tay, Yi; Dehghani, Mostafa; Tran, Vinh Q.; Garcia, Xavier; Wei, Jason; Wang, Xuezhi; Chung, Hyung Won; Shakeri, Siamak; Bahri, Dara (2023-02-28), UL2: Unifying Language Learning Paradigms, arXiv:2205.05131\\n\\n^ Press, Ofir; Wolf, Lior (2017-02-21), Using the Output Embedding to Improve Language Models, arXiv:1608.05859\\n\\n^ Lintz, Nathan (2016-04-18). \"Sequence Modeling with Neural Networks (Part 2): Attention Models\". Indico. Archived from the original on 2020-10-21. Retrieved 2019-10-15.\\n\\n^ a b c Alammar, Jay. \"The Illustrated Transformer\". jalammar.github.io. Archived from the original on 2020-10-18. Retrieved 2019-10-15.\\n\\n^ Team, Keras. \"Keras documentation: GPT2Backbone model\". keras.io. Retrieved 2024-08-08.\\n\\n^ Clark, Kevin; Khandelwal, Urvashi; Levy, Omer; Manning, Christopher D. (August 2019). \"What Does BERT Look at? An Analysis of BERT\\'s Attention\". Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Florence, Italy: Association for Computational Linguistics: 276–286. arXiv:1906.04341. doi:10.18653/v1/W19-4828. Archived from the original on 2020-10-21. Retrieved 2020-05-20.\\n\\n^ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Russ R; Le, Quoc V (2019). \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\". Advances in Neural Information Processing Systems. 32. Curran Associates, Inc. arXiv:1906.08237.\\n\\n^ Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya (11 June 2018). \"Improving Language Understanding by Generative Pre-Training\" (PDF). OpenAI. p.\\xa012. Archived (PDF) from the original on 26 January 2021. Retrieved 23 January 2021.\\n\\n^ Wang, Qiang; Li, Bei; Xiao, Tong; Zhu, Jingbo; Li, Changliang; Wong, Derek F.; Chao, Lidia S. (2019-06-04), Learning Deep Transformer Models for Machine Translation, arXiv:1906.01787\\n\\n^ Phuong, Mary; Hutter, Marcus (2022-07-19), Formal Algorithms for Transformers, arXiv:2207.09238\\n\\n^ a b c Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\". Journal of Machine Learning Research. 21 (140): 1–67. arXiv:1910.10683. ISSN\\xa01533-7928.\\n\\n^ \"Recent Advances in Google Translate\". Google Research. June 8, 2020. Archived from the original on 4 Jul 2024. Retrieved 2024-08-07.\\n\\n^ Shazeer, Noam (2020-02-01). \"GLU Variants Improve Transformer\". arXiv:2002.05202 [cs.LG].\\n\\n^ Hendrycks, Dan; Gimpel, Kevin (2016-06-27). \"Gaussian Error Linear Units (GELUs)\". arXiv:1606.08415v5 [cs.LG].\\n\\n^ Shazeer, Noam (February 14, 2020). \"GLU Variants Improve Transformer\". arXiv:2002.05202 [cs.LG].\\n\\n^ Zhang, Biao; Sennrich, Rico (2019). \"Root Mean Square Layer Normalization\". Advances in Neural Information Processing Systems. 32. Curran Associates, Inc. arXiv:1910.07467.\\n\\n^ Tembine, Hamidou, Manzoor Ahmed Khan, and Issa Bamia. 2024. \"Mean-Field-Type Transformers\" Mathematics 12, no. 22: 3506. https://doi.org/10.3390/math12223506\\n\\n^ a b Nguyen, Toan Q.; Salazar, Julian (2019-11-02). Niehues, Jan; Cattoni, Rolando; Stüker, Sebastian; Negri, Matteo; Turchi, Marco; Ha, Thanh-Le; Salesky, Elizabeth; Sanabria, Ramon; Barrault, Loic (eds.). \"Transformers without Tears: Improving the Normalization of Self-Attention\". Proceedings of the 16th International Conference on Spoken Language Translation. Hong Kong: Association for Computational Linguistics. arXiv:1910.05895. doi:10.5281/zenodo.3525484.\\n\\n^ Dufter, Philipp; Schmitt, Martin; Schütze, Hinrich (2022-06-06). \"Position Information in Transformers: An Overview\". Computational Linguistics. 48 (3): 733–763. arXiv:2102.11090. doi:10.1162/coli_a_00445. ISSN\\xa00891-2017. S2CID\\xa0231986066.\\n\\n^ Gehring, Jonas; Auli, Michael; Grangier, David; Yarats, Denis; Dauphin, Yann N. (2017-07-17). \"Convolutional Sequence to Sequence Learning\". Proceedings of the 34th International Conference on Machine Learning. PMLR: 1243–1252.\\n\\n^ Haviv, Adi; Ram, Ori; Press, Ofir; Izsak, Peter; Levy, Omer (2022-12-05), Transformer Language Models without Positional Encodings Still Learn Positional Information, arXiv:2203.16634\\n\\n^ Su, Jianlin; Lu, Yu; Pan, Shengfeng; Murtadha, Ahmed; Wen, Bo; Liu, Yunfeng (2021-04-01). \"RoFormer: Enhanced Transformer with Rotary Position Embedding\". arXiv:2104.09864 [cs.CL].\\n\\n^ Press, Ofir; Smith, Noah A.; Lewis, Mike (2021-08-01). \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\". arXiv:2108.12409 [cs.CL].\\n\\n^ Shaw, Peter; Uszkoreit, Jakob; Vaswani, Ashish (2018). \"Self-Attention with Relative Position Representations\". arXiv:1803.02155 [cs.CL].\\n\\n^ Ke, Guolin; He, Di; Liu, Tie-Yan (2021-03-15), Rethinking Positional Encoding in Language Pre-training, arXiv:2006.15595\\n\\n^ Kwon, Woosuk; Li, Zhuohan; Zhuang, Siyuan; Sheng, Ying; Zheng, Lianmin; Yu, Cody Hao; Gonzalez, Joseph; Zhang, Hao; Stoica, Ion (2023-10-23). \"Efficient Memory Management for Large Language Model Serving with PagedAttention\". Proceedings of the 29th Symposium on Operating Systems Principles. SOSP \\'23. New York, NY, USA: Association for Computing Machinery. pp.\\xa0611–626. arXiv:2309.06180. doi:10.1145/3600006.3613165. ISBN\\xa0979-8-4007-0229-7.\\n\\n^ vllm-project/vllm, vLLM, 2024-06-20, retrieved 2024-06-20\\n\\n^ Contribution), Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal (2023-06-20). \"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\". vLLM Blog. Retrieved 2024-06-20.{{cite web}}:  CS1 maint: multiple names: authors list (link)\\n\\n^ Dao, Tri; Fu, Dan; Ermon, Stefano; Rudra, Atri; Ré, Christopher (2022-12-06). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\". Advances in Neural Information Processing Systems. 35: 16344–16359. arXiv:2205.14135.\\n\\n^ \"Stanford CRFM\". crfm.stanford.edu. Retrieved 2023-07-18.\\n\\n^ \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\". Princeton NLP. 2023-06-17. Retrieved 2023-07-18.\\n\\n^ \"Introducing Together AI Chief Scientist Tri Dao, as he releases FlashAttention-2 to speed up model training and inference\". TOGETHER. Retrieved 2023-07-18.\\n\\n^ Ainslie, Joshua; Lee-Thorp, James; de Jong, Michiel; Zemlyanskiy, Yury; Lebrón, Federico; Sanghai, Sumit (2023-12-23). \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\". arXiv:2305.13245 [cs.CL].\\n\\n^ Chowdhery, Aakanksha; Narang, Sharan; Devlin, Jacob; Bosma, Maarten; Mishra, Gaurav; Roberts, Adam; Barham, Paul; Chung, Hyung Won; Sutton, Charles; Gehrmann, Sebastian; Schuh, Parker; Shi, Kensen; Tsvyashchenko, Sasha; Maynez, Joshua; Rao, Abhishek (2022-04-01). \"PaLM: Scaling Language Modeling with Pathways\". arXiv:2204.02311 [cs.CL].\\n\\n^ Ainslie, Joshua; Lee-Thorp, James; de Jong, Michiel; Zemlyanskiy, Yury; Lebrón, Federico; Sanghai, Sumit (2023-12-23), GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints, arXiv:2305.13245\\n\\n^ a b DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (19 June 2024), DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model, arXiv:2405.04434.\\n\\n^ a b Leviathan, Yaniv; Kalman, Matan; Matias, Yossi (2023-05-18), Fast Inference from Transformers via Speculative Decoding, arXiv:2211.17192\\n\\n^ Fu, Yao (2023-12-13). \"Towards 100x Speedup: Full Stack Transformer Inference Optimization\".\\n\\n^ Chen, Charlie; Borgeaud, Sebastian; Irving, Geoffrey; Lespiau, Jean-Baptiste; Sifre, Laurent; Jumper, John (2023-02-02), Accelerating Large Language Model Decoding with Speculative Sampling, arXiv:2302.01318\\n\\n^ Gloeckle, Fabian; Idrissi, Badr Youbi; Rozière, Baptiste; Lopez-Paz, David; Synnaeve, Gabriel (2024-04-30), Better & Faster Large Language Models via Multi-token Prediction, arXiv, doi:10.48550/arXiv.2404.19737, arXiv:2404.19737\\n\\n^ DeepSeek-AI; Liu, Aixin; Feng, Bei; Xue, Bing; Wang, Bingxuan; Wu, Bochao; Lu, Chengda; Zhao, Chenggang; Deng, Chengqi (2024-12-27), DeepSeek-V3 Technical Report, arXiv, doi:10.48550/arXiv.2412.19437, arXiv:2412.19437\\n\\n^ a b Kitaev, Nikita; Kaiser, Łukasz; Levskaya, Anselm (2020). \"Reformer: The Efficient Transformer\". arXiv:2001.04451 [cs.LG].\\n\\n^ Liu, Ze; Lin, Yutong; Cao, Yue; Hu, Han; Wei, Yixuan; Zhang, Zheng; Lin, Stephen; Guo, Baining (2021). \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\". 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. pp.\\xa09992–10002. arXiv:2103.14030. doi:10.1109/ICCV48922.2021.00986. ISBN\\xa0978-1-6654-2812-5.\\n\\n^ Ristea, Nicolaea Catalin; Ionescu, Radu Tudor; Khan, Fahad Shahbaz (2022-09-18). \"SepTr: Separable Transformer for Audio Spectrogram Processing\". Interspeech. ISCA: 4103–4107. arXiv:2203.09581. doi:10.21437/Interspeech.2022-249.\\n\\n^ Tay, Yi; Dehghani, Mostafa; Abnar, Samira; Shen, Yikang; Bahri, Dara; Pham, Philip; Rao, Jinfeng; Yang, Liu; Ruder, Sebastian; Metzler, Donald (2020-11-08). \"Long Range Arena: A Benchmark for Efficient Transformers\". arXiv:2011.04006 [cs.LG].\\n\\n^ \"Reformer: The Efficient Transformer\". Google AI Blog. 16 January 2020. Archived from the original on 2020-10-22. Retrieved 2020-10-22.\\n\\n^ Gomez, Aidan N; Ren, Mengye; Urtasun, Raquel; Grosse, Roger B (2017). \"The Reversible Residual Network: Backpropagation Without Storing Activations\". Advances in Neural Information Processing Systems. 30. Curran Associates, Inc. arXiv:1707.04585.\\n\\n^ Child, Rewon; Gray, Scott; Radford, Alec; Sutskever, Ilya (2019-04-23), Generating Long Sequences with Sparse Transformers, arXiv:1904.10509\\n\\n^ \"Constructing Transformers For Longer Sequences with Sparse Attention Methods\". Google AI Blog. 25 March 2021. Archived from the original on 2021-09-18. Retrieved 2021-05-28.\\n\\n^ Zhai, Shuangfei; Talbott, Walter; Srivastava, Nitish; Huang, Chen; Goh, Hanlin; Zhang, Ruixiang; Susskind, Josh (2021-09-21). \"An Attention Free Transformer\". arXiv:2105.14103 [cs.LG].\\n\\n^ Peng, Hao; Pappas, Nikolaos; Yogatama, Dani; Schwartz, Roy; Smith, Noah A.; Kong, Lingpeng (2021-03-19). \"Random Feature Attention\". arXiv:2103.02143 [cs.CL].\\n\\n^ Choromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Belanger, David; Colwell, Lucy; Weller, Adrian (2020-09-30). \"Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers\". arXiv:2006.03555 [cs.LG].\\n\\n^ Lu, Kevin; Grover, Aditya; Abbeel, Pieter; Mordatch, Igor (2022-06-28). \"Frozen Pretrained Transformers as Universal Computation Engines\". Proceedings of the AAAI Conference on Artificial Intelligence. 36 (7): 7628–7636. doi:10.1609/aaai.v36i7.20729. ISSN\\xa02374-3468.\\n\\n^ \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org\". lmsys.org. Retrieved 2024-08-11.\\n\\n^ Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-12-15). \"Visual Instruction Tuning\". Advances in Neural Information Processing Systems. 36: 34892–34916.\\n\\n^ Radford, Alec; Kim, Jong Wook; Xu, Tao; Brockman, Greg; McLeavey, Christine; Sutskever, Ilya (2022). \"Robust Speech Recognition via Large-Scale Weak Supervision\". arXiv:2212.04356 [eess.AS].\\n\\n^ Jaegle, Andrew; Gimeno, Felix; Brock, Andrew; Zisserman, Andrew; Vinyals, Oriol; Carreira, Joao (2021-06-22). \"Perceiver: General Perception with Iterative Attention\". arXiv:2103.03206 [cs.CV].\\n\\n^ Jaegle, Andrew; Borgeaud, Sebastian; Alayrac, Jean-Baptiste; Doersch, Carl; Ionescu, Catalin; Ding, David; Koppula, Skanda; Zoran, Daniel; Brock, Andrew; Shelhamer, Evan; Hénaff, Olivier (2021-08-02). \"Perceiver IO: A General Architecture for Structured Inputs & Outputs\". arXiv:2107.14795 [cs.LG].\\n\\n^ \"Parti: Pathways Autoregressive Text-to-Image Model\". sites.research.google. Retrieved 2024-08-09.\\n\\n^ a b Villegas, Ruben; Babaeizadeh, Mohammad; Kindermans, Pieter-Jan; Moraldo, Hernan; Zhang, Han; Saffar, Mohammad Taghi; Castro, Santiago; Kunze, Julius; Erhan, Dumitru (2022-09-29). \"Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions\". {{cite journal}}: Cite journal requires |journal= (help)\\n\\n^ a b Chang, Huiwen; Zhang, Han; Barber, Jarred; Maschinot, A. J.; Lezama, Jose; Jiang, Lu; Yang, Ming-Hsuan; Murphy, Kevin; Freeman, William T. (2023-01-02). \"Muse: Text-To-Image Generation via Masked Generative Transformers\". arXiv:2301.00704 [cs.CV].\\n\\n^ Ramesh, Aditya; Pavlov, Mikhail; Goh, Gabriel; Gray, Scott; Voss, Chelsea; Radford, Alec; Chen, Mark; Sutskever, Ilya (2021-02-26), Zero-Shot Text-to-Image Generation, arXiv:2102.12092\\n\\n^ Yu, Jiahui; Xu, Yuanzhong; Koh, Jing Yu; Luong, Thang; Baid, Gunjan; Wang, Zirui; Vasudevan, Vijay; Ku, Alexander; Yang, Yinfei (2022-06-21), Scaling Autoregressive Models for Content-Rich Text-to-Image Generation, arXiv:2206.10789\\n\\n^ Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023). \"Precision information extraction for rare disease epidemiology at scale\". Journal of Translational Medicine. 21 (1): 157. doi:10.1186/s12967-023-04011-y. PMC\\xa09972634. PMID\\xa036855134.\\n\\n\\nFurther reading[edit]\\n\\nAlexander Rush, The Annotated transformer Archived 2021-09-22 at the Wayback Machine, Harvard NLP group, 3 April 2018\\nPhuong, Mary; Hutter, Marcus (2022). \"Formal Algorithms for Transformers\". arXiv:2207.09238 [cs.LG].\\nFerrando, Javier; Sarti, Gabriele; Bisazza, Arianna; Costa-jussà, Marta R. (2024-05-01). \"A Primer on the Inner Workings of Transformer-based Language Models\". arXiv:2405.00208 [cs.CL].\\n\\nvteGoogle AI\\nGoogle\\nGoogle Brain\\nGoogle DeepMind\\nComputer programsAlphaGoVersions\\nAlphaGo (2015)\\nMaster (2016)\\nAlphaGo Zero (2017)\\nAlphaZero (2017)\\nMuZero (2019)\\nCompetitions\\nFan Hui (2015)\\nLee Sedol (2016)\\nKe Jie (2017)\\nIn popular culture\\nAlphaGo (2017)\\nThe MANIAC (2023)\\nOther\\nAlphaFold (2018)\\nAlphaStar (2019)\\nAlphaDev (2023)\\nAlphaGeometry (2024)\\nMachine learningNeural networks\\nInception (2014)\\nWaveNet (2016)\\nMobileNet (2017)\\nTransformer (2017)\\nEfficientNet (2019)\\nGato (2022)\\nOther\\nQuantum Artificial Intelligence Lab\\nTensorFlow\\nTensor Processing Unit\\nGenerative AIChatbots\\nAssistant (2016)\\nSparrow (2022)\\nGemini (2023)\\nLanguage models\\nBERT (2018)\\nXLNet (2019)\\nT5 (2019)\\nLaMDA (2021)\\nChinchilla (2022)\\nPaLM (2022)\\nGemini (2023)\\nVideoPoet (2024)\\nOther\\nDreamBooth (2022)\\nNotebookLM (2023)\\nVids (2024)\\nSee also\\n\"Attention Is All You Need\"\\nFuture of Go Summit\\nGenerative pre-trained transformer\\nGoogle Labs\\nGoogle Pixel\\nGoogle Workspace\\nRobot Constitution\\n\\n Category\\n Commons\\n\\nvteArtificial intelligence (AI)History (timeline)Concepts\\nParameter\\nHyperparameter\\nLoss functions\\nRegression\\nBias–variance tradeoff\\nDouble descent\\nOverfitting\\nClustering\\nGradient descent\\nSGD\\nQuasi-Newton method\\nConjugate gradient method\\nBackpropagation\\nAttention\\nConvolution\\nNormalization\\nBatchnorm\\nActivation\\nSoftmax\\nSigmoid\\nRectifier\\nGating\\nWeight initialization\\nRegularization\\nDatasets\\nAugmentation\\nPrompt engineering\\nReinforcement learning\\nQ-learning\\nSARSA\\nImitation\\nPolicy gradient\\nDiffusion\\nLatent diffusion model\\nAutoregression\\nAdversary\\nRAG\\nUncanny valley\\nRLHF\\nSelf-supervised learning\\nRecursive self-improvement\\nWord embedding\\nHallucination\\nApplications\\nMachine learning\\nIn-context learning\\nArtificial neural network\\nDeep learning\\nLanguage model\\nLarge language model\\nNMT\\nArtificial general intelligence\\nImplementationsAudio–visual\\nAlexNet\\nWaveNet\\nHuman image synthesis\\nHWR\\nOCR\\nSpeech synthesis\\n15.ai\\nElevenLabs\\nSpeech recognition\\nWhisper\\nFacial recognition\\nAlphaFold\\nText-to-image models\\nAurora\\nDALL-E\\nFirefly\\nFlux\\nIdeogram\\nImagen\\nMidjourney\\nStable Diffusion\\nText-to-video models\\nDream Machine\\nGen-3 Alpha\\nHailuo AI\\nKling\\nSora\\nVeo\\nMusic generation\\nSuno AI\\nUdio\\nText\\nWord2vec\\nSeq2seq\\nGloVe\\nBERT\\nT5\\nLlama\\nChinchilla AI\\nPaLM\\nGPT\\n1\\n2\\n3\\nJ\\nChatGPT\\n4\\n4o\\no1\\no3\\nClaude\\nGemini\\nchatbot\\nGrok\\nLaMDA\\nBLOOM\\nProject Debater\\nIBM Watson\\nIBM Watsonx\\nGranite\\nPanGu-Σ\\nDeepSeek\\nQwen\\nDecisional\\nAlphaGo\\nAlphaZero\\nOpenAI Five\\nSelf-driving car\\nMuZero\\nAction selection\\nAutoGPT\\nRobot control\\nPeople\\nAlan Turing\\nWarren Sturgis McCulloch\\nWalter Pitts\\nJohn von Neumann\\nClaude Shannon\\nMarvin Minsky\\nJohn McCarthy\\nNathaniel Rochester\\nAllen Newell\\nCliff Shaw\\nHerbert A. Simon\\nOliver Selfridge\\nFrank Rosenblatt\\nBernard Widrow\\nJoseph Weizenbaum\\nSeymour Papert\\nSeppo Linnainmaa\\nPaul Werbos\\nJürgen Schmidhuber\\nYann LeCun\\nGeoffrey Hinton\\nJohn Hopfield\\nYoshua Bengio\\nLotfi A. Zadeh\\nStephen Grossberg\\nAlex Graves\\nAndrew Ng\\nFei-Fei Li\\nAlex Krizhevsky\\nIlya Sutskever\\nDemis Hassabis\\nDavid Silver\\nIan Goodfellow\\nAndrej Karpathy\\nArchitectures\\nNeural Turing machine\\nDifferentiable neural computer\\nTransformer\\nVision transformer (ViT)\\nRecurrent neural network (RNN)\\nLong short-term memory (LSTM)\\nGated recurrent unit (GRU)\\nEcho state network\\nMultilayer perceptron (MLP)\\nConvolutional neural network (CNN)\\nResidual neural network (RNN)\\nHighway network\\nMamba\\nAutoencoder\\nVariational autoencoder (VAE)\\nGenerative adversarial network (GAN)\\nGraph neural network (GNN)\\n\\n Portals\\nTechnology\\n Category\\nArtificial neural networks\\nMachine learning\\n List\\nCompanies\\nProjects\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Transformer_(deep_learning_architecture)&oldid=1275591055\"\\nCategories: Google softwareNeural network architectures2017 in artificial intelligenceHidden categories: CS1 maint: multiple names: authors listCS1 errors: missing periodicalArticles with short descriptionShort description is different from WikidataWebarchive template wayback links\\n\\n\\n\\n\\n\\n\\n This page was last edited on 13 February 2025, at 22:36\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nTransformer (deep learning architecture)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n28 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
